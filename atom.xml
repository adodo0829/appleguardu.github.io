<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>AppleGuard的博客</title>
  
  <subtitle>天下事有难易乎？为之，则难者亦易矣；不为，则易者亦难矣。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-10-29T13:35:37.396Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>huhua</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>关于css+div的一些总结</title>
    <link href="http://yoursite.com/2018/10/28/%E5%85%B3%E4%BA%8Ecss%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2018/10/28/关于css的一些总结/</id>
    <published>2018-10-28T06:35:00.000Z</published>
    <updated>2018-10-29T13:35:37.396Z</updated>
    
    <content type="html"><![CDATA[<p>关于css阶段性的学习总结…<br><a id="more"></a></p><h2 id="css选择器"><a href="#css选择器" class="headerlink" title="css选择器"></a>css选择器</h2><p>css选择器是用于对网页中的元素进行样式的控制，比如文字，背景，图片等等…  </p><ul><li>选择器的分类<br>用的比较多的是标签，类，id，像*，！important这些比较少用</li><li>复合选择器<br>后代选择器，交集选择器(li.name,提高元素控制精确度，少用)，并集选择器(,)，伪类选择器(a:hover/li:nth-child等…),子代选择器(li&gt;a)</li><li>注意: 在使用选择器作用某一个元素时，选择器不要太冗长，一般1-3个，对用属性相同的元素类名可以复用，还要注意复合选择器的权重和优先级。</li></ul><h2 id="css的三大特性"><a href="#css的三大特性" class="headerlink" title="css的三大特性"></a>css的三大特性</h2><p>继承，层叠，优先级 </p><ul><li>层叠性<br>  多个选择器(权重相同情况下)作用到同一标签，发生属性冲突，后来者居上</li><li>继承性<br>  存在嵌套关系的元素，子元素会继承父元素的部分属性<br>  特殊性（a标签的颜色，h标签的大小）</li><li>优先级<br>  继承 &lt; 通配符 &lt; 标签选择器 &lt; 类选择器 &lt; id选择器 &lt; 行内式 &lt; ！important</li></ul><h2 id="css中的元素的显示模式"><a href="#css中的元素的显示模式" class="headerlink" title="css中的元素的显示模式"></a>css中的元素的显示模式</h2><ul><li>行内元素<br>一行显示多个，不能设置宽高；如标签span a ins strong del…..</li><li>块级元素<br> 单独占据一行，可以设置宽高；如标签div h p ul li…..</li><li>行内块元素<br> 具有以上两者的特点，一行显示多个，可以设置宽高；如标签img input……</li><li>模式之间的转化<br>可以通过display: inline block inline-block进行模式转化</li></ul><h2 id="css中常用的样式属性"><a href="#css中常用的样式属性" class="headerlink" title="css中常用的样式属性"></a>css中常用的样式属性</h2><ul><li>字体<br>大小，字体库，颜色，style，行高lh（文字居中的关键，也可调节多行文字的间距）</li><li>背景<br>bgc(可以写颜色名,rgb,rgba,注意transparent的使用)<br>尽量使用连写方式，bg：url 是否平铺 位置(可以是xy坐标，可以是方位) 颜色<br>注意：bga：fixed；下拉滚动条，背景不动  </li><li>插入图片和背景图片的区别<ol><li>插入图片 我们用的最多 比如产品展示类  移动位置只能靠盒模型 padding margin</li><li>背景图片我们一般用于小图标背景 或者 超大背景图片  背景图片 只能通过  background-position</li><li>图片去除间隙方法：转为块级；vertical-align<h2 id="盒子模型"><a href="#盒子模型" class="headerlink" title="盒子模型"></a>盒子模型</h2>盒子的组成：内容，边框border，内边距padding（内容与边框的距离），外边距margin（盒子之间的距离）<br>margin，padding，border都有四边，属性连写顺序顺时针方向top right bottom left,页面布局中经常用到，学会合理的使用！！！</li></ol></li></ul><p><strong>对于盒子的理解：</strong><br>网页中的标签元素其实都可以看作一个盒子，我们可以把盒子看作是一个存放内容（图片，文本，链接…）的容器，这个容器有它的高宽，内容，还有和内容的间距，以及容器之间的间距。最常用的盒子：div标签</p><p><strong>盒子的真实宽高</strong> = content-width/height + padding + border   </p><blockquote><pre><code>盒子水平居中,这个在项目中经常用到，margin:0 auto 1.块级block  2.盒子有宽度width</code></pre></blockquote><p><strong>盒子常用属性和注意点：</strong>  </p><blockquote><pre><code>1.常用属性：宽高，边框，内边距，外边距，圆角，背景，overflow等等....2.注意点：2.1 padding和margin对行内元素不起作用2.2 避免垂直塌陷(上下盒子排布会取较大的margin)；包含塌陷(嵌套关系的盒子，子盒子设置mt会带偏父盒子，给父元素添加overflow或者mt)2.3 元素隐藏display:none，位置不保留2.4 文字在盒子中溢出显示    white-space:nowrap    text-overflow: ellipsis    overflow:hidden2.5 如何盒子水平方向内容经常变化，宽度不写死，用内边距撑开2.6 盒子的高度一般情况下也不宜写死，由内容来撑开</code></pre></blockquote><h2 id="关于浮动"><a href="#关于浮动" class="headerlink" title="关于浮动"></a>关于浮动</h2><p>浮动：给元素添加float属性，会使元素脱离标准流，浮动起来。。。  </p><p><strong>浮动的特点</strong>  </p><blockquote><p>  1-浮动的元素，实现了模式的转换（拥有了行内块的特点）<br>  2-浮动的元素，顺序与结构代码有关，依次排列。<br>  3-浮动的元素，脱离了标准流，层级比标准流高，会浮在标准元素之上,标准          流会去占据浮动元素的位置。<br>  4-浮动元素是顶部对齐。<br>  5-浮动的父元素会把子元素带跑<br>  6-浮动的子元素依旧会在父元素范围内<br>  7-如果浮动的元素前面有标准流，那么浮动元素只能在标准流之后浮动</p></blockquote><p><strong>浮动的影响和解决</strong><br>在项目中，我们使用嵌套盒子时，一般不会给父盒子设高度，当父盒子是标准流时，若子盒子又浮动起来，脱离了标准流，父盒子就会丧失高度，其他标准流的元素就会来占据他的位置，造成布局混乱。<br>解决措施：给父盒子清除浮动：高度法，单双伪元素法，添加overflow属性  </p><blockquote><p>   这里写一个单伪元素法,5个属性！！！<br>     <code>.clearfix:after {        content：&quot;.&quot;;        display: block;           clear: both;           visibility: hidden;           height: 0;    }</code></p></blockquote><h2 id="关于定位"><a href="#关于定位" class="headerlink" title="关于定位"></a>关于定位</h2><ul><li>相对定位<br>不会转换显示模式，没有脱离标准流，还会保留原来的位置，但是显示的层级会变高，一般用于配合绝对定位。  </li><li>固定定位<br>实现了模式转化，具有行内块的特点；脱离了标准流，不会占据标准流的位置；<br>位移参照物为body。</li><li>绝对定位<br>实现了模式转化，具有行内块的特点；脱离了标准流，不会占据标准流的位置；<br>位移参照物为最近的具有定位属性的父盒子。  </li><li>显示层级<br>定位 &gt;  浮动  &gt;  标准流</li></ul><h2 id="关于css-div布局"><a href="#关于css-div布局" class="headerlink" title="关于css+div布局"></a>关于css+div布局</h2><p>静态页面布局的一般步骤：  </p><ul><li>1.将网页先拆分纵向分布的大盒子  </li><li>2.再对大盒子进行逐级拆分  </li><li>3.选择合适的方式(<strong>标准流，浮动，定位</strong>)，先分析目标盒子的特征，比如是一行显示多个，还是多行显示，或者是混搭。  </li></ul><p>常用技巧：</p><ol><li>类似头部导航栏一行显示多个，可以采用ul&gt;li标签+浮动来做  </li><li>个别独立的小模块可以用一个盒子写好，直接定位过去。</li><li>图文组合在一个盒子中的，文字换行存在间距的用行高撑开比较好</li><li>logo的写法，a+背景图片</li><li>底部footer栏，dl+dt&gt;span + dd&gt;a,然后浮动</li><li>底部about栏，直接用标准流写比较好 a+|的形式</li><li>精灵图 修改背景图的bgp即可</li><li>版心布局</li><li>滑动门的原理，不定宽度，父子元素使用同一个背景图片，距离padding撑开。</li><li>暂时先写这么多。。。未完待续。。。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于css阶段性的学习总结…&lt;br&gt;
    
    </summary>
    
      <category term="前端学习" scheme="http://yoursite.com/categories/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="css" scheme="http://yoursite.com/tags/css/"/>
    
  </entry>
  
  <entry>
    <title>聊聊一段自学经历的感受</title>
    <link href="http://yoursite.com/2018/10/15/%E4%B8%AA%E4%BA%BA%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93-%E4%B8%80/"/>
    <id>http://yoursite.com/2018/10/15/个人学习总结-一/</id>
    <published>2018-10-15T01:33:23.000Z</published>
    <updated>2018-10-17T09:55:24.528Z</updated>
    
    <content type="html"><![CDATA[<p>写这篇博客的目的在于对之前几个月的python学习做一个阶段性的总结，当然在这几个月自学过程中确实踩了不少坑，不过令人欣慰的是，也收获到在编程道路上的一些学习经验和教训。<br><a id="more"></a></p><h2 id="经历"><a href="#经历" class="headerlink" title="经历"></a>经历</h2><p>在接触python这门语言之前，自己工作内容中涉及到的编程主要是plc编程，主要应用于自动化行业，用来控制机器人，非标设备，生产线的正常运转。但与互联网行业中软件编程是完全不同的概念。  </p><p>所以在最开始的学习阶段，我采用主要学习方式还是跟当初在学校时学习模式一样，一边看着视频教程，一边在笔记本上写笔记，花在敲代码的时间比较少。(这种学习方法也为后面埋下很多坑)，当时就是一直按照这种学习节奏，一般最开始学习新东西的时候积极性特别高，所以每天的下班的时间和周末的休息时间都用来学习，就这样断断续续的从三月份学到了六月份，把python的基础内容学完了。然后就跟领导提了辞职，说了一下自己的想法，领导人也挺不错的，马上就同意放人了。  </p><p>python相关岗位有web开发，爬虫，数据分析，自动化运维等方向，我当时觉得爬虫挺好玩的，通过运行自己写的程序能够把别人网站的数据爬下来，所以就打算往这个方向发展。之后就在GitHub上找了一些项目，想着自己把它实现一遍，然后写在简历上方便以后找工作。但是在后面写项目的时候，啥都写不出来，脑子一片空白，不知从哪里下手，当时我就意识到一定是自己得到学习方法出了问题。因为周围的同学要么是做java开发的，要么是前端开发，在python方面都没有经验，我也没人去交流，当时就想无头苍蝇一样，没有方向，后来还是一位好朋友的哥哥介绍了一位python方面比较精通的前辈给我认识，在跟他聊天的过程中，确实感觉到自己跟他差距很大，谈吐之间就体现出一个程序员的专业素养，从逻辑思维，对问题的认识深度，以及业务流程，职业发展方向的大局观上等各方面都可以反应出来。之后那个人帮我指出身上存在的一些问题。</p><h2 id="感悟"><a href="#感悟" class="headerlink" title="感悟"></a>感悟</h2><p>之后我也对自身存在的一些问题做了一些总结。  </p><ul><li>1.学习模式上<br>写笔记学习的方式，效率真的太低，写10行笔记还不如在编辑器上敲上一行代码。个人认为比较有效的学习方法就是：先构建知识体系，框架，然后填充具体内容；在理解的内容的基础上一定要多注重实践，多敲代码才是正道。现在回想一下当初学习的时候，笔记写了100多张a4纸，感觉真的SB。</li><li>2.思维的转变<br>之前写代码不知道从何下手，没有头绪，发现自己的思维方式没有转变过来。要用编程思维去看待一个问题，以往工作模式很多都只是停留在事物，问题的表面，现在我们需要要用计算机的思维去看待一个问题。就是先将问题进行分解成小步骤，然后抽象成自己熟悉的模式，最后再设计这些步骤去解决问题。</li><li>3.思想的转变<br>之前很多代码都是跟着书上的内容敲出来的，结果前一秒把书一关，后一秒就不知道怎么敲了；作为一个程序员，一定要有自己独立的思考方式，要培养自己的逻辑思维能力，独立的去完成代码功能的实现。遇到需求或问题首先要学会自己主动去思考，解决问题，实在搞不定可以咨询别人，完全依赖书本或他人是完全万万行不通的。因为以后工作中是没人可以依靠的，可能工作以后遇到不懂问题我们依靠最多回事百度和Google。</li><li>4.做事要有明确的目标和方向  </li><li>5.定期对自己做一个自我反思。要清楚自己的定位。</li></ul><h2 id="寄语"><a href="#寄语" class="headerlink" title="寄语"></a>寄语</h2><ul><li>既然选择了做技术，就一定学会自主，持续，高效的去学习新知识，互联网行业技术更新迭代的速度很快，不学习很快就被淘汰了。</li><li>多关注行业技术发展，优秀博文，与大牛交流。。。</li><li>加油！！！！！！   </li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;写这篇博客的目的在于对之前几个月的python学习做一个阶段性的总结，当然在这几个月自学过程中确实踩了不少坑，不过令人欣慰的是，也收获到在编程道路上的一些学习经验和教训。&lt;br&gt;
    
    </summary>
    
      <category term="个人随笔" scheme="http://yoursite.com/categories/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="杂谈" scheme="http://yoursite.com/tags/%E6%9D%82%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title>html相关知识点</title>
    <link href="http://yoursite.com/2018/10/14/html%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    <id>http://yoursite.com/2018/10/14/html相关知识点/</id>
    <published>2018-10-14T07:53:15.000Z</published>
    <updated>2018-10-14T08:53:44.520Z</updated>
    
    <content type="html"><![CDATA[<p>关于html的一些总结。<br><a id="more"></a></p><h2 id="固定结构代码"><a href="#固定结构代码" class="headerlink" title="固定结构代码"></a>固定结构代码</h2><ul><li>快速生成：！+ enter</li><li>结构：根节点html（设置语言），头节点head（包含meta，link,style等标签元素）<br>，网页主体body。</li></ul><h2 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h2><ul><li>分类：单标签，双标签</li><li>关系：嵌套，并列</li></ul><h2 id="相关标签"><a href="#相关标签" class="headerlink" title="相关标签"></a>相关标签</h2><ul><li>h系列标签，标题，h1一般只有一个，用于主页中最重要的一个logo。</li><li>段落p，独自成行</li><li>水平线hr，换行br</li><li>布局标签div，块元素，独自成行，span，行内元素，一行显示多个</li><li>文本格式化：粗斜下划线删除线，strong，em，ins，del</li><li>图片：img，属性：src图片路径，alt，加载失败描述，title，鼠标悬浮，width，height控制大小</li><li>链接：a，属性：href=”跳转文件所在路径”(#表示空连接)，target=“_blank”保留本页面并跳转到另一页面，self不保留当前页面，当a链接较多时，可以使用base标签放在head标签中，<base target="_blank"></li><li>转义：&gt; &lt; &nbsp; 大于小于空格，其他可以查w3c手册</li></ul><h2 id="定位"><a href="#定位" class="headerlink" title="定位"></a>定位</h2><ul><li>锚点定位，a标签中 href=”#某一标签的id属性值” </li><li>跨页面定位，href=”页面路径#id的属性值”</li></ul><h2 id="列表标签"><a href="#列表标签" class="headerlink" title="列表标签"></a>列表标签</h2><ul><li>无序ul，下一级为li，li中可以嵌套其他标签</li><li>有序ol</li><li>自定义，dl，dt，dd</li><li>快速创建时可使用ul&gt;li*5；可同时创建多个</li></ul><h2 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h2><ul><li>表格的结构：标题caption&gt;h3,表头thead&gt;tr&gt;th,表格数据tbody&gt;tr&gt;td;同样，表格的td单元格中可以放其他标签元素，像链接，图片，表单控件等等。</li><li>单元格中还可以再创建表格来存放其他元素</li><li>table：属性：border，cellspacing，cellpadding，width，height，align等。</li></ul><h2 id="常用表单控件"><a href="#常用表单控件" class="headerlink" title="常用表单控件"></a>常用表单控件</h2><ul><li>表单需要放在form标签内。<br><img src="https://i.imgur.com/QfgRjns.png" alt=""></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于html的一些总结。&lt;br&gt;
    
    </summary>
    
      <category term="前端学习" scheme="http://yoursite.com/categories/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="html" scheme="http://yoursite.com/tags/html/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统命令</title>
    <link href="http://yoursite.com/2018/10/10/Linux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4/"/>
    <id>http://yoursite.com/2018/10/10/Linux系统命令/</id>
    <published>2018-10-10T03:21:45.000Z</published>
    <updated>2018-10-10T03:59:37.789Z</updated>
    
    <content type="html"><![CDATA[<p>关于Linux系统的常用命令记录。<br><a id="more"></a></p><h2 id="目录相关命令"><a href="#目录相关命令" class="headerlink" title="目录相关命令"></a>目录相关命令</h2><ul><li>mkdir：创建目录，参数-p　　当需要创建父目录时创建父目录　　-v　　每次创建一个目录返回一个信息</li><li>ls：查看目录，参数-l　　长格式显示具体属性　　-d　　只查看目录本身　　-a　　显示隐藏文件和目录　　-F　　显示结果加上对应结束符　　-r　　降序排序　　-t　　按修改时间排序　　-h　　显示占用的空间大小　　-i　　显示索引节点inode节点号　　–time-style　　格式化时间(–time-style=long-iso)</li><li>cd：切换目录</li><li>pwd：显示当前目录的绝对路径，参数-L　　显示系统变量PWD的值，为逻辑路径，默认使用该参数　　-P　　显示物理路径，链接文件会显示其指向的文件路径</li><li>mv：移动文件或目录到指定位置，移动目录时结尾不要加/</li><li>tree：以树形图形式显示目录结构，参数-a　　显示目录的所有内容　　-d　　只显示目录　　-L　　显示目录层级，可以接数字指定　　-f　　显示每个文件的完整路径　　-i　　不显示横线　　-F　　显示结果加上相应结束符</li></ul><h2 id="文件相关命令"><a href="#文件相关命令" class="headerlink" title="文件相关命令"></a>文件相关命令</h2><ul><li>cat：查看文件内容，参数-n　　显示行号，特别用法：cat  文件1 文件2 &gt; 文件3   将文件内容上下合并并重定向到新文件中</li><li>touch：创建文件和更新时间戳，参数-a　　只更新访问时间　　-m　　只更新修改时间</li><li>vim：文本编辑器，由vi扩展而来，一般用于开发，有四种工作模式且每种模式有各自的命令集。</li><li>echo：输出指定字符串，参数-n　与下一个echo的输出在同一行显示　　-e　　支持转义字符(\t,\n等)</li><li>cp：复制文件或目录，参数-r　　递归复制　　-p　　保持文件或目录的属性　　-f　　强制复制　　-d　　如果源文件是链接文件则拷贝链接文件而不是其指向的目标文件　　-a　　相当于-rpd　</li><li>rm：删除文件或目录，参数-f　　强制删除　　-r　　递归删除，用于删除目录</li><li>grep：过滤结果，Linux三剑客之一，参数-i　　不区分大小写　　-v　　排除　　-E　　过滤多个关键词　　–color　　匹配结果显示颜色　　-n　　显示行号　　-o　　只显示匹配的内容　　-B　　显示匹配行及其之前的n行　　-A　　显示匹配行及其之后的n行　　-C　　显示匹配行及其前后各n行</li><li>head：获取文件头部指定多少行的内容，参数-n　　指定需要获取的总行数，可以简写为-2,-3等，默认获取前十行</li><li>tail：获取文件尾部指定多少行的内容，参数-n　　指定需要获取的总行数，可以简写，默认获取最后十行</li><li>sed：过滤结果，Linux三剑客之一，参数-n　　取消默认输出　　-i　　修改文件　　-e　　允许多项编辑　　-r　　使用扩展的正则表达式，功能参数p　　打印(^打印首行，$打印尾行)　　s　　替换　　g　　全局</li><li>awk：过滤结果，Linux三剑客之一，参数-F　　指定分隔符(指定多个分隔符需要将多个分隔符写在方括号中在方括号后跟上+号，如指定两个分隔符空格和冒号  [ :]+)，内置变量NR　　用于显示数据行的编号，可用于过滤文件，统计目录内的文件数等</li><li>cut：分割数据，参数-d　　指定分隔符　　-f　　指定获取分割结果的哪些位置的字段　　-c　　指定获取哪些位置的字符</li><li>stat：查看文件详细属性，参数-c　　以指定的格式输出(%a：获取文件的权限并以八进制数字显示，%A：获取文件的权限并以可读的方式显示)</li><li>nl：查看文件时显示行号</li><li>less：分页显示文件内容，参数-N　　显示行号</li><li>more：分页显示内容，无法向前翻页</li><li>chattr：设置文件或目录的属性，参数-i　　设定文件不能被删除、改名、设定链接关系，同时不能写入或新增内容</li><li>tr：删除，替换，压缩标准输入中的字符，参数-d　　删除指定字符　　-c　　一般与-d参数配合使用删除指定字符外的所有字符　　-s　　指定字符如果重复出现则只显示一个　</li><li>rename：文件重命名</li><li>rev：将字符串的第一个字符和最后一个字符位置互换</li><li>tac：将文件内容按行上下位置互换</li><li>split：用于切割文件内容，命令格式split 被切割的文件 新文件的文件名前缀，参数-l 　　指定文件切割的行数　　 -a 　　指定新文件后缀长度 　　-b 　　指定文件切割的大小(单位K、M。。。) 　　-d 　　使用数字后缀</li><li>paste：用于将两个文件的内容合并，命令格式paste 文件1 文件2，将文件左右合并，参数-d 　　指定文件内容之间的分隔符 -　　s　　 一行内容以一列的形式显示</li><li>dos2unix：将Windows文件格式转换为unix文件格式</li><li>diff：比较文件或目录的不同，结果中a为add c为change d为delete</li></ul><h2 id="网络相关命令"><a href="#网络相关命令" class="headerlink" title="网络相关命令"></a>网络相关命令</h2><ul><li>ifconfig：查看服务器网卡信息，参数-a　　显示所有网卡信息</li><li>ifdown：禁用指定网卡</li><li>ifup：启用指定网卡</li><li>ip：一般用于查看网卡和路由信息，用法ip  addr查看网卡信息　　ip  route查看路由信息</li><li>netstat：查看端口状态，参数-a　　显示所有开启的端口　　-n　　以数字形式显示　　-t　　只显示TCP连接　　-p　　显示进程名　　-l　　只显示监听状态的端口</li><li>ping：检测与指定IP间是否连通，参数-t　　Ping 指定的IP直到中断　　-n　　指定ping包个数　　-a　　将IP解析为主机名　　-l　　指定ping包大小，最大值是65,527字节　　-w　　ping的超时时间</li><li>mtr：路由追踪，参数-s　　指定数据包大小　　-n　　不做域名解析　　-a　　指定发送数据包的IP(服务器有多个IP时使用)　　-i　　指定ping的时间间隔，单位秒　</li><li>telnet：检测远程主机端口连通性，用法telnet　　IP　　端口</li><li>nmap：检测远程主机端口开放情况，用法namp　　ip</li><li>route：查询与设置路由信息，参数-n　　以数字形式显示　　-f　　清空路由　　add　　添加路由信息　　del　　删除路由信息　　-net　　目标网络　　-host　　目标主机　　netmask　　目标网络的子网掩码　　dev　　指定路由生效的网卡　　default　　默认路由　　gw　　设置网关　　　reject　　屏蔽的路由</li></ul><h2 id="用户与用户组相关命令"><a href="#用户与用户组相关命令" class="headerlink" title="用户与用户组相关命令"></a>用户与用户组相关命令</h2><ul><li>useradd：新建用户账号，参数-c　　添加注释信息　　-U　　指定用户组　　-u　　指定UID　　-d　　指定家目录　　-e　　指定账号失效时间　　-g　　指定GID　　-G　　指定附加组　　-M　　不创建家目录　　-s　　指定shell环境</li><li>userdel：删除账号，参数-r　　删除该账号下所有文件和信息</li><li>usermod：修改账号属性，参数-l　　修改用户名　　-L　　锁定账号　　-U　　解锁账号</li><li>chage：修改密码有效期，参数-l　　列出密码有效期信息　　-E　　指定密码失效时间　　-I　　密码失效后多久锁定账号　　-m　　密码最短多久需要修改　　-M　　密码最长多久需要修改　　-m　　用户家目录不存在就创建</li><li>passwd：设置修改密码，参数–stdin　　从标准输入获取密码　　-d　　清除用户密码　　-l　　锁定账号　　-u　　解锁账号　　-S　　查看账号状态</li><li>id：查看用户ID</li><li>su：切换用户，用法su  -  用户名，不加用户名默认切换到root,参数-　 切换系统环境变量到当前用户下　　-c　　以指定用户身份执行单条命令</li><li>chmod：修改文件或目录的权限设置，参数-R　　权限设置对目录下所有文件和子目录生效</li><li>chown：修改文件或目录的属主</li><li>chgrp：修改文件或目录的属组</li><li>groupadd：增加用户组，参数-g　　指定GID</li><li>groupdel：删除用户组</li></ul><h2 id="打包压缩相关命令"><a href="#打包压缩相关命令" class="headerlink" title="打包压缩相关命令"></a>打包压缩相关命令</h2><ul><li>tar：打包命令，参数–exclude　　打包时排除指定文件　　-c　　创建归档文件　　-x　　解开归档文件　　-C　　指定释放目录　　-f　　使用归档文件　　-z　　使用gzip压缩和解压　　-j　　使用bzip2压缩和解压　　-v　　显示打包和解包的过程　　-t　　查看归档文件中包含哪些文件</li></ul><h2 id="搜索操作相关命令"><a href="#搜索操作相关命令" class="headerlink" title="搜索操作相关命令"></a>搜索操作相关命令</h2><ul><li>find：查询命令，命令格式find  查找的目标路径  参数  -exec  命令  {}\;，参数-type　　按类型查找，f文件,d目录,c字符,l符号链接,sSocket文件,b块设备　　-mtime　　按修改时间查找+n表示多少天之前修改的，n表示第几天修改的，-n表示多少天内修改的　　-！　　取反(需写在取反条件之前，如-！ -name ‘test.txt’)　　-name　　按名字查找　　-size　　文件大小(+nM|K　　大于nM，n　　等于nM,-n　　小于nM)　　-a　　表示左右条件同时满足　　-o　　表示左右条件满足一个即可　　-exec　　用于对查询结果做进一步处理 　　-delete　　删除，详细参数说明<a href="http://man.linuxde.net/find" target="_blank" rel="noopener">http://man.linuxde.net/find</a></li><li>which：查询扩展命令的绝对路径</li></ul><h2 id="命令帮助相关命令"><a href="#命令帮助相关命令" class="headerlink" title="命令帮助相关命令"></a>命令帮助相关命令</h2><ul><li>man：查询扩展命令的帮助信息</li><li>help：查询内置命令的帮助信息　</li></ul><h2 id="服务器信息相关命令"><a href="#服务器信息相关命令" class="headerlink" title="服务器信息相关命令"></a>服务器信息相关命令</h2><ul><li>top：查看系统实时运行状态信息，参数-H　　显示线程信息　　-p　　显示指定进程号的进程信息　　-u　　显示指定用户的进程信息　　-i　　指定刷新时间间隔，功能选项k　　杀死指定进程号的进程　　q　　退出</li><li>strace：跟踪显示进程的系统调用信息，参数-p　　显示指定进程的系统调用信息</li><li>uname：显示系统信息，参数-a　　显示所有信息　　-m　　显示操作系统位数　　-n　　显示主机名　　-r　　显示内核版本　　-s　　显示操作系统名称　　-p　　显示处理器位数</li><li>whoami：查看当前用户用户名</li><li>who：查看所有登陆用户的登陆信息</li><li>hostname：查看和临时设置主机名</li><li>runlevel：查看系统运行级别</li><li>dmesg：查看系统故障信息</li><li>env：查看系统环境变量</li><li>history：显示命令行历史记录，参数-c　　清空历史记录　　-d　　删除指定编号的命令行历史</li><li>hwclock：查看设置硬件时间，参数-w　　与系统时间同步　　-r　　显示当前硬件时间</li><li>date：查看设置系统时间，参数-s　　设置系统时间　　+时间日期格式　　以指定格式显示时间，date命令详细说明</li><li>ulimit：查看和修改系统限制，参数-a　　显示系统所有限制信息　　-n　　查看或设置系统最大可用文件描述符(ulimit  -n  65535)　　</li><li>dumpe2fs：查看文件系统信息</li><li>df：查看磁盘空间以及inode节点使用信息，参数-h　　可读的格式显示大小　　-i　　显示inode信息</li><li>du：查看目录或文件占用的空间大小，参数-s　　只显示指定目录占用空间的大小总和　　-h　　以可读的方式显示大小</li><li>lsof：列出被进程调用的文件的信息，一般用于查看某个端口是否已启用，参数-i　　列出符号条件的进程(4即ipv4，6即ipv6，协议，:端口，@IP)，详细参数说明<a href="http://man.linuxde.net/lsof" target="_blank" rel="noopener">http://man.linuxde.net/lsof</a></li><li>last:显示用户最近登陆信息</li><li>lastlog：显示所有用户最近一次登陆信息</li></ul><h2 id="安全相关命令"><a href="#安全相关命令" class="headerlink" title="安全相关命令"></a>安全相关命令</h2><ul><li>sudo：使普通用户可以使用超出权限的命令，用法sudo  cmd,比如sudo  /etc/init.d/network restart，参数-l　　查看当前用户被授权执行的命令　　-v　　追踪最新的时间戳　　-u　　指定用户身份执行命令　　-k　　删除时间戳</li><li>visudo： 配置sudo权限的编辑命令，参数-c　　检查配置文件语法是否正确</li><li>getenforce：查看selinux运行级别</li><li>setenforce：设置selinux运行级别</li></ul><h2 id="进程相关命令"><a href="#进程相关命令" class="headerlink" title="进程相关命令"></a>进程相关命令</h2><ul><li>ps：查看当前服务器运行中的进程，参数-e　　显示系统所有进程　　-f　　以完整格式显示　　-a　　显示当前终端下的所有进程　　-u　　以用户为主的格式显示　　-x　　显示当前用户在所有终端下的进程</li></ul><h2 id="其他命令"><a href="#其他命令" class="headerlink" title="其他命令"></a>其他命令</h2><ul><li>xargs：用于从标准输入获取内容，并将该内容作为后面命令的参数，主要用于将前面命令产生的多行结果合并为一行再让其后的命令进行处理，参数-n　　表示分组数，没有该参数默认在一行显示　　-i　　将标准输入的内容与{}关联起来</li><li>seq：生成数字序列，命令格式seq  参数  起始数字  步长  结束数字，参数-s　　指定分隔符，默认是换行符</li><li>sort：排序命令，参数-r　　降序排序　　-n　　按数值排序　　-k　　指定按第几列的数据排序　　-u　　排除所有重复行　　-t　　指定每列的分隔符</li><li>uniq：报告或排除重复数据，参数-c　　统计数据行重复次数　　-d　　只显示重复的数据行　　-u　　显示只出现一次的数据行</li><li>alias：查询和设置命令别名，命令格式alias  别名=‘命令’，直接执行alias可查询所有已设置的别名</li><li>unalias：取消别名，命令格式unalias  别名</li><li>rz：上传文件，该命令由包lrzsz提供</li><li>sz：下载文件，该命令由包lrzsz提供</li><li>init：修改系统运行级别</li><li>shutdown：关闭与重启系统，参数-h　　关机　　-r　　重启</li><li>halt：关机</li><li>reboot：重启</li><li>chkconfig：查看与设置开机启动服务，参数–list　　显示指定服务所有级别下的自启动设置，不加服务名显示所有服务　　–add　　将指定服务添加到自启动管理　　–del　　将指定服务从自启动管理中删除　　–level　　指定服务在哪些运行级别自启动，比如chkconfig  –level  35 mysqld on,该命令行不加–level参数表示mysqld服务在所有运行级别自启动</li><li>source：用于执行shell程序，shell程序中新建和修改的变量会影响当前shell环境，可以用.代替source</li><li>sh：用于执行shell程序，新建子shell环境并执行shell程序，shell程序中新建和修改的变量不影响当前shell环境</li><li>export：修改环境变量</li><li>mount：挂载设备，参数-t　　设备类型(iso9660：光盘或镜像，vfat：fat32设备，ntfs：ntfs设备，nfs：unix网络共享，cifs：Windows网络共享)　　-o　　挂载方式(loop：将文件作为磁盘分区，ro：只读，rw：读写，iocharset：指定访问文件系统所用字符集,例如iocharset=utf8，remount：重新挂载)</li><li>umount：取消挂载</li><li>ntpdate：网络时间同步，参数-u　　越过防火墙与时间同步服务器进行时间同步(ntp.api.bz，上海的时间同步服务器)</li><li>basename：获取路径中最后一个路径分隔符后的内容</li><li>dirname：获取路径中最后一个路径分隔符前的内容</li><li>file：查看文件类型</li><li>ln：创建链接文件，参数-s　　创建软链接</li><li>readlink：获取符号链接所指向的源文件文件名</li><li>md5sum：生成和校验md5值，参数-c　　从指定文件获取md5值并校验　　–status　　校验成功是不输出任何信息　　-w　　当校验不正确时输出警告信息</li><li>wc：参数-l 　　查看行数 　　-L 　　查看最长一行的字符数</li><li>tee：内容输出到屏幕同时重定向到指定文件中，参数-a 　　追加</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于Linux系统的常用命令记录。&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>MySQL的一些优化</title>
    <link href="http://yoursite.com/2018/09/16/MySQL%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BC%98%E5%8C%96/"/>
    <id>http://yoursite.com/2018/09/16/MySQL的一些优化/</id>
    <published>2018-09-16T10:46:55.000Z</published>
    <updated>2018-09-16T11:40:22.495Z</updated>
    
    <content type="html"><![CDATA[<p>MySQL单表记录数过大时，增删改查性能都会急剧下降，我们可以采取一些措施来优化其性能。<br><a id="more"></a>  </p><h2 id="单表优化"><a href="#单表优化" class="headerlink" title="单表优化"></a>单表优化</h2><p>除非单表数据未来会一直不断上涨，否则不要一开始就考虑拆分，拆分会带来逻辑、部署、运维的各种复杂度，一般以整型值为主的表在千万级以下，字符串为主的表在五百万以下是没有太大问题的。而事实上很多时候MySQL单表的性能依然有不少优化空间，甚至能正常支撑千万级以上的数据量.</p><h4 id="字段"><a href="#字段" class="headerlink" title="字段"></a>字段</h4><ul><li>尽量使用TINYINT、SMALLINT、MEDIUM_INT作为整数类型而非INT，如果非负则加上UNSIGNED</li><li>VARCHAR的长度只分配真正需要的空间</li><li>使用枚举或整数代替字符串类型</li><li>尽量使用TIMESTAMP而非DATETIME，</li><li>单表不要有太多字段，建议在20以内</li><li>避免使用NULL字段，很难查询优化且占用额外索引空间</li><li>用整型来存IP<h4 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h4></li></ul><p>创建索引<br>这是最基本的索引，它没有任何限制。它有以下几种创建方式：</p><blockquote><pre><code>CREATE INDEX indexName ON mytable(username(length)); 如果是CHAR，VARCHAR类型，length可以小于字段实际长度；如果是BLOB和TEXT类型，必须指定 length。</code></pre></blockquote><blockquote><pre><code>修改表结构(添加索引)ALTER table tableName ADD INDEX indexName(columnName)</code></pre></blockquote><blockquote><pre><code>创建表的时候直接指定CREATE TABLE mytable(       ID INT NOT NULL,        username VARCHAR(16) NOT NULL,       INDEX [indexName] (username(length))      );  删除索引的语法DROP INDEX [indexName] ON mytable; </code></pre></blockquote><ul><li>索引并不是越多越好，要根据查询有针对性的创建，考虑在WHERE和ORDER BY命令上涉及的列建立索引，可根据EXPLAIN来查看是否用了索引还是全表扫描</li><li>应尽量避免在WHERE子句中对字段进行NULL值判断，否则将导致引擎放弃使用索引而进行全表扫</li><li>值分布很稀少的字段不适合建索引，例如”性别”这种只有两三个值的字段</li><li>字符字段只建前缀索引</li><li>字符字段最好不要做主键</li><li>不用外键，由程序保证约束</li><li>尽量不用UNIQUE，由程序保证约束</li><li>使用多列索引时主意顺序和查询条件保持一致，同时删除不必要的单列索引</li></ul><h4 id="查询SQL"><a href="#查询SQL" class="headerlink" title="查询SQL"></a>查询SQL</h4><ul><li>可通过开启慢查询日志来找出较慢的SQL</li><li>不做列运算：SELECT id WHERE age + 1 = 10，任何对列的操作都将导致表扫描，它包括数据库教程函数、计算表达式等等，查询时要尽可能将操作移至等号右边</li><li>sql语句尽可能简单：一条sql只能在一个cpu运算；大语句拆小语句，减少锁时间；一条大sql可以堵死整个库</li><li>不用SELECT *</li><li>OR改写成IN：OR的效率是n级别，IN的效率是log(n)级别，in的个数建议控制在200以内</li><li>不用函数和触发器，在应用程序实xian</li><li>避免%xxx式查询</li><li>少用JOIN</li><li>使用同类型进行比较，比如用’123’和’123’比，123和123比</li><li>尽量避免在WHERE子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描</li><li>对于连续数值，使用BETWEEN不用IN：SELECT id FROM t WHERE num BETWEEN 1 AND 5</li><li>列表数据不要拿全表，要使用LIMIT来分页，每页数量也不要太大</li></ul><h4 id="引擎"><a href="#引擎" class="headerlink" title="引擎"></a>引擎</h4><p>目前广泛使用的是MyISAM和InnoDB两种引擎：</p><p>MyISAM</p><p>MyISAM引擎是MySQL 5.1及之前版本的默认引擎，它的特点是：</p><ul><li>不支持行锁，读取时对需要读到的所有表加锁，写入时则对表加排它锁</li><li>不支持事务</li><li>不支持外键</li><li>不支持崩溃后的安全恢复</li><li>在表有读取查询的同时，支持往表中插入新纪录</li><li>支持BLOB和TEXT的前500个字符索引，支持全文索引</li><li>支持延迟更新索引，极大提升写入性能</li><li>对于不会进行修改的表，支持压缩表，极大减少磁盘空间占用</li></ul><p>InnoDB</p><p>InnoDB在MySQL 5.5后成为默认索引，它的特点是：</p><ul><li>支持行锁，采用MVCC来支持高并发</li><li>支持事务</li><li>支持外键</li><li>支持崩溃后的安全恢复</li><li>不支持全文索引</li></ul><p>总体来讲，MyISAM适合SELECT密集型的表，而InnoDB适合INSERT和UPDATE密集型的表</p><h2 id="读写分离"><a href="#读写分离" class="headerlink" title="读写分离"></a>读写分离</h2><p>也是目前常用的优化，从库读主库写，一般不要采用双主或多主引入很多复杂性，尽量采用文中的其他方案来提高性能。同时目前很多拆分的解决方案同时也兼顾考虑了读写分离.  </p><h2 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h2><p>缓存可以发生在这些层次：</p><ul><li>MySQL内部：在系统调优参数介绍了相关设置</li><li>数据访问层：比如MyBatis针对SQL语句做缓存，而Hibernate可以精确到单个记录，这里缓存的对象主要是持久化对象Persistence Object</li><li>应用服务层：这里可以通过编程手段对缓存做到更精准的控制和更多的实现策略，这里缓存的对<br>是数据传输对象Data Transfer Object</li><li>Web层：针对web页面做缓存</li><li>浏览器客户端：用户端的缓存</li></ul><p>可以根据实际情况在一个层次或多个层次结合加入缓存。这里重点介绍下服务层的缓存实现，目前主要有两种方式：</p><p>直写式（Write Through）：在数据写入数据库后，同时更新缓存，维持数据库与缓存的一致性。这也是当前大多数应用缓存框架如Spring Cache的工作方式。这种实现非常简单，同步好，但效率一般。<br>回写式（Write Back）：当有数据要写入数据库时，只会更新缓存，然后异步批量的将缓存数据同步到数据库上。这种实现比较复杂，需要较多的应用逻辑，同时可能会产生数据库与缓存的不同步，但效率非常高。</p><h2 id="表分区"><a href="#表分区" class="headerlink" title="表分区"></a>表分区</h2><p>MySQL在5.1版引入的分区是一种简单的水平拆分，用户需要在建表的时候加上分区参数，对应用是透明的无需修改代码</p><p>对用户来说，分区表是一个独立的逻辑表，但是底层由多个物理子表组成，实现分区的代码实际上是通过对一组底层表的对象封装，但对SQL层来说是一个完全封装底层的黑盒子。MySQL实现分区的方式也意味着索引也是按照分区的子表定义，没有全局索引</p><p><img src="https://i.imgur.com/4UenpUK.jpg" alt=""></p><p>用户的SQL语句是需要针对分区表做优化，SQL条件中要带上分区条件的列，从而使查询定位到少量的分区上，否则就会扫描全部分区，可以通过EXPLAIN PARTITIONS来查看某条SQL语句会落在那些分区上，从而进行SQL优化，如下图5条记录落在两个分区上</p><p><img src="https://i.imgur.com/cRydn4u.png" alt=""></p><p>分区的好处是：</p><ul><li>可以让单表存储更多的数据</li><li>分区表的数据更容易维护，可以通过清楚整个分区批量删除大量数据，也可以增加新的分区来支持新插入的数据。另外，还可以对一个独立分区进行优化、检查、修复等操作</li><li>部分查询能够从查询条件确定只落在少数分区上，速度会很快</li><li>分区表的数据还可以分布在不同的物理设备上，从而高效利用多个硬件设备</li><li>使用分区表赖避免某些特殊瓶颈，例如InnoDB单个索引的互斥访问、ext3文件系统的inode锁竞争</li><li>可以备份和恢复单个分区</li></ul><p>分区的限制和缺点：</p><ul><li>一个表最多只能有1024个分区</li><li>如果分区字段中有主键或者唯一索引的列，那么所有主键列和唯一索引列都必须包含进来</li><li>分区表无法使用外键约束</li><li>NULL值会使分区过滤无效</li><li>所有分区必须使用相同的存储引擎</li></ul><p>分区的类型：</p><ul><li>RANGE分区：基于属于一个给定连续区间的列值，把多行分配给分区</li><li>LIST分区：类似于按RANGE分区，区别在于LIST分区是基于列值匹配一个离散值集合中的某个值来进行选择</li><li>HASH分区：基于用户定义的表达式的返回值来进行选择的分区，该表达式使用将要插入到表中的这些行的列值进行计算。这个函数可以包含MySQL中有效的、产生非负整数值的任何表达式</li><li>KEY分区：类似于按HASH分区，区别在于KEY分区只支持计算一列或多列，且MySQL服务器提供其自身的哈希函数。必须有一列或多列包含整数值</li></ul><p>分区适合的场景有：</p><p>最适合的场景数据的时间序列性比较强，则可以按时间来分区，如下所示：</p><p><img src="https://i.imgur.com/49ATKFU.png" alt=""></p><p>查询时加上时间范围条件效率会非常高。</p><h2 id="垂直拆分"><a href="#垂直拆分" class="headerlink" title="垂直拆分"></a>垂直拆分</h2><p>垂直分库是根据数据库里面的数据表的相关性进行拆分，比如：一个数据库里面既存在用户数据，又存在订单数据，那么垂直拆分可以把用户数据放到用户库、把订单数据放到订单库。垂直分表是对数据表进行垂直拆分的一种方式，常见的是把一个多字段的大表按常用字段和非常用字段进行拆分，每个表里面的数据记录数一般情况下是相同的，只是字段不一样，使用主键关联</p><p>比如原始的用户表是：</p><p><img src="https://i.imgur.com/tANr4Cp.jpg" alt=""></p><p>垂直拆分后是：</p><p><img src="https://i.imgur.com/DxH5uvc.jpg" alt=""></p><p>垂直拆分的优点是：</p><ul><li>可以使得行数据变小，一个数据块(Block)就能存放更多的数据，在查询时就会减少I/O次数(每次查询时读取的Block 就少)</li><li>可以达到最大化利用Cache的目的，具体在垂直拆分的时候可以将不常变的字段放一起，将经常改变的放一起</li><li>数据维护简单</li></ul><p>缺点是：</p><ul><li>主键出现冗余，需要管理冗余列</li><li>会引起表连接JOIN操作（增加CPU开销）可以通过在业务服务器上进行join来减少数据库压力</li><li>依然存在单表数据量过大的问题（需要水平拆分）</li><li>事务处理复杂</li></ul><h2 id="水平拆分"><a href="#水平拆分" class="headerlink" title="水平拆分"></a>水平拆分</h2><p>水平拆分是通过某种策略将数据分片来存储，分库内分表和分库两部分，每片数据会分散到不同的MySQL表或库，达到分布式的效果，能够支持非常大的数据量。前面的表分区本质上也是一种特殊的库内分表。  </p><p>库内分表，仅仅是单纯的解决了单一表数据过大的问题，由于没有把表的数据分布到不同的机器上，因此对于减轻MySQL服务器的压力来说，并没有太大的作用，大家还是竞争同一个物理机上的IO、CPU、网络，这个就要通过分库来解决</p><p>前面垂直拆分的用户表如果进行水平拆分，结果是：  </p><p><img src="https://i.imgur.com/Uw3IfpV.jpg" alt=""></p><p>实际情况中往往会是垂直拆分和水平拆分的结合，即将Users_A_M和Users_N_Z再拆成Users和UserExtras，这样一共四张表。</p><p>水平拆分的优点是:</p><ul><li>不存在单库大数据和高并发的性能瓶颈</li><li>应用端改造较少</li><li>提高了系统的稳定性和负载能力</li></ul><p>缺点是：</p><ul><li>分片事务一致性难以解决</li><li>跨节点Join性能差，逻辑复杂</li><li>数据多次扩展难度跟维护量极大</li></ul><h4 id="分片原则"><a href="#分片原则" class="headerlink" title="分片原则"></a>分片原则</h4><ul><li>能不分就不分，参考单表优化</li><li>分片数量尽量少，分片尽量均匀分布在多个数据结点上，因为一个查询SQL跨分片越多，则总体性能越差，虽然要好于所有数据在一个分片的结果，只在必要的时候进行扩容，增加分片数量</li><li>分片规则需要慎重选择做好提前规划，分片规则的选择，需要考虑数据的增长模式，数据的访问模式，分片关联性问题，以及分片扩容问题，最近的分片策略为范围分片，枚举分片，一致性Hash分片，这几种分片都有利于扩容</li><li>尽量不要在一个事务中的SQL跨越多个分片，分布式事务一直是个不好处理的问题</li><li>查询条件尽量优化，尽量避免Select * 的方式，大量数据结果集下，会消耗大量带宽和CPU资源，查询尽量避免返回大量结果集，并且尽量为频繁使用的查询语句建立索引。</li><li>通过数据冗余和表分区赖降低跨库Join的可能</li></ul><p>这里特别强调一下分片规则的选择问题，如果某个表的数据有明显的时间特征，比如订单、交易记录等，则他们通常比较合适用时间范围分片，因为具有时效性的数据，我们往往关注其近期的数据，查询条件中往往带有时间字段进行过滤，比较好的方案是，当前活跃的数据，采用跨度比较短的时间段进行分片，而历史性的数据，则采用比较长的跨度存储。</p><p>总体上来说，分片的选择是取决于最频繁的查询SQL的条件，因为不带任何Where语句的查询SQL，会遍历所有的分片，性能相对最差，因此这种SQL越多，对系统的影响越大，所以我们要尽量避免这种SQL的产生。</p><h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>由于水平拆分牵涉的逻辑比较复杂，当前也有了不少比较成熟的解决方案。这些方案分为两大类：客户端架构和代理架构。</p><p><strong>客户端架构</strong></p><p>通过修改数据访问层，如JDBC、Data Source、MyBatis，通过配置来管理多个数据源，直连数据库，并在模块内完成数据的分片整合，一般以Jar包的方式呈现</p><p>这是一个客户端架构的例子：</p><p><img src="https://i.imgur.com/Agf5sLW.jpg" alt=""></p><p>可以看到分片的实现是和应用服务器在一起的，通过修改Spring JDBC层来实现</p><p>客户端架构的优点是：</p><ul><li>应用直连数据库，降低外围系统依赖所带来的宕机风险</li><li>集成成本低，无需额外运维的组件</li></ul><p>缺点是：</p><ul><li>限于只能在数据库访问层上做文章，扩展性一般，对于比较复杂的系统可能会力不从心</li><li>将分片逻辑的压力放在应用服务器上，造成额外风险</li></ul><p><strong>代理架构</strong></p><p>通过独立的中间件来统一管理所有数据源和数据分片整合，后端数据库集群对前端应用程序透明，需要独立部署和运维代理组件</p><p>这是一个代理架构的例子：</p><p><img src="https://i.imgur.com/3qo732p.jpg" alt=""></p><p>代理组件为了分流和防止单点，一般以集群形式存在，同时可能需要Zookeeper之类的服务组件来管理</p><p>代理架构的优点是：</p><ul><li>能够处理非常复杂的需求，不受数据库访问层原来实现的限制，扩展性强</li><li>对于应用服务器透明且没有增加任何额外负载</li></ul><p>缺点是：</p><ul><li>需部署和运维独立的代理中间件，成本高</li><li>应用需经过代理来连接数据库，网络上多了一跳，性能有损失且有额外风险</li></ul><p>各方案比较：  </p><p><img src="https://i.imgur.com/lF4mQje.jpg" alt=""></p><p>如此多的方案，如何进行选择？可以按以下思路来考虑：</p><ul><li>确定是使用代理架构还是客户端架构。中小型规模或是比较简单的场景倾向于选择客户端架构，复杂场景或大规模系统倾向选择代理架构</li><li>具体功能是否满足，比如需要跨节点ORDER BY，那么支持该功能的优先考虑</li><li>不考虑一年内没有更新的产品，说明开发停滞，甚至无人维护和技术支持</li><li>最好按大公司-&gt;社区-&gt;小公司-&gt;个人这样的出品方顺序来选择</li><li>选择口碑较好的，比如github星数、使用者数量质量和使用者反馈</li><li>开源的优先，往往项目有特殊需求可能需要改动源代码</li></ul><p>按照上述思路，推荐以下选择：</p><ul><li>客户端架构：ShardingJDBC</li><li>代理架构：MyCat或者Atlas</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MySQL单表记录数过大时，增删改查性能都会急剧下降，我们可以采取一些措施来优化其性能。&lt;br&gt;
    
    </summary>
    
      <category term="数据库" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="MySQL" scheme="http://yoursite.com/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy抓取装修图片</title>
    <link href="http://yoursite.com/2018/07/26/Scrapy%E6%8A%93%E5%8F%96%E8%A3%85%E4%BF%AE%E5%9B%BE%E7%89%87/"/>
    <id>http://yoursite.com/2018/07/26/Scrapy抓取装修图片/</id>
    <published>2018-07-26T12:35:00.000Z</published>
    <updated>2018-09-01T16:08:34.191Z</updated>
    
    <content type="html"><![CDATA[<p>scrapy框架也学了几天了，这次来试一下抓取360的家居装修图，加强对框架的理解和掌握。<br><a id="more"></a>  </p><h1 id="抓取分析"><a href="#抓取分析" class="headerlink" title="抓取分析"></a>抓取分析</h1><h2 id="网页分析"><a href="#网页分析" class="headerlink" title="网页分析"></a>网页分析</h2><p>目标站点定位： <a href="http://image.so.com/" target="_blank" rel="noopener">http://image.so.com/</a><br>打开浏览器后，我们在发现图片是通过发送Ajax请求加载的，所以我们直接找到对应的请求接口，然后访问，获取里面的json数据，数据中包含的图片的链接地址和其他信息，另外，我们拿到图片链接后，再去请求，便可以下载图片到本地。基本思路就是这样了。下面开始做吧。<br><img src="https://i.imgur.com/ZsIbFIH.png" alt=""></p><h1 id="url分析"><a href="#url分析" class="headerlink" title="url分析"></a>url分析</h1><p>寻找url规律：<br><img src="https://i.imgur.com/uA6x16d.png" alt=""></p><p>sn为偏移量，每页会显示30张图片，其他参数不变，那么我们可以构造一个参数字典，然后通过urlencode方法转成URL的格式，这样就构造出了需要访问的url。</p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h1><p>首先，创建项目，并生成一个Spider。</p><h2 id="编写items文件，确定抓取目标"><a href="#编写items文件，确定抓取目标" class="headerlink" title="编写items文件，确定抓取目标"></a>编写items文件，确定抓取目标</h2><p><img src="https://i.imgur.com/qvDOYkY.png" alt=""><br>这里，我们打算将图片信息放到mongodb和mysql中，先在这里定义集合和表名称，等会方便插入数据。  </p><h2 id="编写spider文件，生成数据和请求。"><a href="#编写spider文件，生成数据和请求。" class="headerlink" title="编写spider文件，生成数据和请求。"></a>编写spider文件，生成数据和请求。</h2><p><img src="https://i.imgur.com/REXVhXc.png" alt=""><br>这里，我们只要遍历json文件中的list字段就可以获取相应的信息。</p><h2 id="编写管道文件Pipeline，处理数据"><a href="#编写管道文件Pipeline，处理数据" class="headerlink" title="编写管道文件Pipeline，处理数据"></a>编写管道文件Pipeline，处理数据</h2><ul><li><p>存储到MongoDB<br><img src="https://i.imgur.com/foSb5dd.png" alt=""></p></li><li><p>存储到mysql<br>这里我们先提前在mysql中创建好相应的数据库和表，用来保存接下来的信息，这里就不多讲了。<br><img src="https://i.imgur.com/MWervfP.png" alt="">  </p></li><li><p>存储图片到本地<br>这里需要用到Scrapy专门处理图片的Pipeline，我们先定义一个当前目录下的文件存储路径 <strong>IMAGES_STORE = ‘.\images’</strong>,之后我们下载的图片会保存在这里。<br><img src="https://i.imgur.com/yJ1Fvgo.png" alt=""></p></li><li><p>setting文件配置<br>接下来我们需要配置setting.py里面的信息了。<br>数据库信息，设置最大抓取页数，请求头啥的。<br><img src="https://i.imgur.com/ZsksNAZ.png" alt=""></p></li></ul><p>管道文件<br><img src="https://i.imgur.com/xxEIa6B.png" alt=""></p><h2 id="运行程序"><a href="#运行程序" class="headerlink" title="运行程序"></a>运行程序</h2><p><img src="https://i.imgur.com/yDqIwWr.png" alt=""></p><p>这样，scrapy便开始下载图片和图片信息了，刚开始出现了几次报错，有参数的类型错误，还有初始化Mysql类属性时，命名造成了干涉等，导致后面方法报错。  </p><p>下面效果展示：<br>MONGO存储：<br><img src="https://i.imgur.com/au1ZCLy.png" alt=""></p><p>MySQL存储：<br><img src="https://i.imgur.com/NtIm5Me.png" alt=""></p><p>本地存储：<br><img src="https://i.imgur.com/AysaEQB.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;scrapy框架也学了几天了，这次来试一下抓取360的家居装修图，加强对框架的理解和掌握。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>破解验证码识别</title>
    <link href="http://yoursite.com/2018/07/24/%E7%A0%B4%E8%A7%A3%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB/"/>
    <id>http://yoursite.com/2018/07/24/破解验证码识别/</id>
    <published>2018-07-24T02:49:58.000Z</published>
    <updated>2018-09-01T16:08:17.448Z</updated>
    
    <content type="html"><![CDATA[<p>现在，很多网站采取各种各样的措施来反爬虫，其中之一就是使用验证码。当我们访问网页时，必须先通过验证码才能够访问页面。下面我们便来讲2种验证码的识别方式和一些思路。当然我们也可以直接使用付费的打码平台，那样可以增加识别的准确度，毕竟出了钱的嘛。哈哈！<br><a id="more"></a>  </p><h2 id="PIL库"><a href="#PIL库" class="headerlink" title="PIL库"></a>PIL库</h2><p>其实，验证码识别归根到底还是对各种各样图片的识别和操作，python中有很对图像处理的库，其中PIL就是其中之一。 所以在处理验证码识别之前，必须先了解PIL库。 下面附上其API源码地址，以及对应的学习博客。<br>源码地址：<a href="https://pillow-cn.readthedocs.io/zh_CN/latest/reference/index.html" target="_blank" rel="noopener">https://pillow-cn.readthedocs.io/zh_CN/latest/reference/index.html</a><br>参考博客：<a href="https://blog.csdn.net/louishao/article/details/69879981" target="_blank" rel="noopener">https://blog.csdn.net/louishao/article/details/69879981</a><br>下面我们就开始验证码识别之路了。</p><h2 id="图形验证码"><a href="#图形验证码" class="headerlink" title="图形验证码"></a>图形验证码</h2><p>以中国知网为例：<br><img src="https://i.imgur.com/KfnZxWP.png" alt=""></p><p>首先，我们先拿到上图中绿线标记的验证码，下载到本地项目文件中，<br>然后，编写如下代码:<br><img src="https://i.imgur.com/Dx7xscT.png" alt=""><br>输出结果为：486V， 可是实际图片为4S6V，这是因为验证码内多余线条干扰了图片的识别，像这类情况，还需要做出额外的处理，比如转灰度，二值化等。对应的方法如下：<br><img src="https://i.imgur.com/WagZ5W3.png" alt=""></p><p>当然，实际处理中并不是这样，一般我们会先对模糊图片进行灰度处理后，再设定二值化的阈值，实际处理如下。<br><img src="https://i.imgur.com/li0queT.png" alt=""></p><p>输出结果：4S6V<br>进行识别时，先设定好二值化阈值threshold，进行适当调试，直到图片能正常识别为止。</p><h2 id="滑动验证码"><a href="#滑动验证码" class="headerlink" title="滑动验证码"></a>滑动验证码</h2><p><strong>过程分析：</strong></p><p>滑动验证码主要的验证方式是拖动滑块，拼合图像；如图象完全拼合，则验证成功，即表单提交成功，否则需要重新验证。<br>如图：<br><img src="https://i.imgur.com/pYD8C6K.png" alt=""></p><p>下面，我们就以极验的验证码为例，来讲诉一下识别方法。<br>因为极验的验证码在拖动验证码后会生成一个加密的表单提交到后台，所有为了避免麻烦我们直接用selenium模拟浏览器行为来完成验证。<br>登陆网站：极验官网</p><blockquote><pre><code>目标站点：https://account.geetest.com/login</code></pre></blockquote><p><img src="https://i.imgur.com/sVfDi03.png" alt=""></p><p>首先，我们发现登陆界面有个智能按钮，一般来说，在输入邮箱之后，点击按钮就会弹出滑动验证窗口，然后我们在拖动验证码完成图像拼接，完成验证。<br>弹出滑块验证：<br><img src="https://i.imgur.com/u1iul7H.png" alt=""><br>拖动滑块，完成验证：<br><img src="https://i.imgur.com/dKjUoEK.png" alt=""></p><p>所以，滑块验证识别需要完成以下步骤：</p><blockquote><pre><code>1. 模拟点击验证按钮2. 识别滑块的缺口位置3. 模拟拖动滑块</code></pre></blockquote><p><strong>如何实现以上步骤呢？我们先需要将任务进行分解，看似只有三大步骤，其实里面坑还有很多的，稍后会做解释。</strong>  </p><p>第一步，输入账号，获取智能按钮，使用selenium模拟点击，获取带有缺口的图片。  </p><p>第二步，获取上面缺口图片中的完整图片。这里有个地方要注意，正常情况下我们在网页源代码里是找不到完整图的，因为它被隐藏了，必须执行javascript语句才能出现完整图。<br><img src="https://i.imgur.com/v8ouv0P.png" alt=""></p><p>我们将display参数改为block，opacity参数改为1，然后进行截图，就可以拿到完整的验证码图片了。</p><p>第三步，对比两张图片的所有RGB像素点，得到缺口位置。</p><p>第四步，模拟人的拖动习惯，这里也有坑，极验的验证码增加了机器轨迹识别，匀速移动，或者随机速度移动滑块都不能通过验证，所以我们将需要拖动的总位移分成一段一段小的轨迹，先匀加速拖动后匀减速拖动。</p><p>第五步，按照规定轨迹进行拖动，完成验证。</p><p>第六步，完成账号登陆。</p><p>过程分析完了，下面我们就来写代码试一下：<br>首先，我们先将整个代码的一个逻辑思路做一个大致的概括吧。<br><img src="https://i.imgur.com/Ayg9jgg.png" alt="">  </p><p>接下来，我们便来逐一完成main函数里要实现的功能了。</p><p><strong>代码示例：</strong><br><img src="https://i.imgur.com/vCAntIv.png" alt=""><br><img src="https://i.imgur.com/p7giBb3.png" alt=""><br><img src="https://i.imgur.com/EC60eS5.png" alt=""></p><p>通过以上代码我们便拿到了完整的验证码和带有缺口的验证码。<br>缺口图片：<br><img src="https://i.imgur.com/YHzDimh.png" alt="">  </p><p>完整图片：<br><img src="https://i.imgur.com/KNLDRsB.png" alt=""></p><p>这里我在调试的时候碰到一个坑，因为chrome中，location方法不滚动，直接返回相对整个html的坐标，我的电脑是15.6寸的，显示设置上布局的缩放大小被放大到1.25倍，导致location返回的坐标与验证码的坐标有误差。修改布局为100%后就解决了。 下面便是对比图片找出缺口位置。这里我们需要遍历图片的坐标点，获取像素点的RGB数据。  </p><p><strong>代码示例</strong><br><img src="https://i.imgur.com/TQzHllN.png" alt=""></p><p>接下来就是获取滑块的移动路径和模拟拖动行为了。<br><img src="https://i.imgur.com/Rlb2brE.png" alt=""><br><img src="https://i.imgur.com/uaEc7Jg.png" alt=""></p><p>执行代码：<br><img src="https://i.imgur.com/G8pc1GS.png" alt=""> </p><p>最后终于成功了，踩了这么多坑，终于完成了滑块验证码的破解。。。现在已经实现功能，因为还可能出现其他情况，接下来我们还需要完善一下代码，其实也可封装成一个类，测试的时候我们会发现，图片会弹出小怪兽被吃了，那是因为系统识别我们是机器行为，所以不通过，这里我们需要修改加速度参数，再增加一个回调。<br><img src="https://i.imgur.com/f9DPlNE.png" alt=""></p><p>代码执行效果：<br><img src="https://i.imgur.com/br02DBK.png" alt=""></p><p>这样我们就成功破解验证码，并登陆到网页界面了。。。忙活了一上午，吃饭去了。</p><p>源码地址：<a href="https://github.com/appleguardu/spider_projects/tree/master/Captcha" target="_blank" rel="noopener">https://github.com/appleguardu/spider_projects/tree/master/Captcha</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;现在，很多网站采取各种各样的措施来反爬虫，其中之一就是使用验证码。当我们访问网页时，必须先通过验证码才能够访问页面。下面我们便来讲2种验证码的识别方式和一些思路。当然我们也可以直接使用付费的打码平台，那样可以增加识别的准确度，毕竟出了钱的嘛。哈哈！&lt;br&gt;
    
    </summary>
    
      <category term="反爬虫" scheme="http://yoursite.com/categories/%E5%8F%8D%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="反爬虫" scheme="http://yoursite.com/tags/%E5%8F%8D%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>python之json基础</title>
    <link href="http://yoursite.com/2018/07/19/python%E4%B9%8Bjson%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/2018/07/19/python之json数据解析/</id>
    <published>2018-07-19T05:27:17.000Z</published>
    <updated>2018-09-01T16:07:40.743Z</updated>
    
    <content type="html"><![CDATA[<p>当我们爬取异步加载的网页从上面获取的往往是json类型的文件，通常我们需要对这类数据进行处理后才能使用，下面就来说说python中对json数据的基本处理方法。<br><a id="more"></a>  </p><h2 id="什么是JSON"><a href="#什么是JSON" class="headerlink" title="什么是JSON"></a>什么是JSON</h2><p>JSON 指的是 JavaScript 对象表示法（JavaScript Object Notation）；是一种轻量级的文本数据交换格式；适用于进行数据交互的场景，比如网站前台与后台之间的数据交互。  </p><p>简单来说json就是javascript中的对象和数组，所以这两种结构就是对象和数组两种结构，通过这两种结构可以表示各种复杂的结构：  </p><blockquote><pre><code>1.对象：对象在js中表示为{ }括起来的内容，数据结构为 { key：value, key：value, ... }的键值对的结构，在面向对象的语言中，key为对象的属性，value为对应的属性值，所以很容易理解，取值方法为 对象.key 获取属性值，这个属性值的类型可以是数字、字符串、数组、对象这几种。</code></pre></blockquote><blockquote><pre><code>2.数组：数组在js中是中括号[ ]括起来的内容，数据结构为 [&quot;Python&quot;, &quot;javascript&quot;, &quot;C++&quot;, ...]，取值方式和所有语言中一样，使用索引获取，字段值的类型可以是 数字、字符串、数组、对象几种。</code></pre></blockquote><h2 id="JSON模块"><a href="#JSON模块" class="headerlink" title="JSON模块"></a>JSON模块</h2><p>Python中提供了json模块，总共有四个方法：dumps、dump、loads、load，用于字符串 和 python数据类型间进行转换。<br><strong>1.json.loads()</strong><br>对数据进行解码，把Json格式字符串 解码 转换成Python对象 。<br><img src="https://i.imgur.com/gFu4n1P.png" alt=""></p><p>实例：<br><img src="https://i.imgur.com/3zOg7fE.png" alt=""></p><p><strong>2.json.dumps()</strong><br>对数据进行编码，返回一个str对象，把一个Python对象编码转换成Json字符串。如果直接将dict类型的数据写入json文件中会发生报错，因此在将数据写入时需要用到该函数。<br><img src="https://i.imgur.com/foRbCB5.png" alt=""></p><p>实例：<br><img src="https://i.imgur.com/N5ids9K.png" alt=""></p><p><strong>3.json.dump()</strong><br>json.dump()用于将dict类型的数据转成str，并写入到json文件中  </p><p><img src="https://i.imgur.com/b06PiXX.png" alt=""></p><p><strong>4.json.load()</strong><br>json.load()直接从json文件中读取数据，并将str转为dict类型</p><p>实例：<br><img src="https://i.imgur.com/bBO2Wo7.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当我们爬取异步加载的网页从上面获取的往往是json类型的文件，通常我们需要对这类数据进行处理后才能使用，下面就来说说python中对json数据的基本处理方法。&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="json" scheme="http://yoursite.com/tags/json/"/>
    
  </entry>
  
  <entry>
    <title>Ajax数据的爬取</title>
    <link href="http://yoursite.com/2018/07/18/Ajax%E6%95%B0%E6%8D%AE%E7%9A%84%E7%88%AC%E5%8F%96/"/>
    <id>http://yoursite.com/2018/07/18/Ajax数据的爬取/</id>
    <published>2018-07-18T02:53:33.000Z</published>
    <updated>2018-09-01T16:07:21.360Z</updated>
    
    <content type="html"><![CDATA[<p>不知道你在写爬虫时有没有遇到这种情况：在浏览器中可以看到正常显示的页面数据，但是用requests得到的html文件里却没有这些数据。这是因为浏览器中的页面时经过JavaScript处理数据后生成的，Ajax加载就是其中一种方式，下面我们便来讲一下如何在异步加载的网页中抓取有用数据。<br><a id="more"></a>  </p><h2 id="什么是Ajax"><a href="#什么是Ajax" class="headerlink" title="什么是Ajax"></a>什么是Ajax</h2><p><strong>Ajax</strong>，就是异步的JavaScript和XML。它是利用JavaScript在保证页面不被刷新，页面链接不改变的情况下，后台与服务器进行数据交互并更新部分网页的技术。  </p><h2 id="Ajax的实现过程"><a href="#Ajax的实现过程" class="headerlink" title="Ajax的实现过程"></a>Ajax的实现过程</h2><blockquote><pre><code>第一步，创建xmlhttprequest对象，var xmlhttp =new XMLHttpRequest（);XMLHttpRequest对象用来和服务器交换数据。第二步，使用xmlhttprequest对象的open（）和send（）方法发送资源请求给服务器。第三步，使用xmlhttprequest对象的responseText或responseXML属性获得服务器的响应。第四步，onreadystatechange函数，当发送请求到服务器，我们想要服务器响应执行一些功能就需要使用onreadystatechange函数，每次xmlhttprequest对象的readyState发生改变都会触发onreadystatechange函数。</code></pre></blockquote><p>真实的数据都是通过JavaScript向服务器发送ajax请求得到的，所以要抓取这些数据，必须搞清楚这些请求是如何发送的，以及发送到了哪里，参数有哪些等等。下面我们便来分析一下。</p><h2 id="Ajax的分析方法"><a href="#Ajax的分析方法" class="headerlink" title="Ajax的分析方法"></a>Ajax的分析方法</h2><p><strong>1.分析请求</strong>  </p><p>以微博为例，当我们拖动刷新额内容时，此时网页由Ajax加载的，页面url不会发生变化，下面我们就借助chrome浏览器来分析。<br>首先，打开我巴萨新闻的官方微博链接（本人巴萨球迷。。。）  </p><p><img src="https://i.imgur.com/56DDocS.png" alt=""><br>element选项可以查看网页源代码，但这不是我们要看的。<br>点击network选项卡，刷新一下页面，发现出现很多内容，那么这些内容便是页面在加载过程中浏览器与服务器之间发送请求和接受响应内容的记录。  </p><p><img src="https://i.imgur.com/MHF0oEP.png" alt=""></p><p>这里我们找到右边条目栏中有一个getIndex的请求，点击它，查看相应信息。<br><img src="https://i.imgur.com/gJAwlqS.png" alt=""><br>我们发现Request Headers里面有一个信息围为<strong>X-Request-with：XMLHttpRequest</strong>，这就表明此请求为Ajax请求；ajax其实有特殊的请求类型，叫作<strong>xhr</strong>，稍后我们会讲到。</p><p><img src="https://i.imgur.com/RU6k1BN.png" alt=""><br>preview选项可以查看响应内容，发现响应内容为json格式。</p><p><img src="https://i.imgur.com/C1IyRw2.png" alt=""><br>response选项卡中为真实的返回数据</p><p>现在我们回到chrome浏览器最开始发送的请求<br><img src="https://i.imgur.com/XAmdgHA.png" alt=""><br>原始链接返回的response代码知识执行了一些JS。所以我们看到的微博页面的真实数据不是原始页面，而是执行了页面中的JS代码后再次向后台发送Ajax请求后，浏览器拿到数据在进一步进行渲染出来的页面。  </p><p><strong>2.筛选Ajax请求</strong>  </p><p>前面提到ajax请求类型为xhr，这里我们点击network选项卡下的XHR选项，然后刷新页面发现，下面的请求都是Ajax请求。打开一条请求，我们会看到Request URL，request Headers，response headers, response body等内容。</p><p><img src="https://i.imgur.com/VE5CfXF.png" alt=""></p><p>对Ajax请求分析完毕后，现在我们就可以模拟这些请求，来抓取数据啦啦啦！！！</p><h2 id="抓取Ajax加载的数据"><a href="#抓取Ajax加载的数据" class="headerlink" title="抓取Ajax加载的数据"></a>抓取Ajax加载的数据</h2><p><strong>1.分析请求</strong>  </p><p><img src="https://i.imgur.com/9CzVySM.png" alt=""></p><p>我们选择其中一条请求，发现这是一个GET类型额请求，</p><blockquote><pre><code>https://m.weibo.cn/api/container/getIndex?type=uid&amp;value=1990303727&amp;containerid=1076031990303727&amp;page=2</code></pre></blockquote><p>URL包含4个参数，分别是：type，value，containerid，page。<br>然后我们分析一下规律后发现，type一直为uid，value值也不变，containerid就是107603加上value，唯一变化的参数就是page，顾名思义应该就是第几页的意思。请求分析完了。接下来看看响应内容。  </p><p><strong>2.分析响应</strong>  </p><p><img src="https://i.imgur.com/CCbittX.png" alt=""></p><p><img src="https://i.imgur.com/27cs9hW.png" alt=""></p><p>观察可知，响应内容为JSON格式，然后主要的信息全包含在cardlistInfo和cards里面。其中cards里面有一个mblog，包含的信息都是微博里面的点赞数，评论数，转发数等，还有page_Info主要包含的时博文内容这些信息。接下来我们就将这些格式化的内容提取出来。哈哈！  </p><h2 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h2><p>首先构造URL，发送请求，拿到json文本内容。<br><img src="https://i.imgur.com/4gqvJj8.png" alt=""></p><p>返回内容为：<br><img src="https://i.imgur.com/w80FzyK.png" alt=""></p><p>接着就是解析json文本，提取需要的数据了，我们就提取cards里面的mblog里面的一些内容，比如正文，点赞数，评论数，转发数。<br><img src="https://i.imgur.com/JP5arE0.png" alt=""></p><p>这样我们便抓取到第二页所需的微博信息啦：<br><img src="https://i.imgur.com/uLDc29w.png" alt=""></p><p>之前是抓取的一个页面的数据，如果要抓取所有页面，只需传入page的最大值，然后使用for循环即可。  </p><h2 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h2><p>我们将数据保存至mongodb中，由于是字典，我们直接进行存储即可<br>连接数据库：<br><img src="https://i.imgur.com/aUFTnAK.png" alt=""></p><p>保存：<br><img src="https://i.imgur.com/x74m1XQ.png" alt=""><br>运行结果：<br><img src="https://i.imgur.com/22jrCiD.png" alt=""></p><p>查看数据库：<br><img src="https://i.imgur.com/7enGi11.png" alt=""></p><p>这样我们便完成了Ajax加载的网页数据的抓取啦，在分析ajax请求时，我们还可以使用抓包工具Fiddler来进行分析。<br>另外，有些网站把ajax请求的所有参数全部加密了。我们根本没办法构造自己所需要的数据的请求。这种情况下就用<strong>selenium+phantomJS</strong>，调用浏览器内核，并利用phantomJS执行js来模拟人为操作以及触发页面中的js脚本。从填写表单到点击按钮再到滚动页面，全部都可以模拟，不考虑具体的请求和响应过程，只是完完整整的把人浏览页面获取数据的过程模拟一遍。 这个我们会在下篇博文来讲解。 </p><p>文末附上写的另一个小demo，今日头条街拍图片：<br><a href="https://github.com/appleguardu/spider_projects/blob/master/Ajax_spider/toutiao_today.py" target="_blank" rel="noopener">https://github.com/appleguardu/spider_projects/blob/master/Ajax_spider/toutiao_today.py</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不知道你在写爬虫时有没有遇到这种情况：在浏览器中可以看到正常显示的页面数据，但是用requests得到的html文件里却没有这些数据。这是因为浏览器中的页面时经过JavaScript处理数据后生成的，Ajax加载就是其中一种方式，下面我们便来讲一下如何在异步加载的网页中抓取有用数据。&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Selenium爬取淘宝商品</title>
    <link href="http://yoursite.com/2018/07/13/selenium%E7%88%AC%E5%8F%96%E6%B7%98%E5%AE%9D%E5%95%86%E5%93%81/"/>
    <id>http://yoursite.com/2018/07/13/selenium爬取淘宝商品/</id>
    <published>2018-07-13T05:28:28.000Z</published>
    <updated>2018-09-01T16:06:56.803Z</updated>
    
    <content type="html"><![CDATA[<p>前一天学习了selenium的相关用法，今天打算用selenium来模拟浏览器操作，抓取数据，抓取目标就定为淘宝网吧，接下来我们就开工啦！<br><a id="more"></a>  </p><h2 id="目标设定"><a href="#目标设定" class="headerlink" title="目标设定"></a>目标设定</h2><p>目标定为抓取淘宝iPhone手机销售的相关信息：其中包括商品的图片，名字，最关键的价格，购买量，商铺，地点等信息。目标定好了，接下来我们就去分析网页规律了。  </p><h2 id="网页分析"><a href="#网页分析" class="headerlink" title="网页分析"></a>网页分析</h2><p>首先，确定抓取入口，也就是start url为淘宝搜索页面，url的链接为：  </p><blockquote><pre><code>https://s.taobao.com/search?q=iphone</code></pre></blockquote><p>当然，这是第一页的链接。但是我们要获取的是所有页面的，先下拉到页面底部，我们发现：<br><img src="https://i.imgur.com/B2oVHX4.png" alt="">  </p><p>用黑线圈起来的是一个分页导航，其中包括前5页的链接，下一页的链接，同时还有一个任意页码跳转的链接，页面总数为100。 若要获取每一页的内容， 我们的思路是将页码从1到100的顺序进行遍历一遍。 此时我们便又两种方式来进行遍历：  </p><blockquote><pre><code>1.点击下一页，等下一页面加载出来后进行爬取2.在页面跳转框逐一输入1~100，点击确定按钮后跳转到对应页码进行爬取  </code></pre></blockquote><p>对于方法一，有一个弊端就是如果爬取过程中出现异常，程序退出了，我在点击下一页时，可能无法快速切换到后续的页面；所以，在爬取过程中还需要记录当前页码数，如果点击下一页加载失败的话，还要增加异常检查等。 方法二，我们只需输入对应页面的数值就可以跳转，比较方便。<br>当页面成功加载出来后，我们利用selenium获取页面源代码，然后进行解析，就可以获取目标信息了。  </p><h2 id="获取商品列表"><a href="#获取商品列表" class="headerlink" title="获取商品列表"></a>获取商品列表</h2><p>首先，构造出要抓取的url：  </p><blockquote><pre><code>https://s.taobao.com/search?q=iphone </code></pre></blockquote><p>q指得是搜索的关键字，改变这个参数就可以搜索不同的商品，这里我们把定义成一个变量keyword。<br>接下来我们进行代码演练了。<br><img src="https://i.imgur.com/9zhFFeT.png" alt=""><br><img src="https://i.imgur.com/2Bu9bhH.png" alt=""></p><p>输出结果：<br><img src="https://i.imgur.com/4NA5EqV.png" alt=""></p><p>这样我们便拿到了第一页商品信息的源码啦，当然这个代码还不完善，还没添加跳转功能，因为page我们设计是从第一页抓取的，当page值为2时，我们就要翻页了，那么怎么进行翻页呢？首先获取页码输入框啊，然后获取确定按钮进行操作啦。<br><img src="https://i.imgur.com/Aepl0Mc.png" alt=""><br><img src="https://i.imgur.com/94knVze.png" alt=""></p><p>我们发现，当前页面都是高亮显示的，那么我们只需要判断高亮的页码数是当前页码数就证明页面跳转成功。这里可以使用等待条件：text_to_be_present_in_element,它会等待指定的文本出现在某一节点里面时即返回成功。<br><img src="https://i.imgur.com/djIRfga.png" alt=""></p><h2 id="解析商品列表"><a href="#解析商品列表" class="headerlink" title="解析商品列表"></a>解析商品列表</h2><p>既然上面已经能拿到页面商品的源代码了，那么我们接下来就可以对源码进行解析了。<br>分析可得，每个页面有44家店铺，在源代码里我们可以发现整个商品列表使用的CSS选择器是#mainsrp-itemlist .items .item, 它匹配了整个页面的44件商品，然后我们先用items()方法将其全部提取出来，他会返回一个生成器，再使用for循环进行遍历，得到每个item对象，然后用pyquery的find方法选取我们所需要的信息即可。<br>代码如下：<br><img src="https://i.imgur.com/JDIeNPe.png" alt=""></p><p>提取结果：<br><img src="https://i.imgur.com/gxIzM05.png" alt=""></p><p>哈哈，这样我们就已经拿到第一页所有商品的数据了，接下，我们要把100页的信息都爬取下来，这里可以直接使用for循环进行遍历页码。  </p><h2 id="爬取所有商品，保存数据库"><a href="#爬取所有商品，保存数据库" class="headerlink" title="爬取所有商品，保存数据库"></a>爬取所有商品，保存数据库</h2><p>先创建数据库：<br><img src="https://i.imgur.com/cV5FihW.png" alt=""></p><p>然后for循环遍历所有页面<br><img src="https://i.imgur.com/YYEc2ko.png" alt=""></p><p>最后的结果：<br><img src="https://i.imgur.com/tjwiYUs.png" alt=""><br>MongoDB数据库：<br><img src="https://i.imgur.com/BJyyvDe.png" alt=""></p><p>然后，所有页面的iphone数据都被我们爬取下来了。。。当然，代码还可以再精简一点。。。  </p><p>源码地址:<a href="https://github.com/appleguardu/spider_projects/blob/master/taobao_iphone.py" target="_blank" rel="noopener">https://github.com/appleguardu/spider_projects/blob/master/taobao_iphone.py</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前一天学习了selenium的相关用法，今天打算用selenium来模拟浏览器操作，抓取数据，抓取目标就定为淘宝网吧，接下来我们就开工啦！&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="selenium" scheme="http://yoursite.com/tags/selenium/"/>
    
  </entry>
  
  <entry>
    <title>selenium的使用</title>
    <link href="http://yoursite.com/2018/07/11/selenium%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2018/07/11/selenium的使用/</id>
    <published>2018-07-11T09:10:04.000Z</published>
    <updated>2018-09-01T16:06:41.136Z</updated>
    
    <content type="html"><![CDATA[<p>对于像淘宝，京东这样的网站，虽然它也是采用Ajax获取数据，但是Ajax接口中含有许多加密参数，我们很难去直接找出其规律；所以我们需要直接使用模拟浏览器运行的方式来实现，这样就可以做到在浏览器中看到的是什么样，抓取的源码就是什么样的啦，即可见可爬，系不系很可怕。<br><a id="more"></a>  </p><h2 id="Selenium"><a href="#Selenium" class="headerlink" title="Selenium"></a>Selenium</h2><p>selenium是一个自动化测试工具，使用它可以驱动浏览器执行特定的动作，如点击，下拉，填充表单等操作，同时可以获取浏览器当前呈现页面的源代码，做到可见可爬。<br>下面以chrome浏览器为例，来说一下selenium的使用方式。  </p><blockquote><pre><code>首先，配置谷歌浏览器对于版本的ChromeDriver，可以找资源去下载，然后将chromedriver.exe放在python文件夹下的Scripts文件夹里。然后，安装python的selenium库，自行百度；接下来我们就可以使用selenium啦!!!</code></pre></blockquote><p><img src="https://i.imgur.com/mZll8Ht.png" alt=""></p><h2 id="声明浏览器对象"><a href="#声明浏览器对象" class="headerlink" title="声明浏览器对象"></a>声明浏览器对象</h2><p>首先初始化浏览器对象，接着就可调用browser对象，让其模拟浏览器操作啦！<br><img src="https://i.imgur.com/gxAlhda.png" alt=""></p><h2 id="访问页面"><a href="#访问页面" class="headerlink" title="访问页面"></a>访问页面</h2><p>使用get()方法访问页面<br><img src="https://i.imgur.com/CUAkEEO.png" alt=""></p><p>运行代码后，会弹出一个淘宝界面，并输出网页的源代码。<br><img src="https://i.imgur.com/5NwK8Ag.jpg" alt=""></p><h2 id="节点操作"><a href="#节点操作" class="headerlink" title="节点操作"></a>节点操作</h2><p>当我们用selenium驱动浏览器模拟操作时，需要先获取其操作节点，比如输入框，按钮的位置。  </p><p><strong>单个节点</strong><br>以淘宝搜索框为例，先从源代码里找到其位置。<br><img src="https://i.imgur.com/ed1UMjD.png" alt=""></p><blockquote><pre><code>&lt;input id=&quot;q&quot; name=&quot;q&quot; aria-label=&quot;请输入搜索文字&quot; accesskey=&quot;s&quot; autofocus=&quot;autofocus&quot; autocomplete=&quot;off&quot; class=&quot;search-combobox-input&quot; aria-haspopup=&quot;true&quot; aria-combobox=&quot;list&quot; role=&quot;combobox&quot; x-webkit-grammar=&quot;builtin:translate&quot; tabindex=&quot;0&quot;&gt;</code></pre></blockquote><p>这个节点或者说是标签中有很多属性，比如id属性：q， name属性：q；那么我们可以根据css选择器或者xpath获取这个节点。  </p><blockquote><pre><code>1. find_element_by_id(&apos;q&apos;)  2. find_element_by_css_selector(&apos;#q&apos;)3. find_element_by_xpath(&apos;//*[@id=&quot;q&quot;]&apos;)以上方法都可获取搜索输入框，一般获取单个节点用以上方法就足够了，另外还可以根据name或者class_name来获取。  </code></pre></blockquote><p>selenium还有通用方法，find_element(By.ID, ‘q’),需要传入两个参数，一个时查找方式，一个是查找值。返回结果为webelement类型。</p><p><strong>多个节点</strong><br>如果我们需要查找满足条件的所有节点，就绪要用到find_elements()方法。返回结果为列表类型，列表在每个节点为webelement类型。查找方式与单个节点一致。  </p><p><strong>节点交互</strong><br>selenium可以驱动浏览器执行相应的操作，我们先找到这个节点，比如淘宝搜索框，然后调用方法：  </p><blockquote><pre><code>send_keys() 输入文字clear() 清空文字click() 点击按钮 还有很多其他的操作，可以去google查</code></pre></blockquote><p><img src="https://i.imgur.com/11otno6.png" alt="">  </p><p>代码运行效果：<br><img src="https://i.imgur.com/Q0duWFX.png" alt=""></p><h2 id="动作链"><a href="#动作链" class="headerlink" title="动作链"></a>动作链</h2><p>还有一些其他的操作，他们没有像上面那样是针对单个节点的，他们没有特定的对象，比如鼠标拖动，键盘按键等，这些动作就是动作链。  </p><p><img src="https://i.imgur.com/kleR7Xi.png" alt="">  </p><p>动作前：<br><img src="https://i.imgur.com/odEyGQP.png" alt=""></p><p>动作后：<br><img src="https://i.imgur.com/yApOg8G.png" alt=""></p><h2 id="执行JavaScript"><a href="#执行JavaScript" class="headerlink" title="执行JavaScript"></a>执行JavaScript</h2><p>对于某些操作，selenium API并没有提供；比如，下拉进度条，它可直接模拟运行JS脚本，此时可以使用<strong>execute_script()</strong>方法实现。<br><img src="https://i.imgur.com/9IILHeu.png" alt=""></p><p>执行效果：<br><img src="https://i.imgur.com/rsl9U5d.png" alt=""></p><h2 id="获取节点信息"><a href="#获取节点信息" class="headerlink" title="获取节点信息"></a>获取节点信息</h2><p>通过前面讲的page_source属性可以获取网页的源代码，那么我们都可以对获取的网页源码进行解析(re, BS, pyquery)。不过selenium自带相关的方法和属性来直接提取节点信息。  </p><p><strong>获取属性值</strong><br>先选中节点，再通过节点对象调用get_attribute(‘属性名’)就可获取这个属性对应的内容。<br><img src="https://i.imgur.com/A4Ts0H9.png" alt=""> </p><p><strong>获取文本值</strong><br>每个WeBElement节点都有text属性，直接调用这个属性就可以获取节点内部的文本信息，这相当于BS中的get_text()和pyquery中的text()方法。<br>加入获取的节点为p，那么p.text就是其文本内容。这里就不演示了  </p><p><strong>获取其他值</strong><br>另外，一个节点还有其他属性，方法都差不多。  </p><blockquote><pre><code>节点.id   # 获取节点id节点.location   # 获取节点在页面中的位置节点.tag_name   # 获取节点标签名称节点.size   # 获取节点宽高</code></pre></blockquote><h2 id="切换frame"><a href="#切换frame" class="headerlink" title="切换frame"></a>切换frame</h2><p>网页中有一种节点叫作 iframe， 即子frame，相当于页面的子页面，它的结构和外部网页的结构完全一致。selenium打开页面后，默认在父级的Frame里操作，若此时页面中还有子Frame，它是不能获取到子Frame里面的节点的，所以，此时需要使用<strong>switch_to.frame()</strong>方法来切换Frame。  </p><h2 id="延时等待"><a href="#延时等待" class="headerlink" title="延时等待"></a>延时等待</h2><p>在selenium中， get()方法会在网页框架加载结束后执行，此时若获取page_soucre，可能并不是浏览器完全加载的页面，如果某些页面有额外的Ajax请求，我们就不一定能捕获成功。 所以，这里需要延时等待一定时间，确保节点已经加载完全。<br><strong>隐式等待</strong><br>当使用隐式等待时，如果selenium没有在DOM中找到节点，将继续等待，超出设定时间后，则会抛出找不到节点的异常。<br><img src="https://i.imgur.com/Zq8XTVv.png" alt=""></p><p><strong>显式等待</strong><br>由于页面加载有时候会受到网络的影响，所以隐式等待这种设置固定时间并不是很靠谱；这里引入显式等待。<br>显式等待指定了要查找的节点，然后指定一个最长等待时间，如果在规定时间加载出来便返回该节点；如果没有，即抛出超时异常。  </p><p>首先，需要引入WebDriverWait这个对象，调用until方法，传入等待条件excepted_conditions。<br><strong>expected_conditions模块:</strong>  </p><p>以下两个条件类验证title，验证传入的参数title是否等于或包含于driver.title </p><blockquote><pre><code>title_is title_contains</code></pre></blockquote><p>以下两个条件验证元素是否出现，传入的参数都是元组类型的locator，如(By.ID, ‘kw’) ;顾名思义，一个只要一个符合条件的元素加载出来就通过；另一个必须所有符合条件的元素都加载出来才行 </p><blockquote><pre><code>presence_of_element_located presence_of_all_elements_located</code></pre></blockquote><p>以下三个条件验证元素是否可见，前两个传入参数是元组类型的locator，第三个传入WebElement ;第一个和第三个其实质是一样的 </p><blockquote><pre><code>visibility_of_element_located invisibility_of_element_located visibility_of</code></pre></blockquote><p>以下两个条件判断某段文本是否出现在某元素中，一个判断元素的text，一个判断元素的value </p><blockquote><pre><code>text_to_be_present_in_element text_to_be_present_in_element_value</code></pre></blockquote><p>以下条件判断frame是否可切入，可传入locator元组或者直接传入定位方式：id、name、index或WebElement </p><blockquote><pre><code>frame_to_be_available_and_switch_to_it</code></pre></blockquote><p>以下条件判断是否有alert出现 </p><blockquote><pre><code>alert_is_present</code></pre></blockquote><p>以下条件判断元素是否可点击，传入locator<br>element_to_be_clickable</p><p>以下四个条件判断元素是否被选中，第一个条件传入WebElement对象，第二个传入locator元组<br>第三个传入WebElement对象以及状态，相等返回True，否则返回False<br>第四个传入locator以及状态，相等返回True，否则返回False </p><blockquote><pre><code>element_to_be_selected element_located_to_be_selected element_selection_state_to_be element_located_selection_state_to_be</code></pre></blockquote><p>最后一个条件判断一个元素是否仍在DOM中，传入WebElement对象，可以判断页面是否刷新了 </p><blockquote><pre><code>staleness_of</code></pre></blockquote><p><img src="https://i.imgur.com/Xpj1Vj2.png" alt=""></p><h2 id="前进和后退"><a href="#前进和后退" class="headerlink" title="前进和后退"></a>前进和后退</h2><blockquote><pre><code>browser.back()   后退，浏览器中返回上一个页面browser.forward()   前进，下一个页面</code></pre></blockquote><h2 id="Cookies操作"><a href="#Cookies操作" class="headerlink" title="Cookies操作"></a>Cookies操作</h2><p>使用selenium还可以方便的对cookies进行操作<br><img src="https://i.imgur.com/GMq4nfL.png" alt=""></p><h2 id="选项卡管理"><a href="#选项卡管理" class="headerlink" title="选项卡管理"></a>选项卡管理</h2><p>在实际访问网页时，会开启一个个选项卡。在selenium中，我们也会对其进行操作。  </p><p><img src="https://i.imgur.com/ZrNbSSZ.png" alt=""></p><p>效果：<br><img src="https://i.imgur.com/P0NCssl.png" alt=""></p><h2 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h2><p>在使用selenium时，也会遇到异常情况；比如访问超时，节点无法找到等，所有也需要使用try。。。except来捕获各类异常。<br>下面举个例子：<br><img src="https://i.imgur.com/agPTE5p.png" alt="">  </p><p>那么到目前为止，selenium的常规用法基本上都讲了一遍了，若是想了解更多相关内容的话，可以参考以下几个链接：<br>selenium官方文档：<br><a href="http://selenium-python.readthedocs.io/installation.html#introduction" title="selenium官方文档" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/installation.html#introduction</a><br>博文参考：<br><a href="https://blog.csdn.net/wenboyu/article/details/78162667" target="_blank" rel="noopener">https://blog.csdn.net/wenboyu/article/details/78162667</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于像淘宝，京东这样的网站，虽然它也是采用Ajax获取数据，但是Ajax接口中含有许多加密参数，我们很难去直接找出其规律；所以我们需要直接使用模拟浏览器运行的方式来实现，这样就可以做到在浏览器中看到的是什么样，抓取的源码就是什么样的啦，即可见可爬，系不系很可怕。&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>爬取猫眼电影信息</title>
    <link href="http://yoursite.com/2018/07/05/%E7%88%AC%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1%E4%BF%A1%E6%81%AF/"/>
    <id>http://yoursite.com/2018/07/05/爬取猫眼电影信息/</id>
    <published>2018-07-05T07:52:36.000Z</published>
    <updated>2018-09-01T16:05:11.731Z</updated>
    
    <content type="html"><![CDATA[<p>本篇内容主要讲解的是一个基础爬虫实例——爬去猫眼电影排行榜前100的影片信息，通过这个实例，来理解一个爬虫的基本思路。<br><a id="more"></a>  </p><h2 id="抓取目标"><a href="#抓取目标" class="headerlink" title="抓取目标"></a>抓取目标</h2><p>确定需要爬去的信息：猫眼电影top100的影片信息，这些信息包括电影名称，上映时间，演员，图片等。</p><h2 id="网页分析"><a href="#网页分析" class="headerlink" title="网页分析"></a>网页分析</h2><p>我们发现榜单top100对应的url是maoyan.com/board/4 。这便是我们要抓取的目标站点。<br><img src="https://i.imgur.com/uP4n5f7.png" alt=""></p><p>首先我们看到，霸王别姬排名第一，页面中的有效信息包括电影名，主演，上映时间，地区，评分和图片等。接着我们继续翻页，观察页面url和内容的变化；<br><img src="https://i.imgur.com/1mniQsL.png" alt=""></p><p>可以看到页面的url变成了<a href="http://maoyan.com/board/4?offset=10，多了一个参数offset=10，目前页面显示内容为排行11~20的电影；我们再点击下一页，发现此时url变成了http://maoyan.com/board/4?offset=20，显示结果为排行21~30的电影，所以：" target="_blank" rel="noopener">http://maoyan.com/board/4?offset=10，多了一个参数offset=10，目前页面显示内容为排行11~20的电影；我们再点击下一页，发现此时url变成了http://maoyan.com/board/4?offset=20，显示结果为排行21~30的电影，所以：</a><br>网页显示规律为：offset表示偏移量，如果偏移量为n，那么显示电影排行为n+1到n+10，每页显示10部电影。如果要获取top100，则需要请求10次，每次抓取一个页面，提取出里面有用的信息。  </p><h2 id="抓取第一页"><a href="#抓取第一页" class="headerlink" title="抓取第一页"></a>抓取第一页</h2><p>首先我们来抓取第一页的内容。在此之前我们先创建一个主函数main(),用来执行主要操作。<br>第一步，获取网页内容。</p><p><img src="https://i.imgur.com/2AQs44l.png" alt=""></p><h2 id="解析页面"><a href="#解析页面" class="headerlink" title="解析页面"></a>解析页面</h2><p>获取到网页源代码后，就需要对页面进行解析，然后提取出我们想要的信息。我们先回到网页查看一下真实源码，在<strong>开发者模式下的Network监听组件</strong>中查看源代码。注意：不要在Elements选项卡中查看源代码，因为那里的源码可能是经过JavaScript操作，与原始请求不同，所以需要从Network下查看。<br>排行第一的源代码片段：<br><img src="https://i.imgur.com/ydbNLEA.png" alt=""> </p><p>经过分析发现，每部电影的信息都包含在一个<dd></dd>的节点中。  </p><blockquote><pre><code>排名对应节点： &lt;i class=&quot;board-index board-index-1&quot;&gt;1&lt;/i&gt;图片对应节点：&lt;img data-src=&quot;http://p1.meituan.net/movie/20803f59291c47e1e116c11963ce019e68711.jpg@160w_220h_1e_1c&quot; alt=&quot;霸王别姬&quot; class=&quot;board-img&quot; /&gt;电影名称：&lt;p class=&quot;name&quot;&gt;&lt;a href=&quot;/films/1203&quot; title=&quot;霸王别姬&quot; data-act=&quot;boarditem-click&quot; data-val=&quot;{movieId:1203}&quot;&gt;霸王别姬&lt;/a&gt;&lt;/p  主演：&lt;p class=&quot;star&quot;&gt;主演：张国荣,张丰毅,巩俐&lt;/p&gt;上映时间：&lt;p class=&quot;releasetime&quot;&gt;上映时间：1993-01-01(中国香港)&lt;/p&gt;</code></pre></blockquote><p>下面我们就用正则表达式来提取上述信息，当然也可以使用Xpath和css选择器，我这里为了巩固一下正则用法，就先用re来写，虽然麻烦点。下面定义一个解析的方法。<br><img src="https://i.imgur.com/sVlFvdE.png" alt=""></p><p>这样我们就得到了第一个页面的信息，以列表list的形式返回的，每部电影信息保存在一个元组中。<br><img src="https://i.imgur.com/tAlDjx9.png" alt=""></p><p>由于数据的格式比较乱，我们需要对其进行格式的整理，这里选择遍历数据列表，把每部电影信息保存为字典dict形式。现在将方法修改一下。<br><img src="https://i.imgur.com/gZUzIXx.png" alt=""></p><p>整理后的信息如下：<br><img src="https://i.imgur.com/HYgoqD7.png" alt=""></p><h2 id="保存提取信息"><a href="#保存提取信息" class="headerlink" title="保存提取信息"></a>保存提取信息</h2><p>由于上面我们将每部电影信息都保存为了字典类型，这样更为结构化；现在我们将他们写入到文本文件中。这里通过JSON库的dumps()方法将字典进行序列化，由于输出结果有中文，需要将ensure_ascii指定为False，这样可以保证中文输出，而不是Unicode编码格式。<br><img src="https://i.imgur.com/bilBPae.png" alt=""></p><h2 id="main-方法调用"><a href="#main-方法调用" class="headerlink" title="main()方法调用"></a>main()方法调用</h2><p>最后我们定义一个main()方法用来调用前面的方法，将单页的电影信息全部写入到文件中。<br><img src="https://i.imgur.com/wUBtzNw.png" alt=""></p><p>单页文件输出结果：<br><img src="https://i.imgur.com/RzwULTw.png" alt=""></p><p>这样，我们就成功完成了第一页电影信息的爬取，但是我们目标是爬取所有电影，接下来就来试试把</p><h2 id="分页爬取"><a href="#分页爬取" class="headerlink" title="分页爬取"></a>分页爬取</h2><p>之前已经分析过了，每次翻页之后，url都会有变化，多了一个参数offset，所以我们只需要构造一下URL即可。下面将代码修改一下：<br><img src="https://i.imgur.com/M3gGvdY.png" alt=""></p><p>输出结果：<br><img src="https://i.imgur.com/9gFXrP7.png" alt=""></p><h2 id="代码最后的修改"><a href="#代码最后的修改" class="headerlink" title="代码最后的修改"></a>代码最后的修改</h2><p>现在我们已经基本上完成了所有电影信息的抓取，但是部分代码还是有缺陷，比如请求部分未作异常处理，爬取速度太快，可能会被反爬措施封锁等，所以还需要将代码优化一下。<br>代码地址：<a href="https://github.com/appleguardu/spider_projects" target="_blank" rel="noopener">https://github.com/appleguardu/spider_projects</a></p><p>好了，一个简单爬虫基本上完成！！！继续加油！！！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇内容主要讲解的是一个基础爬虫实例——爬去猫眼电影排行榜前100的影片信息，通过这个实例，来理解一个爬虫的基本思路。&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>python爬虫基本流程</title>
    <link href="http://yoursite.com/2018/07/02/python%E7%88%AC%E8%99%AB%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B/"/>
    <id>http://yoursite.com/2018/07/02/python爬虫基本流程/</id>
    <published>2018-07-02T06:34:17.000Z</published>
    <updated>2018-09-01T16:04:45.177Z</updated>
    
    <content type="html"><![CDATA[<h2 id="何为爬虫"><a href="#何为爬虫" class="headerlink" title="何为爬虫"></a>何为爬虫</h2><p>爬虫：请求网站并提取数据的自动化程序<br><a id="more"></a>  </p><h2 id="爬虫的基本流程"><a href="#爬虫的基本流程" class="headerlink" title="爬虫的基本流程"></a>爬虫的基本流程</h2><p>1.. 发起请求<br>通过HTTP库(主要用到urllib和requests这两个库)向目标站点发起请求，即发送一个Request，请求可以包含headers等信息，等待服务器响应。  </p><blockquote><pre><code>Request：浏览器就发送消息给该网址所在的服务器，这个过程叫做HTTP Request。  Request介绍：1.请求方式：主要有GET、POST两种类型，另外还有HEAD、PUT、DELETE、OPTIONS等。2.请求URL：URL全称统一资源定位符，如一个网页文档、一张图片、一个视频等都可以用URL唯一来确定。3.请求头：包含请求时的头部信息，如User-Agent、Host、Cookies等信息。4.请求体：请求时额外携带的数据如表单提交时的表单数据</code></pre></blockquote><p>2.. 获取响应内容<br>如果服务器正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，类型可能是HtML，Json字符串，二进制数据(图片，视频)等类型。  </p><blockquote><pre><code>Response:服务器收到浏览器发送的消息后，能够根据浏览器发送消息的内容，做相应处理，然后把消息回传给浏览器。这个过程叫做HTTP Response。浏览器收到服务器的Response信息后，会对信息进行相应处理，然后展示。 Response介绍：1.响应状态：有多种响应状态，如200代表成功、301跳转、404找不到页面、502服务器错误。2.响应头：如内容类型、内容长度、服务器信息、设置Cookie等等。3.响应体：最主要的部分，包含了请求资源的内容，如网页HTML、图片二进制数据等。 </code></pre></blockquote><p>3.. 解析获取的内容<br>获取的内容若是HTML，可以用正则表达式，网页解析库(BS,XPath等)进行解析。可能是Json，可以直接转为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理。  </p><p>4.. 保存数据<br>保存形式多样，可以存为文本，也可以保存至数据库，或者保存特定格式的文件。  </p><blockquote><pre><code>1.文本：纯文本、Json、Xml等。2.关系型数据库：如MySQL、Oracle等具有结构化表结构形式存储。3.非关系型数据库：如MongoDB、Redis等Key-Value形式存储。4.二进制文件：如图片、视频、音频等等直接保存成特定格式即可。  </code></pre></blockquote><p>另外，爬虫过程还会遇到很多问题，例如JavaScript渲染的问题，网站的反爬措施，爬取速度太慢等等，这些问题会在后期更新博文的时候再做详细的讲解。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;何为爬虫&quot;&gt;&lt;a href=&quot;#何为爬虫&quot; class=&quot;headerlink&quot; title=&quot;何为爬虫&quot;&gt;&lt;/a&gt;何为爬虫&lt;/h2&gt;&lt;p&gt;爬虫：请求网站并提取数据的自动化程序&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>爬虫的思路</title>
    <link href="http://yoursite.com/2018/07/01/%E7%88%AC%E8%99%AB%E7%9A%84%E6%80%9D%E8%B7%AF/"/>
    <id>http://yoursite.com/2018/07/01/爬虫的思路/</id>
    <published>2018-07-01T01:56:37.000Z</published>
    <updated>2018-09-01T16:04:00.947Z</updated>
    
    <content type="html"><![CDATA[<p>万事开头难，刚开始写爬虫程序的时候，不知从何下手；通过最近一段时间练习写爬虫程序，也渐渐有了一些简单的思路，在这里做个笔记，供日后翻阅。<br><a id="more"></a>  </p><h2 id="准备好目标URL"><a href="#准备好目标URL" class="headerlink" title="准备好目标URL"></a>准备好目标URL</h2><p>一般分为一下两种情况：  </p><p>1.准备start_url</p><blockquote><pre><code>页码总数不确定，规律不明显通过代码提取下一页地址使用xpath/css/re提取url寻找url地址，部分参数在当前页面中，比如当前页码数和总页码数，使用这两个参数构建url</code></pre></blockquote><p>2.准备url_list</p><blockquote><pre><code>页码总数清楚url的地址规律明显</code></pre></blockquote><h2 id="发送请求，获取响应"><a href="#发送请求，获取响应" class="headerlink" title="发送请求，获取响应"></a>发送请求，获取响应</h2><blockquote><pre><code>添加随机user-agent，添加随机代理ip，如果对方判断我们是爬虫之后应该添加更多的headers字段，包含cookie；cookie的处理可以使用session来解决准备一堆能用的cookie，组成cookie池。如果不登录:        准备刚刚开始能够成功请求的cookie，即接收对方网站设置在response的cookie如果登录:       准备多个账号       使用程序获取每个账号的cookie       请求登录之后才能访问的网站随机选择上一步获取的cookie</code></pre></blockquote><h2 id="提取数据"><a href="#提取数据" class="headerlink" title="提取数据"></a>提取数据</h2><p>1.确定数据的位置:  </p><p>如果数据在当前的url地址中,提取的是列表页中的数据;直接请求列表页的url地址，不用进入详情页,提取详情页的数据<br><strong>1.确定url; 2.发送请求; 3.提取数据; 4.返回</strong></p><p>如果数据不在当前的url地址中,在其他响应中(例如Ajax请求)，寻找数据的位置<br> <strong>在network中从上而下找,使用chrome中的过滤条件，选择除了js，css, image之外的按钮,使用chrome的serach all file，搜索数字和英文</strong></p><p>如果碰到加密的Ajax请求，可使用selenium等工具获取响应源码<br><strong>分析源码，获取目标数据</strong></p><p>2.数据的提取  </p><blockquote><pre><code>1.使用re,xpath，css selector从html中提取整块的数据，先分组，之后每一组在提取2.json数据，可使用get方法进行筛选后，在进行相应的提取</code></pre></blockquote><h2 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h2><blockquote><pre><code>保存在本地json ，csv，text,使用open方法或者os创建本地文件夹，将文件保存至本地。保存在数据库，关系型MySQL，提前创建好表，将数据按字段分类；非关系型，MongoDB， redis等。</code></pre></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;万事开头难，刚开始写爬虫程序的时候，不知从何下手；通过最近一段时间练习写爬虫程序，也渐渐有了一些简单的思路，在这里做个笔记，供日后翻阅。&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架学习之CrawlSpider</title>
    <link href="http://yoursite.com/2018/06/17/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E4%B9%8BCrawlSpider/"/>
    <id>http://yoursite.com/2018/06/17/Scrapy框架学习之CrawlSpider/</id>
    <published>2018-06-17T07:56:17.000Z</published>
    <updated>2018-09-01T16:01:16.887Z</updated>
    
    <content type="html"><![CDATA[<p>CrawlSpider是Spider的派生类，Spider类的设计原则是只爬取start_url列表中的网页，而CrawlSpider类定义了一些规则(rule)来提供跟进link的方便的机制，从爬取的网页中获取link并继续爬取。<br><a id="more"></a>  </p><h2 id="源码解读"><a href="#源码解读" class="headerlink" title="源码解读"></a>源码解读</h2><p>快速创建方式：scrapy genspider -t crawl tencent tencent.com </p><p><img src="https://i.imgur.com/xtMx6Ki.png" alt=""><br><img src="https://i.imgur.com/6amTlHs.png" alt=""></p><h2 id="CrawlSpider的属性和方法"><a href="#CrawlSpider的属性和方法" class="headerlink" title="CrawlSpider的属性和方法"></a>CrawlSpider的属性和方法</h2><p><strong>LinkExtractors</strong>  </p><blockquote><pre><code>class scrapy.linkextractors.LinkExtractor</code></pre></blockquote><p>Link Extractors 的目的很简单: 提取链接｡</p><p>每个LinkExtractor有唯一的公共方法是 extract_links()，它接收一个 Response 对象，并返回一个 scrapy.link.Link 对象。</p><p>Link Extractors要实例化一次，并且 extract_links 方法会根据不同的 response 调用多次提取链接｡</p><blockquote><pre><code>class scrapy.linkextractors.LinkExtractor(    allow = (),    deny = (),    allow_domains = (),    deny_domains = (),    deny_extensions = None,    restrict_xpaths = (),    tags = (&apos;a&apos;,&apos;area&apos;),       attrs = (&apos;href&apos;),       canonicalize = True,    unique = True,    process_value = None)</code></pre></blockquote><p>主要参数：</p><ul><li><p>allow：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。</p></li><li><p>deny：与这个正则表达式(或正则表达式列表)不匹配的URL一定不提取。</p></li><li><p>allow_domains：会被提取的链接的domains。</p></li><li><p>deny_domains：一定不会被提取链接的domains。</p></li><li><p>restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接。</p></li></ul><p><strong>rules</strong><br>在rules中包含一个或多个Rule对象，每个Rule对爬取网站的动作定义了特定操作。如果多个rule匹配了相同的链接，则根据规则在本集合中被定义的顺序，第一个会被使用。  </p><blockquote><pre><code>class scrapy.spiders.Rule(    link_extractor,     callback = None,     cb_kwargs = None,     follow = None,     process_links = None,     process_request = None)</code></pre></blockquote><p>主要参数：  </p><ul><li><p>link_extractor：是一个Link Extractor对象，用于定义需要提取的链接。</p></li><li><p>callback： 从link_extractor中每获取到链接时，参数所指定的值作为回调函数，该回调函数接受一个response作为其第一个参数。</p></li></ul><p>注意：当编写爬虫规则时，避免使用parse作为回调函数。由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了 parse方法，crawl spider将会运行失败。</p><ul><li><p>follow：是一个布尔(boolean)值，指定了根据该规则从response提取的链接是否需要跟进。 如果callback为None，follow 默认设置为True ，否则默认为False。</p></li><li><p>process_links：指定该spider中哪个的函数将会被调用，从link_extractor中获取到链接列表时将会调用该函数。该方法主要用来过滤。</p></li><li><p>process_request：指定该spider中哪个的函数将会被调用， 该规则提取到每个request时都会调用该函数。 (用来过滤request)</p></li></ul><p>下面写一个实例：<br><img src="https://i.imgur.com/QTBG9mC.png" alt=""></p><h2 id="Logging"><a href="#Logging" class="headerlink" title="Logging"></a>Logging</h2><p>Scrapy提供了log功能，可以通过 logging 模块使用。</p><p>可以修改配置文件settings.py，任意位置添加下面两行，效果会清爽很多。</p><blockquote><pre><code>LOG_FILE = &quot;TencentSpider.log&quot;LOG_LEVEL = &quot;INFO&quot;</code></pre></blockquote><p>Log levels<br>Scrapy提供5层logging级别:</p><ul><li>CRITICAL - 严重错误(critical)</li><li>ERROR - 一般错误(regular errors)</li><li>WARNING - 警告信息(warning messages)</li><li>INFO - 一般信息(informational messages)</li><li>DEBUG - 调试信息(debugging messages)</li></ul><p>logging设置<br>通过在setting.py中进行以下设置可以被用来配置logging:</p><ul><li>LOG_ENABLED 默认: True，启用logging</li><li>LOG_ENCODING 默认: ‘utf-8’，logging使用的编码</li><li>LOG_FILE 默认: None，在当前目录里创建logging输出文件的文件名</li><li>LOG_LEVEL 默认: ‘DEBUG’，log的最低级别</li><li>LOG_STDOUT 默认: False 如果为 True，进程所有的标准输出(及错误)将会被重定向到log中。例如，执行 print “hello” ，其将会在Scrapy log中显示。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CrawlSpider是Spider的派生类，Spider类的设计原则是只爬取start_url列表中的网页，而CrawlSpider类定义了一些规则(rule)来提供跟进link的方便的机制，从爬取的网页中获取link并继续爬取。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架学习之Downloader Middlewares</title>
    <link href="http://yoursite.com/2018/06/15/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E4%B9%8BDownloader-Middlewares/"/>
    <id>http://yoursite.com/2018/06/15/Scrapy框架学习之Downloader-Middlewares/</id>
    <published>2018-06-15T08:52:03.000Z</published>
    <updated>2018-09-01T16:00:58.679Z</updated>
    
    <content type="html"><![CDATA[<p>Scrapy下载中间件是处于引擎(crawler.engine)和下载(crawler.engine.download())之间的一层组件，可以有多个下载中间件被加载运行；通常用来做异常处理和反爬处理。<br><a id="more"></a>  </p><h2 id="设置下载中间件"><a href="#设置下载中间件" class="headerlink" title="设置下载中间件"></a>设置下载中间件</h2><ul><li><p>当引擎传递请求给下载器的过程中，下载中间件可以对请求进行处理 （例如增加http header信息，增加proxy信息等）；</p></li><li><p>在下载器完成http请求，传递响应给引擎的过程中， 下载中间件可以对响应进行处理（例如进行gzip的解压等）</p></li></ul><p>要激活下载器中间件组件，将其加入到 DOWNLOADER_MIDDLEWARES 设置中。 该设置是一个字典(dict)，键为中间件类的路径，值为其中间件的顺序(order)。</p><p>这里是一个例子:</p><blockquote><pre><code>DOWNLOADER_MIDDLEWARES = {           &apos;mySpider.middlewares.MyDownloaderMiddleware&apos;: 543,  }</code></pre></blockquote><p>源码示例：<br><img src="https://i.imgur.com/jAeXWuy.png" alt=""></p><p>方法介绍：<br><strong>1. process_request(self, request, spider)</strong> </p><p>当每个request通过下载中间件时，该方法被调用。</p><ul><li>process_request() 必须返回以下其中之一：一个 None 、一个 Response 对象、一个 Request 对象或 raise IgnoreRequest:</li></ul><p>如果其返回 None ，Scrapy将继续处理该request，执行其他的中间件的相应方法，直到合适的下载器处理函数(download handler)被调用， 该request被执行(其response被下载)。</p><p>如果其返回 Response 对象，Scrapy将不会调用 任何 其他的 process_request() 或 process_exception() 方法，或相应地下载函数； 其将返回该response。 已安装的中间件的 process_response() 方法则会在每个response返回时被调用。</p><p>如果其返回 Request 对象，Scrapy则停止调用 process_request方法并重新调度返回的request。当新返回的request被执行后， 相应地中间件链将会根据下载的response被调用。</p><p>如果其raise一个 IgnoreRequest 异常，则安装的下载中间件的 process_exception() 方法会被调用。如果没有任何一个方法处理该异常， 则request的errback(Request.errback)方法会被调用。如果没有代码处理抛出的异常， 则该异常被忽略且不记录(不同于其他异常那样)。  </p><ul><li>参数:  </li></ul><p>request (Request 对象) – 处理的request</p><p>spider (Spider 对象) – 该request对应的spider</p><p><strong>2. process_response(self, request, response, spider)</strong></p><p>当下载器完成http请求，传递响应给引擎的时候调用</p><ul><li>process_request() 必须返回以下其中之一: 返回一个 Response 对象、 返回一个 Request 对象或raise一个 IgnoreRequest 异常。</li></ul><p>如果其返回一个 Response (可以与传入的response相同，也可以是全新的对象)， 该response会被在链中的其他中间件的 process_response() 方法处理。</p><p>如果其返回一个 Request 对象，则中间件链停止， 返回的request会被重新调度下载。处理类似于 process_request() 返回request所做的那样。</p><p>如果其抛出一个 IgnoreRequest 异常，则调用request的errback(Request.errback)。 如果没有代码处理抛出的异常，则该异常被忽略且不记录(不同于其他异常那样)。</p><ul><li>参数:</li></ul><p>request (Request 对象) – response所对应的request</p><p>response (Response 对象) – 被处理的response</p><p>spider (Spider 对象) – response所对应的spider</p><h2 id="几种scrapy中通常用来反爬虫的几种措施"><a href="#几种scrapy中通常用来反爬虫的几种措施" class="headerlink" title="几种scrapy中通常用来反爬虫的几种措施"></a>几种scrapy中通常用来反爬虫的几种措施</h2><p>这里因为暂时还没有项目涉及到，我就不写了，引用网上的资料参考。  </p><ul><li>创建middlewares.py文件</li></ul><p>Scrapy代理IP、Uesr-Agent的切换都是通过DOWNLOADER_MIDDLEWARES进行控制，我们在settings.py同级目录下创建middlewares.py文件，包装所有请求。<br><img src="https://i.imgur.com/KrjxbBW.png" alt=""></p><p>为什么HTTP代理要使用base64编码：</p><p>HTTP代理的原理很简单，就是通过HTTP协议与代理服务器建立连接，协议信令中包含要连接到的远程主机的IP和端口号，如果有需要身份验证的话还需要加上授权信息，服务器收到信令后首先进行身份验证，通过后便与远程主机建立连接，连接成功之后会返回给客户端200，表示验证通过，就这么简单，</p><p>下面是具体的信令格式：</p><blockquote><pre><code>CONNECT 59.64.128.198:21 HTTP/1.1Host: 59.64.128.198:21Proxy-Authorization: Basic bGV2I1TU5OTIzUser-Agent: OpenFetion</code></pre></blockquote><p>其中Proxy-Authorization是身份验证信息，Basic后面的字符串是用户名和密码组合后进行base64编码的结果，也就是对username:password进行base64编码。</p><blockquote><pre><code>HTTP/1.0 200 Connection established</code></pre><p>OK，客户端收到收面的信令后表示成功建立连接，接下来要发送给远程主机的数据就可以发送给代理服务器了，代理服务器建立连接后会在根据IP地址和端口号对应的连接放入缓存，收到信令后再根据IP地址和端口号从缓存中找到对应的连接，将数据通过该连接转发出去。</p></blockquote><p><strong>修改settings.py配置USER_AGENTS和PROXIES</strong></p><ul><li>添加USER_AGENTS：</li></ul><p><img src="https://i.imgur.com/E2dgdX0.png" alt=""></p><ul><li>添加代理IP设置PROXIES<br>免费代理IP可以网上搜索，或者付费购买一批可用的私密代理IP</li></ul><p><img src="https://i.imgur.com/eICOPL8.png" alt=""></p><ul><li>除非特殊需要，禁用cookies，防止某些网站根据Cookie来封锁爬虫</li></ul><blockquote><pre><code>COOKIES_ENABLED = False</code></pre></blockquote><ul><li>设置下载延迟</li></ul><blockquote><pre><code>DOWNLOAD_DELAY = 3</code></pre></blockquote><ul><li>最后设置setting.py里的DOWNLOADER_MIDDLEWARES，添加自己编写的下载中间件类</li></ul><blockquote><pre><code>DOWNLOADER_MIDDLEWARES = {      &apos;mySpider.middlewares.MyCustomDownloaderMiddleware&apos;: 543,      &apos;mySpider.middlewares.RandomUserAgent&apos;: 1,      &apos;mySpider.middlewares.ProxyMiddleware&apos;: 100}</code></pre></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scrapy下载中间件是处于引擎(crawler.engine)和下载(crawler.engine.download())之间的一层组件，可以有多个下载中间件被加载运行；通常用来做异常处理和反爬处理。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架学习之Item Pipeline</title>
    <link href="http://yoursite.com/2018/06/12/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E4%B9%8BItem-Pipeline/"/>
    <id>http://yoursite.com/2018/06/12/Scrapy框架学习之Item-Pipeline/</id>
    <published>2018-06-12T06:56:51.000Z</published>
    <updated>2018-09-01T16:00:30.642Z</updated>
    
    <content type="html"><![CDATA[<p>当Item在Spider中被收集之后，它将会被传递到Item Pipeline，这些Item Pipeline组件按定义的顺序处理Item。<br><a id="more"></a></p><h1 id="Item-Pipeline"><a href="#Item-Pipeline" class="headerlink" title="Item Pipeline"></a>Item Pipeline</h1><p>每个Item Pipeline都是实现了简单方法的Python类，比如决定此Item是丢弃而存储。以下是item pipeline的一些典型应用：</p><ul><li>验证爬取的数据(检查item包含某些字段，比如说name字段)</li><li>查重(并丢弃)</li><li>将爬取结果保存到文件或者数据库中</li></ul><h2 id="编写item-pipeline"><a href="#编写item-pipeline" class="headerlink" title="编写item pipeline"></a>编写item pipeline</h2><p>item pipiline组件是一个独立的Python类，其中process_item()方法必须实现:  </p><p>常用方法举例：<br><img src="https://i.imgur.com/6Drjw3H.png" alt=""></p><p>我的实例：pipeline将所有(从所有’spider’中)爬取到的item，存储到一个独立地items.json 文件，每行包含一个序列化为’JSON’格式的’item’:<br><img src="https://i.imgur.com/2Ssebh0.png" alt=""></p><h2 id="启用一个Item-Pipeline组件"><a href="#启用一个Item-Pipeline组件" class="headerlink" title="启用一个Item Pipeline组件"></a>启用一个Item Pipeline组件</h2><p>为了启用Item Pipeline组件，必须将它的类添加到 settings.py文件ITEM_PIPELINES 配置。  </p><p><img src="https://i.imgur.com/sbi6xsH.png" alt=""></p><p>分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内（0-1000随意设置，数值越低，组件的优先级越高）。</p><h2 id="启动爬虫"><a href="#启动爬虫" class="headerlink" title="启动爬虫"></a>启动爬虫</h2><p>我们启动爬虫，就可以得到一个json文件。  </p><p><img src="https://i.imgur.com/sDXT1kj.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当Item在Spider中被收集之后，它将会被传递到Item Pipeline，这些Item Pipeline组件按定义的顺序处理Item。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架学习之Spider</title>
    <link href="http://yoursite.com/2018/06/10/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E4%B9%8BSpider/"/>
    <id>http://yoursite.com/2018/06/10/Scrapy框架学习之Spider/</id>
    <published>2018-06-10T02:29:45.000Z</published>
    <updated>2018-09-01T16:00:12.614Z</updated>
    
    <content type="html"><![CDATA[<p>Spider类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。 换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。<br><a id="more"></a>  </p><h1 id="Spider类"><a href="#Spider类" class="headerlink" title="Spider类"></a>Spider类</h1><p>class scrapy.Spider是最基本的类，所有编写的爬虫必须继承这个类。</p><p>主要用到的函数及调用顺序为：</p><p>__init__() : 初始化爬虫名字和start_urls列表</p><p>start_requests() 调用make_requests_from url():生成Requests对象交给Scrapy下载并返回response</p><p>parse() : 解析response，并返回Item或Requests（需指定回调函数）。Item传给Item pipline持久化 ， 而Requests交由Scrapy下载，并由指定的回调函数处理（默认parse())，一直进行循环，直到处理完所有的数据为止。</p><h2 id="Spider类的源码理解"><a href="#Spider类的源码理解" class="headerlink" title="Spider类的源码理解"></a>Spider类的源码理解</h2><p><img src="https://i.imgur.com/DK3qWhu.png" alt=""><br><img src="https://i.imgur.com/reWWTRh.png" alt=""></p><h2 id="主要属性和方法"><a href="#主要属性和方法" class="headerlink" title="主要属性和方法"></a>主要属性和方法</h2><ul><li>name</li></ul><p>定义spider名字的字符串。</p><p>例如，如果spider爬取 mywebsite.com ，该spider通常会被命名为 mywebsite</p><ul><li>allowed_domains</li></ul><p>包含了spider允许爬取的域名(domain)的列表，可选。</p><ul><li>start_urls</li></ul><p>初始URL元祖/列表。当没有制定特定的URL时，spider将从该列表中开始进行爬取。</p><ul><li>start_requests(self)</li></ul><p>该方法必须返回一个可迭代对象(iterable)。该对象包含了spider用于爬取（默认实现是使用 start_urls 的url）的第一个Request。</p><p>当spider启动爬取并且未指定start_urls时，该方法被调用。</p><ul><li>parse(self, response)</li></ul><p>当请求url返回网页没有指定回调函数时，默认的Request对象回调函数。用来处理网页返回的response，以及生成Item或者Request对象。</p><ul><li>log(self, message[, level, component])</li></ul><p>使用 scrapy.log.msg() 方法记录(log)message。 </p><p>下面附上自己写的spider类实例：   </p><p><strong>items.py文件：确定爬取内容</strong></p><p><img src="https://i.imgur.com/cN7SeIG.png" alt=""></p><p><strong>spider文件：parse()方法</strong>  </p><p><img src="https://i.imgur.com/D0PQrz4.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spider类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。 换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架学习之scrapy shell和selector</title>
    <link href="http://yoursite.com/2018/06/05/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E4%B9%8Bscrapy-shell%E5%92%8Cselector/"/>
    <id>http://yoursite.com/2018/06/05/Scrapy框架学习之scrapy-shell和selector/</id>
    <published>2018-06-05T12:10:32.000Z</published>
    <updated>2018-09-01T15:59:47.655Z</updated>
    
    <content type="html"><![CDATA[<p>Scrapy终端是一个交互终端，我们可以在未启动spider的情况下尝试及调试代码，也可以用来测试XPath或CSS表达式，查看他们的工作方式，方便我们爬取的网页中提取的数据。<br><a id="more"></a>  </p><h1 id="Scrapy-Shell"><a href="#Scrapy-Shell" class="headerlink" title="Scrapy Shell"></a>Scrapy Shell</h1><p>进入项目的根目录，执行下列命令来启动shell:</p><blockquote><pre><code>scrapy shell &quot;http://www.itcast.cn/channel/teacher.shtml&quot;</code></pre></blockquote><p><img src="https://i.imgur.com/F45RBJ9.png" alt=""></p><p>Scrapy Shell根据下载的页面会自动创建一些方便使用的对象，例如 Response 对象，以及 Selector 对象 (对HTML及XML内容)。</p><p>当shell载入后，将得到一个包含response数据的本地 response 变量，输入 response.body将输出response的包体，输出 response.headers 可以看到response的报头。</p><p>输入 response.selector 时， 将获取到一个response 初始化的类 Selector 的对象，此时可以通过使用 response.selector.xpath()或response.selector.css() 来对 response 进行查询。<br>Xpath对象：<br><img src="https://i.imgur.com/z6EN1U1.png" alt=""></p><h1 id="Selectors选择器"><a href="#Selectors选择器" class="headerlink" title="Selectors选择器"></a>Selectors选择器</h1><p>Scrapy Selectors 内置 XPath 和 CSS Selector 表达式机制；Selector有四个基本的方法，最常用的还是xpath:</p><ul><li>xpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表</li><li>extract(): 序列化该节点为Unicode字符串并返回list</li><li>extract_first(): 取出上述节点中的第一个元素，可以传入默认值。</li><li>css(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表，语法同 BeautifulSoup4 </li><li>re(): 根据传入的正则表达式对数据进行提取，返回Unicode字符串list列表  </li></ul><p>XPath表达式的一些例子及对应的含义:</p><blockquote><pre><code>/html/head/title: 选择&lt;HTML&gt;文档中 &lt;head&gt; 标签内的 &lt;title&gt; 元素/html/head/title/text(): 选择上面提到的 &lt;title&gt; 元素的文字//td: 选择所有的 &lt;td&gt; 元素//div[@class=&quot;mine&quot;]: 选择所有具有 class=&quot;mine&quot; 属性的 div 元素</code></pre></blockquote><p>css表达式的一些例子及对应的含义:  </p><blockquote><pre><code>a[href=&quot;image.html&quot;] img: href熟悉为image.html的a节点下的img节点a[href=&quot;image.html&quot;]::text: 获取href为image.html属性a节点下的文本a[href=&quot;image.html&quot;] img::attr(src): 上述img节点下src的属性 </code></pre></blockquote><p>另外，Selector还支持正则re，可以与xpath， css搭配使用</p><blockquote><pre><code>response.xpath(&apos;//a/text()&apos;).re(&apos;name:\s(.*)&apos;) 注意response不能直接调用re，可以先使用xpath和css匹配后，在使用re</code></pre></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scrapy终端是一个交互终端，我们可以在未启动spider的情况下尝试及调试代码，也可以用来测试XPath或CSS表达式，查看他们的工作方式，方便我们爬取的网页中提取的数据。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架学习之入门案例</title>
    <link href="http://yoursite.com/2018/06/03/Scrapy%E6%A1%86%E6%9E%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2018/06/03/Scrapy框架基础学习/</id>
    <published>2018-06-03T12:50:37.000Z</published>
    <updated>2018-09-01T15:59:24.063Z</updated>
    
    <content type="html"><![CDATA[<p>Scrapy是一个用纯Python实现，为了爬取网站数据、提取结构性数据而编写的应用框架；我们只需要定制开发几个模块就可以轻松的实现一个爬虫，用来抓取网页内容以及各种图片，非常之方便。Scrapy 使用了Twisted多线程异步网络框架来处理网络通讯，可以加快我们的下载速度，不用自己去实现异步框架，并且包含了各种中间件接口，可以灵活的完成各种需求。<br><a id="more"></a>  </p><h1 id="Scrapy架构图"><a href="#Scrapy架构图" class="headerlink" title="Scrapy架构图"></a>Scrapy架构图</h1><p>——————————–<img src="https://i.imgur.com/vNUUr2S.png" alt=""></p><p>它可以分为以下几个部分：  </p><ul><li><strong>Scrapy Engine(引擎):</strong> 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。</li><li><strong>Scheduler(调度器):</strong> 它负责接受<code>引擎</code>发送过来的Request请求，并按照一定的方式进行整理排列，入队，当<code>引擎</code>需要时，交还给<code>引擎</code>。</li><li><strong>Downloader（下载器)：</strong>负责下载<code>Scrapy Engine(引擎)</code>发送的所有Requests请求，并将其获取到的Responses交还给<code>Scrapy Engine(引擎)</code>，由<code>引擎</code>交给<code>Spider</code>来处理。</li><li><strong>spiders（爬虫）：</strong>它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给<code>引擎</code>，再次进入<code>Scheduler(调度器)</code>。</li><li><strong>Item Pipeline(管道)：</strong>它负责处理<code>Spider</code>中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方.</li><li><strong>Downloader Middlewares（下载中间件）：</strong>你可以当作是一个可以自定义扩展下载功能的组件。</li><li><strong>Spider Middlewares（Spider中间件）：</strong>你可以理解为是一个可以自定扩展和操作<code>引擎</code>和<code>Spider</code>中间<code>通信</code>的功能组件（比如进入<code>Spider</code>的Responses;和从<code>Spider</code>出去的Requests）。</li></ul><h1 id="Scrapy的运作流程"><a href="#Scrapy的运作流程" class="headerlink" title="Scrapy的运作流程"></a>Scrapy的运作流程</h1><p>代码写好，程序开始运行…</p><ol><li><code>引擎</code>：Hi！<code>Spider</code>, 你要处理哪一个网站？</li><li><code>Spider</code>：老大要我处理xxxx.com。</li><li><code>引擎</code>：你把第一个需要处理的URL给我吧。</li><li><code>Spider</code>：给你，第一个URL是xxxxxxx.com。</li><li><code>引擎</code>：Hi！<code>调度器</code>，我这有request请求你帮我排序入队一下。</li><li><code>调度器</code>：好的，正在处理你等一下。</li><li><code>引擎</code>：Hi！<code>调度器</code>，把你处理好的request请求给我。</li><li><code>调度器</code>：给你，这是我处理好的request。</li><li><code>引擎</code>：Hi！下载器，你按照老大的<code>下载中间件</code>的设置帮我下载一下这个request请求</li><li><code>下载器</code>：好的！给你，这是下载好的东西。（如果失败：sorry，这个request下载失败了。然后<code>引擎</code>告诉<code>调度器</code>，这个request下载失败了，你记录一下，我们待会儿再下载）</li><li><code>引擎</code>：Hi！<code>Spider</code>，这是下载好的东西，并且已经按照老大的<code>下载中间件</code>处理过了，你自己处理一下（注意！这儿responses默认是交给<code>def parse()</code>这个函数处理的）</li><li><code>Spider</code>：（处理完毕数据之后对于需要跟进的URL），Hi！<code>引擎</code>，我这里有两个结果，这个是我需要跟进的URL，还有这个是我获取到的Item数据。</li><li><code>引擎</code>：Hi ！<code>管道</code>我这儿有个item你帮我处理一下！<code>调度器</code>！这是需要跟进URL你帮我处理下。然后从第四步开始循环，直到获取完老大需要全部信息。</li><li><code>管道`</code>调度器`：好的，现在就做！</li></ol><p><strong>注意！只有当</strong> <code>调度器</code> <strong>中不存在任何request了，整个程序才会停止，（也就是说，对于下载失败的URL，Scrapy也会重新下载。）</strong></p><h1 id="Scrapy-爬虫步骤："><a href="#Scrapy-爬虫步骤：" class="headerlink" title="Scrapy 爬虫步骤："></a>Scrapy 爬虫步骤：</h1><ul><li>新建项目 (scrapy startproject xxx)：在终端中执行命令，新建一个爬虫项目，会创建一个xxx文件夹</li><li>明确目标 （编写items.py）：明确你想要抓取的目标，定义爬取的数据结构</li><li>制作爬虫 （spiders/xxspider.py）：制作爬虫开始爬取网页</li><li>存储内容 （pipelines.py）：设计管道存储爬取内容</li></ul><p>下面来就来讲一个简单的案例来熟悉以下scrapy的操作：  </p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ul><li>创建一个Scrapy项目</li><li>定义提取的结构化数据(Item)</li><li>编写爬取网站的 Spider 并提取出结构化数据(Item)</li><li>编写 Item Pipelines 来存储提取到的Item(即结构化数据)</li></ul><h2 id="一-新建项目-scrapy-startproject-studyScrapy"><a href="#一-新建项目-scrapy-startproject-studyScrapy" class="headerlink" title="一. 新建项目(scrapy startproject studyScrapy)"></a>一. 新建项目(scrapy startproject studyScrapy)</h2><ul><li><p>在开始爬取之前，必须创建一个新的Scrapy项目。进入自定义的项目目录中，比如我的是<strong>F:\spider_projects</strong> 运行下列命令：</p><blockquote><pre><code>scrapy startproject studyScrapy</code></pre></blockquote></li><li><p>其中， studyScrapy 为项目名称，可以看到将会创建一个 studyScrapy 的文件夹，目录结构大致如下：<br><img src="https://i.imgur.com/blWwfGE.png" alt=""></p></li></ul><p>下面来简单介绍一下各个主要文件的作用：  </p><ul><li><p>studyScrapy/spiders/ ：存储爬虫代码的文件夹</p></li><li><p>studyScrapy/ ：该项目的Python模块，需要从这里引入；即import</p></li><li><p>studyScrapy/items.py ：定义项目的目标文件，即需要爬取的数据</p></li><li><p>studyScrapy/middlewares.py： 定义爬取时的中间件</p></li><li><p>studyScrapy/pipelines.py ：项目的管道文件，</p></li><li><p>studyScrapy/settings.py ：项目的设置文件</p></li><li><p>scrapy.cfg ：scrapy部署时的配置文件</p></li></ul><h2 id="二-创建QuotesSpider类-studyScrapy-spiders"><a href="#二-创建QuotesSpider类-studyScrapy-spiders" class="headerlink" title="二. 创建QuotesSpider类(studyScrapy/spiders)"></a>二. 创建QuotesSpider类(studyScrapy/spiders)</h2><p>我们打算抓取谚语网：<a href="http://quotes.toscrape.com" target="_blank" rel="noopener">http://quotes.toscrape.com</a> 。使用命令行生成一个Spider：  </p><blockquote><pre><code>scrapy genspider quotes quotes.toscrape.com</code></pre></blockquote><p>cd 到studyScrapy文件目录下，执行上述语句，然后我们在spiders文件下发现一个quotes.py文件。</p><p><img src="https://i.imgur.com/n2EJ8hO.png" alt=""></p><p>这个类必须继承scrapy.Spider， 这里有三个属性，和一个方法。</p><ul><li><code>name = &quot;&quot;</code>：这个爬虫的识别名称，必须是唯一的，在不同的爬虫必须定义不同的名字。</li><li><code>allow_domains = []</code>是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页，不存在的URL会被忽略。</li><li><code>start_urls = []</code>：爬取的URL的列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成。</li><li><code>parse(self, response)</code>：解析的方法，每个初始URL完成下载后将被调用，调用的时候传入从每一个URL传回的Response对象来作为唯一参数，主要作用如下：<ol><li>负责解析返回的网页数据(response.body)，提取结构化数据(生成item)</li><li>生成需要下一页的URL请求；</li><li></li></ol></li></ul><h2 id="三-创建Item类"><a href="#三-创建Item类" class="headerlink" title="三. 创建Item类"></a>三. 创建Item类</h2><ol><li>打开studyScrapy目录下的items.py</li><li>Item 定义结构化数据字段，用来保存爬取到的数据，有点像Python中的dict，但是提供了一些额外的保护机制来减少错误。</li><li>创建Item需要继承scrapy.Item类，并且定义类型为scrapy.Field的字段。</li><li>接下来，创建一个QuoteItem类，定义我们要获取的内容字段，text， author， tags。</li></ol><p>此时，将items.py修改如下：</p><p><img src="https://i.imgur.com/cOhEe1R.png" alt=""></p><h2 id="四-修改parse-方法，解析Response"><a href="#四-修改parse-方法，解析Response" class="headerlink" title="四. 修改parse()方法，解析Response"></a>四. 修改parse()方法，解析Response</h2><p>parse方法中的参数response是start_urls里面的链接爬取的结果；所以我们可直接对response变量中的内容进行解析。<br>首先我们看一下网页结构，每一页都有class为quote的区块，每个区块里包含了text，author，tags，我们先找出所有quote，在提取出里面的信息。<br><img src="https://i.imgur.com/yBzpX75.png" alt=""></p><p>这里需要使用上面创建的Item，可以将它理解为一个字典，不过在声明的时候需要实例化。然后再将图中的结果解析出来赋值给Item中的每个字段，最后使用yield返回即可。</p><p><img src="https://i.imgur.com/6PrClK0.png" alt=""></p><p>这样就获取了初始页面的内容了。 这里为什么用yield而不用return呢？这里要从parse()方法的工作机制来说：</p><blockquote><pre><code>因为使用的是yield，parse函数将会被当做一个生成器使用。scrapy会逐一获取parse方法中生成的结果，并判断该结果是一个什么样的类型；如果是request则加入爬取队列，如果是item类型则使用pipeline处理，其他类型则返回错误信息。scrapy取到第一部分的request不会立马就去发送这个request，只是把这个request放到队列里，然后接着从生成器里获取；取完第一部分的request，然后再获取第二部分的item，取到item了，就会放到对应的pipeline里处理；parse()方法作为回调函数(callback)赋值给了Request，指定parse()方法来处理这些请求 scrapy.Request(url, callback=self.parse)Request对象经过调度，执行生成 scrapy.http.response()的响应对象，并送回给parse()方法，直到调度器中没有Request（递归的思路）取完之后，parse()工作结束，引擎再根据队列和pipelines中的内容去执行相应的操作；程序在取得各个页面的items前，会先处理完之前所有的request队列里的请求，然后再提取items。</code></pre></blockquote><h2 id="五-处理后续的Request"><a href="#五-处理后续的Request" class="headerlink" title="五. 处理后续的Request"></a>五. 处理后续的Request</h2><p>上面知识完成了首页的抓取，那么下一页怎么来完成了，以前我写的爬虫都是分析了url请求的规律，然后构造请求再取获取页面内容。这里我们需要从当前页面找到信息来生成下一个请求，然后再下一个请求中寻找信息构造下下一个请求。就这样循环迭代，实现整个网站的爬取。</p><p><img src="https://i.imgur.com/C0tjdXD.png" alt=""></p><p>将网页下拉到底部，查看下一页按钮，链接为/page/2/， 完整的URL就是 <a href="http://quotes.toscrape.com/page/2/" target="_blank" rel="noopener">http://quotes.toscrape.com/page/2/</a><br>我们现在就可以构造RUL请求了，这里又要用到一个新函数<strong>scrapy.Requst</strong>。它有两个参数—-url， callback。  </p><blockquote><pre><code>url: 新的请求链接callback： 回调函数，当指定了该回调函数的请求完成后，获取到响应，引擎会将该响应作为参数传递给这个回调函数。 回调函数进行解析或者生成下一个请求，这里的回调函数就是上面的parse()函数。</code></pre></blockquote><p>这里由于parse()就是解析text，author，tags的方法，并且下一页面的结构与刚才解析的页面是一样的，所以我们可以再次使用parse来解析页面。接下来我们先利用选择器得到下一页的URL，生成新请求；在parse()方法后面追加如下代码即可：  </p><p><img src="https://i.imgur.com/BvG1S5X.png" alt=""></p><h2 id="六-运行"><a href="#六-运行" class="headerlink" title="六. 运行"></a>六. 运行</h2><p>然后，进入项目所在目录，直接运行 <strong>scrapy crawl quotes</strong>，Scrapy就开始进行抓取啦，当然我们还没设置中间件Middlewares和Piplines里面的内容，后面写其他项目会细细说明的。<br><img src="https://i.imgur.com/pI4YD37.png" alt=""></p><p>另外，可以在项目的根目录下新建main.py文件,在爬虫代码中设置好断点，用于调试 </p><blockquote><pre><code>from scrapy import cmdlinecmdline.execute(&apos;scrapy crawl quotes&apos;.split())</code></pre></blockquote><h2 id="七-保存结果-Item-Pipeline"><a href="#七-保存结果-Item-Pipeline" class="headerlink" title="七.保存结果(Item Pipeline)"></a>七.保存结果(Item Pipeline)</h2><p><strong>保存到文件</strong> </p><p>针对小项目数据，我们可以直接保存成文件格式。</p><p>scrapy保存信息的最简单的方法主要有以下几种，-o 输出指定格式的文件，命令如下：</p><ul><li><p>scrapy crawl quotes -o quotes.json # 输出json文件</p></li><li><p>scrapy crawl quotes -o quotes.csv  # 输出csv文件</p></li><li><p>scrapy crawl quotes -o quotes.xml  # 输出xml格式文件</p></li><li><p>scrapy crawl quotes -o <a href="ftp://user:passwd@ftp.xxxx.com/path/to/quotes.csv" target="_blank" rel="noopener">ftp://user:passwd@ftp.xxxx.com/path/to/quotes.csv</a>  </p></li></ul><p>同时还支持ftp， s3等远程输出。</p><p><strong>保存到数据库</strong><br>如果想输出到数据库中的话，以mongoDB为例，我们可以定义<strong>项目管道Item Pipelin</strong>e进行操作。<br>当Item在Spider中被收集之后，它将会被传递到Item Pipeline，这些Item Pipeline组件按定义的顺序处理Item。<br>每个Item Pipeline都是实现了简单方法的Python类，比如决定此Item是丢弃而存储。以下是item pipeline的常用处理操作：</p><blockquote><pre><code>1. 清理HTML数据2. 验证爬虫数据，检查爬虫字段3. 查重并丢弃重复内容4. 将爬取结果保存到数据库中</code></pre></blockquote><p>编写item pipeline很简单，item pipiline组件是一个独立的Python类，其中process_item()方法必须实现。<br>下面是一些方法示例：<br><img src="https://i.imgur.com/AmBVtAr.png" alt=""></p><p>下面为实际的代码：<br><img src="https://i.imgur.com/ITitSws.png" alt=""><br><img src="https://i.imgur.com/ViRiqt5.png" alt=""></p><p>这样我们便完成了一个入门案例了，正式踏入Scrapy框架的学习之路。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scrapy是一个用纯Python实现，为了爬取网站数据、提取结构性数据而编写的应用框架；我们只需要定制开发几个模块就可以轻松的实现一个爬虫，用来抓取网页内容以及各种图片，非常之方便。Scrapy 使用了Twisted多线程异步网络框架来处理网络通讯，可以加快我们的下载速度，不用自己去实现异步框架，并且包含了各种中间件接口，可以灵活的完成各种需求。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
</feed>
