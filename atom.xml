<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>AppleGuard的博客</title>
  
  <subtitle>天下事有难易乎？为之，则难者亦易矣；不为，则易者亦难矣。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-08-29T13:27:31.859Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>huhua</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MySQL的数据类型</title>
    <link href="http://yoursite.com/2018/08/29/MySQL%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
    <id>http://yoursite.com/2018/08/29/MySQL的数据类型/</id>
    <published>2018-08-29T13:03:10.000Z</published>
    <updated>2018-08-29T13:27:31.859Z</updated>
    
    <content type="html"><![CDATA[<p>这篇是对mysql数据库中数据的类型做一个详细的介绍，方便以后在创建字段时选择合适的数据类型。<br><a id="more"></a>  </p><h1 id="MySQL的数据类型"><a href="#MySQL的数据类型" class="headerlink" title="MySQL的数据类型"></a>MySQL的数据类型</h1><p>主要包括以下五大类：</p><p>整数类型：BIT、BOOL、TINY INT、SMALL INT、MEDIUM INT、 INT、 BIG INT</p><p>浮点数类型：FLOAT、DOUBLE、DECIMAL</p><p>字符串类型：CHAR、VARCHAR、TINY TEXT、TEXT、MEDIUM TEXT、LONGTEXT、TINY BLOB、BLOB、MEDIUM BLOB、LONG BLOB</p><p>日期类型：Date、DateTime、TimeStamp、Time、Year</p><p>其他数据类型：BINARY、VARBINARY、ENUM、SET、Geometry、Point、MultiPoint、LineString、MultiLineString、Polygon、GeometryCollection等</p><h2 id="整型"><a href="#整型" class="headerlink" title="整型"></a>整型</h2><p><img src="https://i.imgur.com/NdA0FLe.png" alt=""></p><p>取值范围如果加了unsigned，则最大值翻倍，如tinyint unsigned的取值范围为(0~256)。</p><p> int(m)里的m是表示SELECT查询结果集中的显示宽度，并不影响实际的取值范围，没有影响到显示的宽度，不知道这个m有什么用。</p><h2 id="浮点型-float和double"><a href="#浮点型-float和double" class="headerlink" title="浮点型(float和double)"></a>浮点型(float和double)</h2><p><img src="https://i.imgur.com/x3pW6dW.png" alt=""></p><p>设一个字段定义为float(6,3)，如果插入一个数123.45678,实际数据库里存的是123.457，但总个数还以实际为准，即6位。整数部分最大是3位，如果插入数12.123456，存储的是12.1234，如果插入12.12，存储的是12.1200.</p><h2 id="定点数decimal"><a href="#定点数decimal" class="headerlink" title="定点数decimal"></a>定点数decimal</h2><p>浮点型在数据库中存放的是近似值，而定点类型在数据库中存放的是精确值。 </p><p>decimal(m,d) 参数m&lt;65 是总个数，d&lt;30且 d&lt;m 是小数位。</p><h2 id="字符串-char-varchar-text"><a href="#字符串-char-varchar-text" class="headerlink" title="字符串(char,varchar,_text)"></a>字符串(char,varchar,_text)</h2><p><img src="https://i.imgur.com/MZB11ih.png" alt=""></p><h4 id="char和varchar："><a href="#char和varchar：" class="headerlink" title="char和varchar："></a>char和varchar：</h4><p>1.char(n) 若存入字符数小于n，则以空格补于其后，查询之时再将空格去掉。所以char类型存储的字符串末尾不能有空格，varchar不限于此。 </p><p>2.char(n) 固定长度，char(4)不管是存入几个字符，都将占用4个字节，varchar是存入的实际字符数+1个字节（n&lt;=255）或2个字节(n&gt;255)，</p><p>所以varchar(4),存入3个字符将占用4个字节。 </p><p>3.char类型的字符串检索速度要比varchar类型的快。</p><h4 id="varchar和text："><a href="#varchar和text：" class="headerlink" title="varchar和text："></a>varchar和text：</h4><p>1.varchar可指定n，text不能指定，内部存储varchar是存入的实际字符数+1个字节（n&lt;=255）或2个字节(n&gt;255)，text是实际字符数+2个字节。 </p><p>2.text类型不能有默认值。 </p><p>3.varchar可直接创建索引，text创建索引要指定前多少个字符。varchar查询速度快于text,在都创建索引的情况下，text的索引似乎不起作用。</p><h2 id="日期时间类型"><a href="#日期时间类型" class="headerlink" title="日期时间类型"></a>日期时间类型</h2><p><img src="https://i.imgur.com/Vn1Q8ZS.png" alt=""></p><p>若定义一个字段为timestamp，这个字段里的时间数据会随其他字段修改的时候自动刷新，所以这个数据类型的字段可以存放这条记录最后被修改的时间。</p><h1 id="数据类型的属性"><a href="#数据类型的属性" class="headerlink" title="数据类型的属性"></a>数据类型的属性</h1><p><img src="https://i.imgur.com/SPVnmcy.png" alt=""></p><h1 id="使用建议"><a href="#使用建议" class="headerlink" title="使用建议"></a>使用建议</h1><p>1、在指定数据类型的时候一般是采用从小原则，比如能用TINY INT的最好就不用INT，能用FLOAT类型的就不用DOUBLE类型，这样会对MYSQL在运行效率上提高很大，尤其是大数据量测试条件下。</p><p>2、不需要把数据表设计的太过复杂，功能模块上区分或许对于后期的维护更为方便，慎重出现大杂烩数据表</p><p>3、数据表和字段的起名字也是一门学问</p><p>4、设计数据表结构之前请先想象一下是你的房间，或许结果会更加合理、高效</p><p>5、数据库的最后设计结果一定是效率和可扩展性的折中，偏向任何一方都是欠妥的</p><h2 id="选择数据类型的基本原则"><a href="#选择数据类型的基本原则" class="headerlink" title="选择数据类型的基本原则"></a>选择数据类型的基本原则</h2><p>前提：使用适合存储引擎。</p><p>选择原则：根据选定的存储引擎，确定如何选择合适的数据类型。</p><h4 id="下面的选择方法按存储引擎分类："><a href="#下面的选择方法按存储引擎分类：" class="headerlink" title="下面的选择方法按存储引擎分类："></a>下面的选择方法按存储引擎分类：</h4><ul><li><p>MyISAM 数据存储引擎和数据列：MyISAM数据表，最好使用固定长度(CHAR)的数据列代替可变长度(VARCHAR)的数据列。</p></li><li><p>MEMORY存储引擎和数据列：MEMORY数据表目前都使用固定长度的数据行存储，因此无论使用CHAR或VARCHAR列都没有关系。两者都是作为CHAR类型处理的。</p></li><li><p>InnoDB 存储引擎和数据列：建议使用 VARCHAR类型。</p></li></ul><p>对于InnoDB数据表，内部的行存储格式没有区分固定长度和可变长度列（所有数据行都使用指向数据列值的头指针），因此在本质上，使用固定长度的CHAR列不一定比使用可变长度VARCHAR列简单。因而，主要的性能因素是数据行使用的存储总量。由于CHAR平均占用的空间多于VARCHAR，因 此使用VARCHAR来最小化需要处理的数据行的存储总量和磁盘I/O是比较好的。</p><h4 id="下面说一下固定长度数据列与可变长度的数据列。"><a href="#下面说一下固定长度数据列与可变长度的数据列。" class="headerlink" title="下面说一下固定长度数据列与可变长度的数据列。"></a>下面说一下固定长度数据列与可变长度的数据列。</h4><p><strong>char与varchar</strong>  </p><p>CHAR和VARCHAR类型类似，但它们保存和检索的方式不同。它们的最大长度和是否尾部空格被保留等方面也不同。在存储或检索过程中不进行大小写转换。</p><p>下面的表显示了将各种字符串值保存到CHAR(4)和VARCHAR(4)列后的结果，说明了CHAR和VARCHAR之间的差别：<br><img src="https://i.imgur.com/Km1R4Nx.png" alt=""></p><p>请注意上表中最后一行的值只适用不使用严格模式时；如果MySQL运行在严格模式，超过列长度不的值不保存，并且会出现错误。<br>从CHAR(4)和VARCHAR(4)列检索的值并不总是相同，因为检索时从CHAR列删除了尾部的空格。</p><p><strong>text和blob</strong>  </p><p>在使用text和blob字段类型时要注意以下几点，以便更好的发挥数据库的性能。</p><p>1.BLOB和TEXT值也会引起自己的一些问题，特别是执行了大量的删除或更新操作的时候。删除这种值会在数据表中留下很大的”空洞”，以后填入这些”空洞”的记录可能长度不同,为了提高性能,建议定期使用 OPTIMIZE TABLE 功能对这类表进行碎片整理.</p><p>2.使用合成的（synthetic）索引。合成的索引列在某些时候是有用的。一种办法是根据其它的列的内容建立一个散列值，并把这个值存储在单独的数据列中。接下来你就可以通过检索散列值找到数据行了。但是，我们要注意这种技术只能用于精确匹配的查询（散列值对于类似&lt;或&gt;=等范围搜索操作符 是没有用处的）。我们可以使用MD5()函数生成散列值，也可以使用SHA1()或CRC32()，或者使用自己的应用程序逻辑来计算散列值。请记住数值型散列值可以很高效率地存储。同样，如果散列算法生成的字符串带有尾部空格，就不要把它们存储在CHAR或VARCHAR列中，它们会受到尾部空格去除的影响。<br>合成的散列索引对于那些BLOB或TEXT数据列特别有用。用散列标识符值查找的速度比搜索BLOB列本身的速度快很多。</p><p>3.在不必要的时候避免检索大型的BLOB或TEXT值。例如，SELECT *查询就不是很好的想法，除非你能够确定作为约束条件的WHERE子句只会找到所需要的数据行。否则，你可能毫无目的地在网络上传输大量的值。这也是 BLOB或TEXT标识符信息存储在合成的索引列中对我们有所帮助的例子。你可以搜索索引列，决定那些需要的数据行，然后从合格的数据行中检索BLOB或 TEXT值。</p><p>4.把BLOB或TEXT列分离到单独的表中。在某些环境中，如果把这些数据列移动到第二张数据表中，可以让你把原数据表中 的数据列转换为固定长度的数据行格式，那么它就是有意义的。这会减少主表中的碎片，使你得到固定长度数据行的性能优势。它还使你在主数据表上运行 SELECT *查询的时候不会通过网络传输大量的BLOB或TEXT值。</p><p><strong>浮点数与定点数</strong></p><p>在mysql中float、double（或real）是浮点数，decimal（或numberic）是定点数。<br>浮点数相对于定点数的优点是在长度一定的情况下，浮点数能够表示更大的数据范围；它的缺点是会引起精度问题。在今后关于浮点数和定点数的应用中，大家要记住以下几点：</p><ul><li>浮点数存在误差问题；</li><li>对货币等对精度敏感的数据，应该用定点数表示或存储；</li><li>编程中，如果用到浮点数，要特别注意误差问题，并尽量避免做浮点数比较；</li><li>要注意浮点数中一些特殊值的处理。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇是对mysql数据库中数据的类型做一个详细的介绍，方便以后在创建字段时选择合适的数据类型。&lt;br&gt;
    
    </summary>
    
      <category term="数据库" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="mysql" scheme="http://yoursite.com/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>Pyhton下的MySQL总结</title>
    <link href="http://yoursite.com/2018/08/29/Pyhton%E4%B8%8B%E7%9A%84MySQL%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2018/08/29/Pyhton下的MySQL总结/</id>
    <published>2018-08-29T11:33:21.000Z</published>
    <updated>2018-08-29T13:08:13.006Z</updated>
    
    <content type="html"><![CDATA[<p>主要是对mysql常用命令的一些总结，首先的对sql语句熟悉，之后我们可以用navacat可视化工具来操作数据库，更快捷。<br><a id="more"></a></p><h2 id="安装与启动"><a href="#安装与启动" class="headerlink" title="安装与启动"></a>安装与启动</h2><ul><li>安装：sudo apt-get install mysql-server</li><li>查看服务：ps ajx | grep mysql</li><li>停止服务:sudo service mysql stop</li><li>开启服务:sudo service mysql start</li><li>重启服务:sudo service mysql restart</li><li>链接数据库：mysql -uroot -p 输入密码</li><li>查看版本：select version();</li></ul><h2 id="数据库常用语句"><a href="#数据库常用语句" class="headerlink" title="数据库常用语句"></a>数据库常用语句</h2><ul><li>查看数据库：show databases;</li><li>创建数据库：create database 库名 [charset = UTF8];</li><li>查看建库语句：show create database 库名;</li><li>使用数据库：use 库名;</li><li>删除数据库：drop 库名;</li></ul><h2 id="数据库表的语句"><a href="#数据库表的语句" class="headerlink" title="数据库表的语句"></a>数据库表的语句</h2><ul><li>查看表：show tables;</li><li>查看表结构：desc 表名;</li><li>创建表：<blockquote><pre><code>CREATE TABLE table_name(  字段1 数据类型 其他属性,  字段2 datatype,  字段3 datatype,  .....  PRIMARY KEY(one or more columns)  );</code></pre></blockquote></li></ul><blockquote><pre><code>常用属性：-- auto_increment 表示自动增长-- not null 表示不为空-- primary key 表示为主键-- defaul 表示默认值</code></pre></blockquote><ul><li><p>删除表：drop table;</p></li><li><p>修改表结构：</p><blockquote><pre><code>添加字段：alter table 表名 add 列名 类型;修改字段(重命名)：alter table 表名 change 原名 新名 类型及约束;修改字段(不重命名)：alter table 表名 modify 列名 类型及约束;删除字段：alter table 表名 drop 列名;</code></pre></blockquote></li></ul><h2 id="常用增删改查"><a href="#常用增删改查" class="headerlink" title="常用增删改查"></a>常用增删改查</h2><h4 id="新增"><a href="#新增" class="headerlink" title="新增"></a>新增</h4><ul><li>全列插入：insert into 表名 values(…); –需要给主键留下占位符,用0或null皆可。</li><li>部分列插入：insert into 表名(列1,…) values(值1,…);</li><li>插入多行全列数据：insert into 表名 values(…),(…)…;</li><li>插入多行部分列数据：insert into 表名(列1,…) values(值1,…),(值1,…)…;   推荐使用这种</li></ul><h4 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h4><ul><li>修改操作：update 表名 set 列1=值1,列2=值2… where 条件;</li></ul><h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h4><ul><li>删除操作(不推荐)：delete from 表名 where 条件;</li><li>逻辑删除(推荐)：update 字段名 set isdelete=0 where id=1; –设置删除字段,执行删除字段的操作即对该字段更新。</li></ul><h4 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h4><ul><li><p>查询消除重复行：select distinct 列1,… from 表名;</p></li><li><p>条件查询</p><blockquote><pre><code>where条件查询：select * from 表名 where 条件;where可以与比较运算符、逻辑运算符、模糊查询、范围查询、空判断搭配使用1-比较运算符等于: =              select * from students where id = 1;大于: &gt;              select * from students where id &gt; 1;大于等于: &gt;=         例子就不举了小于: &lt;小于等于: &lt;=不等于: != 或 &lt;&gt;     select * from students where id != 1;2-逻辑运算符：and    select * from students where id &gt; 3 and gender=0;or     select * from students where id &lt; 4 or is_delete=0;not    select * from students where id not 4;</code></pre></blockquote></li><li><p>模糊查询：</p><blockquote><pre><code>like% 表示任意多个任意字符；  _ 表示一个任意字符select * from students where name like &apos;胡%&apos;;  --查询以胡字开头的内容select * from students where name like &apos;胡_&apos;;  --查询以胡字开头且后面只有一个字的内容select * from students where name like &apos;胡%&apos; or name like &apos;%华&apos;; -- 查询以胡字开头或以华字结尾的内容</code></pre></blockquote></li><li><p>范围查询：</p><blockquote><pre><code>in 表示在一个非连续的范围内                   select * from students where id in(1,3,8);  -- 查询 id 在 1,3,8 当中的内容not in 表示不在一个非连续的范围内              select * from students where id not in(1,3,8);-- 查询 id 不在 1,3,8 当中的内容between ... and ... 表示在一个连续的范围内    select * from students where id between 3 and 8; -- 查询 id 在3到8之间的内容REGEXP 表示正则查询，可以使用正则表达式查询数据  select * from students where name REGEXP &quot;^胡&quot;; -- 查询 name 是以胡字开头的内容 </code></pre></blockquote></li><li><p>空判断</p><blockquote><pre><code>判断是否为空 is null      select * from students where height is null; </code></pre></blockquote></li></ul><p><strong>注意：以上几种预算符优先级为:</strong></p><p>优先级由高到低的顺序为：小括号、not、比较运算符、逻辑运算符。and 比 or 先运算，如果同时出现并希望先算or，需要结合()使用。</p><ul><li><p>排序</p><blockquote><pre><code>asc 升序 ; desc 降序select * from students order by age desc,height desc; --显示所有的学生信息，先按照年龄从大到小排序，当年龄相同时 按照身高从高到矮排序</code></pre></blockquote></li><li><p>聚合函数</p><blockquote><pre><code>count(*)查询总数max(列)表示求此列的最大值min(列)表示求此列的最小值sum(列)表示求此列的和avg(列)表示求此列的平均值对应举例：select count(*) from students;select max(id) from students where gender=2;select min(id) from students where is_delete=0;select sum(age) from students where gender=1;select sum(age)/count(*) from students where gender=1; --求平均年龄select avg(id) from students where is_delete=0 and gender=2;</code></pre></blockquote></li><li><p>分组</p><blockquote><pre><code>group by     将查询结果按照1个或多个字段进行分组，字段值相同的为一组group_concat 表示分组之后，根据分组结果，使用group_concat()来放置每一组的某字段的值的集合select gender,group_concat(name) from students group by gender; 每个gender字段字段后面会显示一个名字的集合</code></pre></blockquote></li><li><p>分页</p><blockquote><pre><code>select * from 表名 limit start,countselect * from students where gender=1 limit 0,3;  --查询前三行的数据</code></pre></blockquote></li><li><p>连接查询</p><blockquote><pre><code>select * from 表1 inner/left/right join 表2 on 表1.列 = 表2.列</code></pre></blockquote></li></ul><blockquote><pre><code>inner join(内连接查询)：查询的结果为两个表匹配到的数据right join(右连接查询)：查询的结果为两个表匹配到的数据，右表特有的数据，对于左表中不存在的数据使用null填充left join(左连接查询)：查询的结果为两个表匹配到的数据，左表特有的数据，对于右表中不存在的数据使用null填充举例：select * from students inner join classes on students.cls_id = classes.id;select * from students as s left join classes as c on s.cls_id = c.id;select * from students as s right join classes as c on s.cls_id = c.id;</code></pre></blockquote><ul><li>子查询<blockquote><pre><code>在一个 select 语句中,嵌入了另外一个 select 语句, 那么被嵌入的 select 语句称之为子查询语句。子查询可以和 in 搭配使用语法： 主查询 where 条件 in (子查询)</code></pre></blockquote></li></ul><h4 id="数据库的备份与恢复"><a href="#数据库的备份与恢复" class="headerlink" title="数据库的备份与恢复"></a>数据库的备份与恢复</h4><ul><li><p>数据库备份</p><blockquote><pre><code>mysqldump –uroot –p 数据库名 &gt; 备份文件名.sql;</code></pre></blockquote></li><li><p>数据库恢复</p><blockquote><pre><code>mysql -uroot –p 新数据库名 &lt; 备份文件名.sql</code></pre></blockquote></li></ul><h2 id="Python与mysql交互"><a href="#Python与mysql交互" class="headerlink" title="Python与mysql交互"></a>Python与mysql交互</h2><p><img src="https://i.imgur.com/pZumaL0.jpg" alt=""></p><ul><li><p>创建connection对象</p><blockquote><pre><code>connection = pymysql.connect(host, port, database, user, password, charset)其中参数如下：host：连接的mysql主机，如果本机是&apos;localhost&apos;port：连接的mysql主机的端口，默认是3306database：数据库的名称user：连接的用户名password：连接的密码charset：通信采用的编码方式，推荐使用utf8</code></pre></blockquote></li><li><p>创建cursor对象</p><blockquote><pre><code>cursor=connection.cursor()其中常用方法：close():关闭cursorexecute(operation [, parameters ]):执行语句，返回受影响的行数，主要用于执行insert、update、delete语句，也可以执行create、alter、drop等语句。fetchone():执行查询语句时，获取查询结果集的第一个行数据，返回一个元组</code></pre><p>fetchall():执行查询时，获取结果集的所有行，一行构成一个元组，再将这些元组装入一个元组返回</p></blockquote></li></ul><h2 id="mysql视图"><a href="#mysql视图" class="headerlink" title="mysql视图"></a>mysql视图</h2><p>视图是对若干张基本表的引用，一张虚表，查询语句执行的结果，不存储具体的数据。</p><ul><li><p>视图语句</p><blockquote><pre><code>创建视图:create view 视图名称 as select语句; --建议视图以v_开头查看视图：show tables;使用视图：select * from 视图名称;删除视图：drop view 视图名称;</code></pre></blockquote></li><li><p>视图作用</p><blockquote><pre><code>提高了重用性，就像一个函数对数据库重构，却不影响程序的运行提高了安全性能，可以对不同的用户让数据更加清晰</code></pre></blockquote></li></ul><h2 id="mysql事务"><a href="#mysql事务" class="headerlink" title="mysql事务"></a>mysql事务</h2><p>事务,它是一个操作序列，这些操作要么都执行，要么都不执行，它是一个不可分割的工作单位。</p><ul><li><p>事务de特点</p><blockquote><pre><code>原子性: 一个事务必须被视为一个不可分割的最小工作单元，整个事务中的所有操作要么全部提交成功，要么全部失败回滚，对于一个事务来说，不可能只执行其中的一部分操作，这就是事务的原子性。一致性: 数据库总是从一个一致性的状态转换到另一个一致性的状态。隔离性: 一个事务所做的修改在最终提交以前，对其他事务是不可见的。持久性: 一旦事务提交，则其所做的修改会永久保存到数据库。</code></pre></blockquote></li><li><p>事务相关命令</p><blockquote><pre><code>开启事务：start transaction; 或者 begin;提交事务：commit;回滚事务：rollback;</code></pre></blockquote></li></ul><h2 id="mysql索引"><a href="#mysql索引" class="headerlink" title="mysql索引"></a>mysql索引</h2><p>数据库索引好比是一本书前面的目录，能加快数据库的查询速度</p><ul><li>索引相关命令<blockquote><pre><code>创建索引：create index 索引名称 on 表名(字段名称(长度)) --当指定索引的字段类型为字符串时，应填写长度查看索引：show index from 表名;删除索引：drop index 索引名称 on 表名;</code></pre></blockquote></li></ul><p>注意:</p><p>建立太多的索引将会影响更新和插入的速度，因为它需要同样更新每个索引文件。对于一个经常需要更新和插入的表格，就没有必要为一个很少使用的where字句单独建立索引了，对于比较小的表，排序的开销不会很大，也没有必要建立另外的索引。<br>建立索引会占用磁盘空间。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;主要是对mysql常用命令的一些总结，首先的对sql语句熟悉，之后我们可以用navacat可视化工具来操作数据库，更快捷。&lt;br&gt;
    
    </summary>
    
      <category term="数据库" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="mysql" scheme="http://yoursite.com/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架学习之Downloader Middlewares</title>
    <link href="http://yoursite.com/2018/08/27/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E4%B9%8BDownloader-Middlewares/"/>
    <id>http://yoursite.com/2018/08/27/Scrapy框架学习之Downloader-Middlewares/</id>
    <published>2018-08-27T08:52:03.000Z</published>
    <updated>2018-08-27T09:30:06.326Z</updated>
    
    <content type="html"><![CDATA[<p>Scrapy下载中间件是处于引擎(crawler.engine)和下载(crawler.engine.download())之间的一层组件，可以有多个下载中间件被加载运行；通常用来做异常处理和反爬处理。<br><a id="more"></a>  </p><h2 id="设置下载中间件"><a href="#设置下载中间件" class="headerlink" title="设置下载中间件"></a>设置下载中间件</h2><ul><li><p>当引擎传递请求给下载器的过程中，下载中间件可以对请求进行处理 （例如增加http header信息，增加proxy信息等）；</p></li><li><p>在下载器完成http请求，传递响应给引擎的过程中， 下载中间件可以对响应进行处理（例如进行gzip的解压等）</p></li></ul><p>要激活下载器中间件组件，将其加入到 DOWNLOADER_MIDDLEWARES 设置中。 该设置是一个字典(dict)，键为中间件类的路径，值为其中间件的顺序(order)。</p><p>这里是一个例子:</p><blockquote><pre><code>DOWNLOADER_MIDDLEWARES = {           &apos;mySpider.middlewares.MyDownloaderMiddleware&apos;: 543,  }</code></pre></blockquote><p>源码示例：<br><img src="https://i.imgur.com/jAeXWuy.png" alt=""></p><p>方法介绍：<br><strong>1. process_request(self, request, spider)</strong> </p><p>当每个request通过下载中间件时，该方法被调用。</p><ul><li>process_request() 必须返回以下其中之一：一个 None 、一个 Response 对象、一个 Request 对象或 raise IgnoreRequest:</li></ul><p>如果其返回 None ，Scrapy将继续处理该request，执行其他的中间件的相应方法，直到合适的下载器处理函数(download handler)被调用， 该request被执行(其response被下载)。</p><p>如果其返回 Response 对象，Scrapy将不会调用 任何 其他的 process_request() 或 process_exception() 方法，或相应地下载函数； 其将返回该response。 已安装的中间件的 process_response() 方法则会在每个response返回时被调用。</p><p>如果其返回 Request 对象，Scrapy则停止调用 process_request方法并重新调度返回的request。当新返回的request被执行后， 相应地中间件链将会根据下载的response被调用。</p><p>如果其raise一个 IgnoreRequest 异常，则安装的下载中间件的 process_exception() 方法会被调用。如果没有任何一个方法处理该异常， 则request的errback(Request.errback)方法会被调用。如果没有代码处理抛出的异常， 则该异常被忽略且不记录(不同于其他异常那样)。  </p><ul><li>参数:  </li></ul><p>request (Request 对象) – 处理的request</p><p>spider (Spider 对象) – 该request对应的spider</p><p><strong>2. process_response(self, request, response, spider)</strong></p><p>当下载器完成http请求，传递响应给引擎的时候调用</p><ul><li>process_request() 必须返回以下其中之一: 返回一个 Response 对象、 返回一个 Request 对象或raise一个 IgnoreRequest 异常。</li></ul><p>如果其返回一个 Response (可以与传入的response相同，也可以是全新的对象)， 该response会被在链中的其他中间件的 process_response() 方法处理。</p><p>如果其返回一个 Request 对象，则中间件链停止， 返回的request会被重新调度下载。处理类似于 process_request() 返回request所做的那样。</p><p>如果其抛出一个 IgnoreRequest 异常，则调用request的errback(Request.errback)。 如果没有代码处理抛出的异常，则该异常被忽略且不记录(不同于其他异常那样)。</p><ul><li>参数:</li></ul><p>request (Request 对象) – response所对应的request</p><p>response (Response 对象) – 被处理的response</p><p>spider (Spider 对象) – response所对应的spider</p><h2 id="几种scrapy中通常用来反爬虫的几种措施"><a href="#几种scrapy中通常用来反爬虫的几种措施" class="headerlink" title="几种scrapy中通常用来反爬虫的几种措施"></a>几种scrapy中通常用来反爬虫的几种措施</h2><p>这里因为暂时还没有项目涉及到，我就不写了，引用网上的资料参考。  </p><ul><li>创建middlewares.py文件</li></ul><p>Scrapy代理IP、Uesr-Agent的切换都是通过DOWNLOADER_MIDDLEWARES进行控制，我们在settings.py同级目录下创建middlewares.py文件，包装所有请求。<br><img src="https://i.imgur.com/KrjxbBW.png" alt=""></p><p>为什么HTTP代理要使用base64编码：</p><p>HTTP代理的原理很简单，就是通过HTTP协议与代理服务器建立连接，协议信令中包含要连接到的远程主机的IP和端口号，如果有需要身份验证的话还需要加上授权信息，服务器收到信令后首先进行身份验证，通过后便与远程主机建立连接，连接成功之后会返回给客户端200，表示验证通过，就这么简单，</p><p>下面是具体的信令格式：</p><blockquote><pre><code>CONNECT 59.64.128.198:21 HTTP/1.1Host: 59.64.128.198:21Proxy-Authorization: Basic bGV2I1TU5OTIzUser-Agent: OpenFetion</code></pre></blockquote><p>其中Proxy-Authorization是身份验证信息，Basic后面的字符串是用户名和密码组合后进行base64编码的结果，也就是对username:password进行base64编码。</p><blockquote><pre><code>HTTP/1.0 200 Connection established</code></pre><p>OK，客户端收到收面的信令后表示成功建立连接，接下来要发送给远程主机的数据就可以发送给代理服务器了，代理服务器建立连接后会在根据IP地址和端口号对应的连接放入缓存，收到信令后再根据IP地址和端口号从缓存中找到对应的连接，将数据通过该连接转发出去。</p></blockquote><p><strong>修改settings.py配置USER_AGENTS和PROXIES</strong></p><ul><li>添加USER_AGENTS：</li></ul><p><img src="https://i.imgur.com/E2dgdX0.png" alt=""></p><ul><li>添加代理IP设置PROXIES<br>免费代理IP可以网上搜索，或者付费购买一批可用的私密代理IP</li></ul><p><img src="https://i.imgur.com/eICOPL8.png" alt=""></p><ul><li>除非特殊需要，禁用cookies，防止某些网站根据Cookie来封锁爬虫</li></ul><blockquote><pre><code>COOKIES_ENABLED = False</code></pre></blockquote><ul><li>设置下载延迟</li></ul><blockquote><pre><code>DOWNLOAD_DELAY = 3</code></pre></blockquote><ul><li>最后设置setting.py里的DOWNLOADER_MIDDLEWARES，添加自己编写的下载中间件类</li></ul><blockquote><pre><code>DOWNLOADER_MIDDLEWARES = {      &apos;mySpider.middlewares.MyCustomDownloaderMiddleware&apos;: 543,      &apos;mySpider.middlewares.RandomUserAgent&apos;: 1,      &apos;mySpider.middlewares.ProxyMiddleware&apos;: 100}</code></pre></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scrapy下载中间件是处于引擎(crawler.engine)和下载(crawler.engine.download())之间的一层组件，可以有多个下载中间件被加载运行；通常用来做异常处理和反爬处理。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架学习之CrawlSpider</title>
    <link href="http://yoursite.com/2018/08/27/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E4%B9%8BCrawlSpider/"/>
    <id>http://yoursite.com/2018/08/27/Scrapy框架学习之CrawlSpider/</id>
    <published>2018-08-27T07:56:17.000Z</published>
    <updated>2018-08-27T08:38:45.730Z</updated>
    
    <content type="html"><![CDATA[<p>CrawlSpider是Spider的派生类，Spider类的设计原则是只爬取start_url列表中的网页，而CrawlSpider类定义了一些规则(rule)来提供跟进link的方便的机制，从爬取的网页中获取link并继续爬取。<br><a id="more"></a>  </p><h2 id="源码解读"><a href="#源码解读" class="headerlink" title="源码解读"></a>源码解读</h2><p>快速创建方式：scrapy genspider -t crawl tencent tencent.com </p><p><img src="https://i.imgur.com/xtMx6Ki.png" alt=""><br><img src="https://i.imgur.com/6amTlHs.png" alt=""></p><h2 id="CrawlSpider的属性和方法"><a href="#CrawlSpider的属性和方法" class="headerlink" title="CrawlSpider的属性和方法"></a>CrawlSpider的属性和方法</h2><p><strong>LinkExtractors</strong>  </p><blockquote><pre><code>class scrapy.linkextractors.LinkExtractor</code></pre></blockquote><p>Link Extractors 的目的很简单: 提取链接｡</p><p>每个LinkExtractor有唯一的公共方法是 extract_links()，它接收一个 Response 对象，并返回一个 scrapy.link.Link 对象。</p><p>Link Extractors要实例化一次，并且 extract_links 方法会根据不同的 response 调用多次提取链接｡</p><blockquote><pre><code>class scrapy.linkextractors.LinkExtractor(    allow = (),    deny = (),    allow_domains = (),    deny_domains = (),    deny_extensions = None,    restrict_xpaths = (),    tags = (&apos;a&apos;,&apos;area&apos;),       attrs = (&apos;href&apos;),       canonicalize = True,    unique = True,    process_value = None)</code></pre></blockquote><p>主要参数：</p><ul><li><p>allow：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。</p></li><li><p>deny：与这个正则表达式(或正则表达式列表)不匹配的URL一定不提取。</p></li><li><p>allow_domains：会被提取的链接的domains。</p></li><li><p>deny_domains：一定不会被提取链接的domains。</p></li><li><p>restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接。</p></li></ul><p><strong>rules</strong><br>在rules中包含一个或多个Rule对象，每个Rule对爬取网站的动作定义了特定操作。如果多个rule匹配了相同的链接，则根据规则在本集合中被定义的顺序，第一个会被使用。  </p><blockquote><pre><code>class scrapy.spiders.Rule(    link_extractor,     callback = None,     cb_kwargs = None,     follow = None,     process_links = None,     process_request = None)</code></pre></blockquote><p>主要参数：  </p><ul><li><p>link_extractor：是一个Link Extractor对象，用于定义需要提取的链接。</p></li><li><p>callback： 从link_extractor中每获取到链接时，参数所指定的值作为回调函数，该回调函数接受一个response作为其第一个参数。</p></li></ul><p>注意：当编写爬虫规则时，避免使用parse作为回调函数。由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了 parse方法，crawl spider将会运行失败。</p><ul><li><p>follow：是一个布尔(boolean)值，指定了根据该规则从response提取的链接是否需要跟进。 如果callback为None，follow 默认设置为True ，否则默认为False。</p></li><li><p>process_links：指定该spider中哪个的函数将会被调用，从link_extractor中获取到链接列表时将会调用该函数。该方法主要用来过滤。</p></li><li><p>process_request：指定该spider中哪个的函数将会被调用， 该规则提取到每个request时都会调用该函数。 (用来过滤request)</p></li></ul><p>下面写一个实例：<br><img src="https://i.imgur.com/QTBG9mC.png" alt=""></p><h2 id="Logging"><a href="#Logging" class="headerlink" title="Logging"></a>Logging</h2><p>Scrapy提供了log功能，可以通过 logging 模块使用。</p><p>可以修改配置文件settings.py，任意位置添加下面两行，效果会清爽很多。</p><blockquote><pre><code>LOG_FILE = &quot;TencentSpider.log&quot;LOG_LEVEL = &quot;INFO&quot;</code></pre></blockquote><p>Log levels<br>Scrapy提供5层logging级别:</p><ul><li>CRITICAL - 严重错误(critical)</li><li>ERROR - 一般错误(regular errors)</li><li>WARNING - 警告信息(warning messages)</li><li>INFO - 一般信息(informational messages)</li><li>DEBUG - 调试信息(debugging messages)</li></ul><p>logging设置<br>通过在setting.py中进行以下设置可以被用来配置logging:</p><ul><li>LOG_ENABLED 默认: True，启用logging</li><li>LOG_ENCODING 默认: ‘utf-8’，logging使用的编码</li><li>LOG_FILE 默认: None，在当前目录里创建logging输出文件的文件名</li><li>LOG_LEVEL 默认: ‘DEBUG’，log的最低级别</li><li>LOG_STDOUT 默认: False 如果为 True，进程所有的标准输出(及错误)将会被重定向到log中。例如，执行 print “hello” ，其将会在Scrapy log中显示。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CrawlSpider是Spider的派生类，Spider类的设计原则是只爬取start_url列表中的网页，而CrawlSpider类定义了一些规则(rule)来提供跟进link的方便的机制，从爬取的网页中获取link并继续爬取。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy抓取装修图片</title>
    <link href="http://yoursite.com/2018/08/26/Scrapy%E6%8A%93%E5%8F%96%E8%A3%85%E4%BF%AE%E5%9B%BE%E7%89%87/"/>
    <id>http://yoursite.com/2018/08/26/Scrapy抓取装修图片/</id>
    <published>2018-08-26T12:35:00.000Z</published>
    <updated>2018-08-27T03:43:26.518Z</updated>
    
    <content type="html"><![CDATA[<p>scrapy框架也学了几天了，这次来试一下抓取360的家居装修图，加强对框架的理解和掌握。<br><a id="more"></a>  </p><h1 id="抓取分析"><a href="#抓取分析" class="headerlink" title="抓取分析"></a>抓取分析</h1><h2 id="网页分析"><a href="#网页分析" class="headerlink" title="网页分析"></a>网页分析</h2><p>目标站点定位： <a href="http://image.so.com/" target="_blank" rel="noopener">http://image.so.com/</a><br>打开浏览器后，我们在发现图片是通过发送Ajax请求加载的，所以我们直接找到对应的请求接口，然后访问，获取里面的json数据，数据中包含的图片的链接地址和其他信息，另外，我们拿到图片链接后，再去请求，便可以下载图片到本地。基本思路就是这样了。下面开始做吧。<br><img src="https://i.imgur.com/ZsIbFIH.png" alt=""></p><h1 id="url分析"><a href="#url分析" class="headerlink" title="url分析"></a>url分析</h1><p>寻找url规律：<br><img src="https://i.imgur.com/uA6x16d.png" alt=""></p><p>sn为偏移量，每页会显示30张图片，其他参数不变，那么我们可以构造一个参数字典，然后通过urlencode方法转成URL的格式，这样就构造出了需要访问的url。</p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h1><p>首先，创建项目，并生成一个Spider。</p><h2 id="编写items文件，确定抓取目标"><a href="#编写items文件，确定抓取目标" class="headerlink" title="编写items文件，确定抓取目标"></a>编写items文件，确定抓取目标</h2><p><img src="https://i.imgur.com/qvDOYkY.png" alt=""><br>这里，我们打算将图片信息放到mongodb和mysql中，先在这里定义集合和表名称，等会方便插入数据。  </p><h2 id="编写spider文件，生成数据和请求。"><a href="#编写spider文件，生成数据和请求。" class="headerlink" title="编写spider文件，生成数据和请求。"></a>编写spider文件，生成数据和请求。</h2><p><img src="https://i.imgur.com/REXVhXc.png" alt=""><br>这里，我们只要遍历json文件中的list字段就可以获取相应的信息。</p><h2 id="编写管道文件Pipeline，处理数据"><a href="#编写管道文件Pipeline，处理数据" class="headerlink" title="编写管道文件Pipeline，处理数据"></a>编写管道文件Pipeline，处理数据</h2><ul><li><p>存储到MongoDB<br><img src="https://i.imgur.com/foSb5dd.png" alt=""></p></li><li><p>存储到mysql<br>这里我们先提前在mysql中创建好相应的数据库和表，用来保存接下来的信息，这里就不多讲了。<br><img src="https://i.imgur.com/MWervfP.png" alt="">  </p></li><li><p>存储图片到本地<br>这里需要用到Scrapy专门处理图片的Pipeline，我们先定义一个当前目录下的文件存储路径 <strong>IMAGES_STORE = ‘.\images’</strong>,之后我们下载的图片会保存在这里。<br><img src="https://i.imgur.com/yJ1Fvgo.png" alt=""></p></li><li><p>setting文件配置<br>接下来我们需要配置setting.py里面的信息了。<br>数据库信息，设置最大抓取页数，请求头啥的。<br><img src="https://i.imgur.com/ZsksNAZ.png" alt=""></p></li></ul><p>管道文件<br><img src="https://i.imgur.com/xxEIa6B.png" alt=""></p><h2 id="运行程序"><a href="#运行程序" class="headerlink" title="运行程序"></a>运行程序</h2><p><img src="https://i.imgur.com/yDqIwWr.png" alt=""></p><p>这样，scrapy便开始下载图片和图片信息了，刚开始出现了几次报错，有参数的类型错误，还有初始化Mysql类属性时，命名造成了干涉等，导致后面方法报错。  </p><p>下面效果展示：<br>MONGO存储：<br><img src="https://i.imgur.com/au1ZCLy.png" alt=""></p><p>MySQL存储：<br><img src="https://i.imgur.com/NtIm5Me.png" alt=""></p><p>本地存储：<br><img src="https://i.imgur.com/AysaEQB.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;scrapy框架也学了几天了，这次来试一下抓取360的家居装修图，加强对框架的理解和掌握。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架学习之Item Pipeline</title>
    <link href="http://yoursite.com/2018/08/26/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E4%B9%8BItem-Pipeline/"/>
    <id>http://yoursite.com/2018/08/26/Scrapy框架学习之Item-Pipeline/</id>
    <published>2018-08-26T06:56:51.000Z</published>
    <updated>2018-08-26T07:15:49.020Z</updated>
    
    <content type="html"><![CDATA[<p>当Item在Spider中被收集之后，它将会被传递到Item Pipeline，这些Item Pipeline组件按定义的顺序处理Item。<br><a id="more"></a></p><h1 id="Item-Pipeline"><a href="#Item-Pipeline" class="headerlink" title="Item Pipeline"></a>Item Pipeline</h1><p>每个Item Pipeline都是实现了简单方法的Python类，比如决定此Item是丢弃而存储。以下是item pipeline的一些典型应用：</p><ul><li>验证爬取的数据(检查item包含某些字段，比如说name字段)</li><li>查重(并丢弃)</li><li>将爬取结果保存到文件或者数据库中</li></ul><h2 id="编写item-pipeline"><a href="#编写item-pipeline" class="headerlink" title="编写item pipeline"></a>编写item pipeline</h2><p>item pipiline组件是一个独立的Python类，其中process_item()方法必须实现:  </p><p>常用方法举例：<br><img src="https://i.imgur.com/6Drjw3H.png" alt=""></p><p>我的实例：pipeline将所有(从所有’spider’中)爬取到的item，存储到一个独立地items.json 文件，每行包含一个序列化为’JSON’格式的’item’:<br><img src="https://i.imgur.com/2Ssebh0.png" alt=""></p><h2 id="启用一个Item-Pipeline组件"><a href="#启用一个Item-Pipeline组件" class="headerlink" title="启用一个Item Pipeline组件"></a>启用一个Item Pipeline组件</h2><p>为了启用Item Pipeline组件，必须将它的类添加到 settings.py文件ITEM_PIPELINES 配置。  </p><p><img src="https://i.imgur.com/sbi6xsH.png" alt=""></p><p>分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内（0-1000随意设置，数值越低，组件的优先级越高）。</p><h2 id="启动爬虫"><a href="#启动爬虫" class="headerlink" title="启动爬虫"></a>启动爬虫</h2><p>我们启动爬虫，就可以得到一个json文件。  </p><p><img src="https://i.imgur.com/sDXT1kj.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当Item在Spider中被收集之后，它将会被传递到Item Pipeline，这些Item Pipeline组件按定义的顺序处理Item。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架学习之Spider</title>
    <link href="http://yoursite.com/2018/08/26/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E4%B9%8BSpider/"/>
    <id>http://yoursite.com/2018/08/26/Scrapy框架学习之Spider/</id>
    <published>2018-08-26T02:29:45.000Z</published>
    <updated>2018-08-26T05:14:45.143Z</updated>
    
    <content type="html"><![CDATA[<p>Spider类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。 换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。<br><a id="more"></a>  </p><h1 id="Spider类"><a href="#Spider类" class="headerlink" title="Spider类"></a>Spider类</h1><p>class scrapy.Spider是最基本的类，所有编写的爬虫必须继承这个类。</p><p>主要用到的函数及调用顺序为：</p><p>__init__() : 初始化爬虫名字和start_urls列表</p><p>start_requests() 调用make_requests_from url():生成Requests对象交给Scrapy下载并返回response</p><p>parse() : 解析response，并返回Item或Requests（需指定回调函数）。Item传给Item pipline持久化 ， 而Requests交由Scrapy下载，并由指定的回调函数处理（默认parse())，一直进行循环，直到处理完所有的数据为止。</p><h2 id="Spider类的源码理解"><a href="#Spider类的源码理解" class="headerlink" title="Spider类的源码理解"></a>Spider类的源码理解</h2><p><img src="https://i.imgur.com/DK3qWhu.png" alt=""><br><img src="https://i.imgur.com/reWWTRh.png" alt=""></p><h2 id="主要属性和方法"><a href="#主要属性和方法" class="headerlink" title="主要属性和方法"></a>主要属性和方法</h2><ul><li>name</li></ul><p>定义spider名字的字符串。</p><p>例如，如果spider爬取 mywebsite.com ，该spider通常会被命名为 mywebsite</p><ul><li>allowed_domains</li></ul><p>包含了spider允许爬取的域名(domain)的列表，可选。</p><ul><li>start_urls</li></ul><p>初始URL元祖/列表。当没有制定特定的URL时，spider将从该列表中开始进行爬取。</p><ul><li>start_requests(self)</li></ul><p>该方法必须返回一个可迭代对象(iterable)。该对象包含了spider用于爬取（默认实现是使用 start_urls 的url）的第一个Request。</p><p>当spider启动爬取并且未指定start_urls时，该方法被调用。</p><ul><li>parse(self, response)</li></ul><p>当请求url返回网页没有指定回调函数时，默认的Request对象回调函数。用来处理网页返回的response，以及生成Item或者Request对象。</p><ul><li>log(self, message[, level, component])</li></ul><p>使用 scrapy.log.msg() 方法记录(log)message。 </p><p>下面附上自己写的spider类实例：   </p><p><strong>items.py文件：确定爬取内容</strong></p><p><img src="https://i.imgur.com/cN7SeIG.png" alt=""></p><p><strong>spider文件：parse()方法</strong>  </p><p><img src="https://i.imgur.com/D0PQrz4.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spider类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。 换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架学习之scrapy shell和selector</title>
    <link href="http://yoursite.com/2018/08/25/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E4%B9%8Bscrapy-shell%E5%92%8Cselector/"/>
    <id>http://yoursite.com/2018/08/25/Scrapy框架学习之scrapy-shell和selector/</id>
    <published>2018-08-25T12:10:32.000Z</published>
    <updated>2018-08-25T12:57:04.463Z</updated>
    
    <content type="html"><![CDATA[<p>Scrapy终端是一个交互终端，我们可以在未启动spider的情况下尝试及调试代码，也可以用来测试XPath或CSS表达式，查看他们的工作方式，方便我们爬取的网页中提取的数据。<br><a id="more"></a>  </p><h1 id="Scrapy-Shell"><a href="#Scrapy-Shell" class="headerlink" title="Scrapy Shell"></a>Scrapy Shell</h1><p>进入项目的根目录，执行下列命令来启动shell:</p><blockquote><pre><code>scrapy shell &quot;http://www.itcast.cn/channel/teacher.shtml&quot;</code></pre></blockquote><p><img src="https://i.imgur.com/F45RBJ9.png" alt=""></p><p>Scrapy Shell根据下载的页面会自动创建一些方便使用的对象，例如 Response 对象，以及 Selector 对象 (对HTML及XML内容)。</p><p>当shell载入后，将得到一个包含response数据的本地 response 变量，输入 response.body将输出response的包体，输出 response.headers 可以看到response的报头。</p><p>输入 response.selector 时， 将获取到一个response 初始化的类 Selector 的对象，此时可以通过使用 response.selector.xpath()或response.selector.css() 来对 response 进行查询。<br>Xpath对象：<br><img src="https://i.imgur.com/z6EN1U1.png" alt=""></p><h1 id="Selectors选择器"><a href="#Selectors选择器" class="headerlink" title="Selectors选择器"></a>Selectors选择器</h1><p>Scrapy Selectors 内置 XPath 和 CSS Selector 表达式机制；Selector有四个基本的方法，最常用的还是xpath:</p><ul><li>xpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表</li><li>extract(): 序列化该节点为Unicode字符串并返回list</li><li>extract_first(): 取出上述节点中的第一个元素，可以传入默认值。</li><li>css(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表，语法同 BeautifulSoup4 </li><li>re(): 根据传入的正则表达式对数据进行提取，返回Unicode字符串list列表  </li></ul><p>XPath表达式的一些例子及对应的含义:</p><blockquote><pre><code>/html/head/title: 选择&lt;HTML&gt;文档中 &lt;head&gt; 标签内的 &lt;title&gt; 元素/html/head/title/text(): 选择上面提到的 &lt;title&gt; 元素的文字//td: 选择所有的 &lt;td&gt; 元素//div[@class=&quot;mine&quot;]: 选择所有具有 class=&quot;mine&quot; 属性的 div 元素</code></pre></blockquote><p>css表达式的一些例子及对应的含义:  </p><blockquote><pre><code>a[href=&quot;image.html&quot;] img: href熟悉为image.html的a节点下的img节点a[href=&quot;image.html&quot;]::text: 获取href为image.html属性a节点下的文本a[href=&quot;image.html&quot;] img::attr(src): 上述img节点下src的属性 </code></pre></blockquote><p>另外，Selector还支持正则re，可以与xpath， css搭配使用</p><blockquote><pre><code>response.xpath(&apos;//a/text()&apos;).re(&apos;name:\s(.*)&apos;) 注意response不能直接调用re，可以先使用xpath和css匹配后，在使用re</code></pre></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scrapy终端是一个交互终端，我们可以在未启动spider的情况下尝试及调试代码，也可以用来测试XPath或CSS表达式，查看他们的工作方式，方便我们爬取的网页中提取的数据。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架学习之入门案例</title>
    <link href="http://yoursite.com/2018/08/23/Scrapy%E6%A1%86%E6%9E%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2018/08/23/Scrapy框架基础学习/</id>
    <published>2018-08-23T12:50:37.000Z</published>
    <updated>2018-08-25T07:54:28.705Z</updated>
    
    <content type="html"><![CDATA[<p>Scrapy是一个用纯Python实现，为了爬取网站数据、提取结构性数据而编写的应用框架；我们只需要定制开发几个模块就可以轻松的实现一个爬虫，用来抓取网页内容以及各种图片，非常之方便。Scrapy 使用了Twisted多线程异步网络框架来处理网络通讯，可以加快我们的下载速度，不用自己去实现异步框架，并且包含了各种中间件接口，可以灵活的完成各种需求。<br><a id="more"></a>  </p><h1 id="Scrapy架构图"><a href="#Scrapy架构图" class="headerlink" title="Scrapy架构图"></a>Scrapy架构图</h1><p>——————————–<img src="https://i.imgur.com/vNUUr2S.png" alt=""></p><p>它可以分为以下几个部分：  </p><ul><li><strong>Scrapy Engine(引擎):</strong> 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。</li><li><strong>Scheduler(调度器):</strong> 它负责接受<code>引擎</code>发送过来的Request请求，并按照一定的方式进行整理排列，入队，当<code>引擎</code>需要时，交还给<code>引擎</code>。</li><li><strong>Downloader（下载器)：</strong>负责下载<code>Scrapy Engine(引擎)</code>发送的所有Requests请求，并将其获取到的Responses交还给<code>Scrapy Engine(引擎)</code>，由<code>引擎</code>交给<code>Spider</code>来处理。</li><li><strong>spiders（爬虫）：</strong>它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给<code>引擎</code>，再次进入<code>Scheduler(调度器)</code>。</li><li><strong>Item Pipeline(管道)：</strong>它负责处理<code>Spider</code>中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方.</li><li><strong>Downloader Middlewares（下载中间件）：</strong>你可以当作是一个可以自定义扩展下载功能的组件。</li><li><strong>Spider Middlewares（Spider中间件）：</strong>你可以理解为是一个可以自定扩展和操作<code>引擎</code>和<code>Spider</code>中间<code>通信</code>的功能组件（比如进入<code>Spider</code>的Responses;和从<code>Spider</code>出去的Requests）。</li></ul><h1 id="Scrapy的运作流程"><a href="#Scrapy的运作流程" class="headerlink" title="Scrapy的运作流程"></a>Scrapy的运作流程</h1><p>代码写好，程序开始运行…</p><ol><li><code>引擎</code>：Hi！<code>Spider</code>, 你要处理哪一个网站？</li><li><code>Spider</code>：老大要我处理xxxx.com。</li><li><code>引擎</code>：你把第一个需要处理的URL给我吧。</li><li><code>Spider</code>：给你，第一个URL是xxxxxxx.com。</li><li><code>引擎</code>：Hi！<code>调度器</code>，我这有request请求你帮我排序入队一下。</li><li><code>调度器</code>：好的，正在处理你等一下。</li><li><code>引擎</code>：Hi！<code>调度器</code>，把你处理好的request请求给我。</li><li><code>调度器</code>：给你，这是我处理好的request。</li><li><code>引擎</code>：Hi！下载器，你按照老大的<code>下载中间件</code>的设置帮我下载一下这个request请求</li><li><code>下载器</code>：好的！给你，这是下载好的东西。（如果失败：sorry，这个request下载失败了。然后<code>引擎</code>告诉<code>调度器</code>，这个request下载失败了，你记录一下，我们待会儿再下载）</li><li><code>引擎</code>：Hi！<code>Spider</code>，这是下载好的东西，并且已经按照老大的<code>下载中间件</code>处理过了，你自己处理一下（注意！这儿responses默认是交给<code>def parse()</code>这个函数处理的）</li><li><code>Spider</code>：（处理完毕数据之后对于需要跟进的URL），Hi！<code>引擎</code>，我这里有两个结果，这个是我需要跟进的URL，还有这个是我获取到的Item数据。</li><li><code>引擎</code>：Hi ！<code>管道</code>我这儿有个item你帮我处理一下！<code>调度器</code>！这是需要跟进URL你帮我处理下。然后从第四步开始循环，直到获取完老大需要全部信息。</li><li><code>管道`</code>调度器`：好的，现在就做！</li></ol><p><strong>注意！只有当</strong> <code>调度器</code> <strong>中不存在任何request了，整个程序才会停止，（也就是说，对于下载失败的URL，Scrapy也会重新下载。）</strong></p><h1 id="Scrapy-爬虫步骤："><a href="#Scrapy-爬虫步骤：" class="headerlink" title="Scrapy 爬虫步骤："></a>Scrapy 爬虫步骤：</h1><ul><li>新建项目 (scrapy startproject xxx)：在终端中执行命令，新建一个爬虫项目，会创建一个xxx文件夹</li><li>明确目标 （编写items.py）：明确你想要抓取的目标，定义爬取的数据结构</li><li>制作爬虫 （spiders/xxspider.py）：制作爬虫开始爬取网页</li><li>存储内容 （pipelines.py）：设计管道存储爬取内容</li></ul><p>下面来就来讲一个简单的案例来熟悉以下scrapy的操作：  </p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ul><li>创建一个Scrapy项目</li><li>定义提取的结构化数据(Item)</li><li>编写爬取网站的 Spider 并提取出结构化数据(Item)</li><li>编写 Item Pipelines 来存储提取到的Item(即结构化数据)</li></ul><h2 id="一-新建项目-scrapy-startproject-studyScrapy"><a href="#一-新建项目-scrapy-startproject-studyScrapy" class="headerlink" title="一. 新建项目(scrapy startproject studyScrapy)"></a>一. 新建项目(scrapy startproject studyScrapy)</h2><ul><li><p>在开始爬取之前，必须创建一个新的Scrapy项目。进入自定义的项目目录中，比如我的是<strong>F:\spider_projects</strong> 运行下列命令：</p><blockquote><pre><code>scrapy startproject studyScrapy</code></pre></blockquote></li><li><p>其中， studyScrapy 为项目名称，可以看到将会创建一个 studyScrapy 的文件夹，目录结构大致如下：<br><img src="https://i.imgur.com/blWwfGE.png" alt=""></p></li></ul><p>下面来简单介绍一下各个主要文件的作用：  </p><ul><li><p>studyScrapy/spiders/ ：存储爬虫代码的文件夹</p></li><li><p>studyScrapy/ ：该项目的Python模块，需要从这里引入；即import</p></li><li><p>studyScrapy/items.py ：定义项目的目标文件，即需要爬取的数据</p></li><li><p>studyScrapy/middlewares.py： 定义爬取时的中间件</p></li><li><p>studyScrapy/pipelines.py ：项目的管道文件，</p></li><li><p>studyScrapy/settings.py ：项目的设置文件</p></li><li><p>scrapy.cfg ：scrapy部署时的配置文件</p></li></ul><h2 id="二-创建QuotesSpider类-studyScrapy-spiders"><a href="#二-创建QuotesSpider类-studyScrapy-spiders" class="headerlink" title="二. 创建QuotesSpider类(studyScrapy/spiders)"></a>二. 创建QuotesSpider类(studyScrapy/spiders)</h2><p>我们打算抓取谚语网：<a href="http://quotes.toscrape.com" target="_blank" rel="noopener">http://quotes.toscrape.com</a> 。使用命令行生成一个Spider：  </p><blockquote><pre><code>scrapy genspider quotes quotes.toscrape.com</code></pre></blockquote><p>cd 到studyScrapy文件目录下，执行上述语句，然后我们在spiders文件下发现一个quotes.py文件。</p><p><img src="https://i.imgur.com/n2EJ8hO.png" alt=""></p><p>这个类必须继承scrapy.Spider， 这里有三个属性，和一个方法。</p><ul><li><code>name = &quot;&quot;</code>：这个爬虫的识别名称，必须是唯一的，在不同的爬虫必须定义不同的名字。</li><li><code>allow_domains = []</code>是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页，不存在的URL会被忽略。</li><li><code>start_urls = []</code>：爬取的URL的列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成。</li><li><code>parse(self, response)</code>：解析的方法，每个初始URL完成下载后将被调用，调用的时候传入从每一个URL传回的Response对象来作为唯一参数，主要作用如下：<ol><li>负责解析返回的网页数据(response.body)，提取结构化数据(生成item)</li><li>生成需要下一页的URL请求；</li><li></li></ol></li></ul><h2 id="三-创建Item类"><a href="#三-创建Item类" class="headerlink" title="三. 创建Item类"></a>三. 创建Item类</h2><ol><li>打开studyScrapy目录下的items.py</li><li>Item 定义结构化数据字段，用来保存爬取到的数据，有点像Python中的dict，但是提供了一些额外的保护机制来减少错误。</li><li>创建Item需要继承scrapy.Item类，并且定义类型为scrapy.Field的字段。</li><li>接下来，创建一个QuoteItem类，定义我们要获取的内容字段，text， author， tags。</li></ol><p>此时，将items.py修改如下：</p><p><img src="https://i.imgur.com/cOhEe1R.png" alt=""></p><h2 id="四-修改parse-方法，解析Response"><a href="#四-修改parse-方法，解析Response" class="headerlink" title="四. 修改parse()方法，解析Response"></a>四. 修改parse()方法，解析Response</h2><p>parse方法中的参数response是start_urls里面的链接爬取的结果；所以我们可直接对response变量中的内容进行解析。<br>首先我们看一下网页结构，每一页都有class为quote的区块，每个区块里包含了text，author，tags，我们先找出所有quote，在提取出里面的信息。<br><img src="https://i.imgur.com/yBzpX75.png" alt=""></p><p>这里需要使用上面创建的Item，可以将它理解为一个字典，不过在声明的时候需要实例化。然后再将图中的结果解析出来赋值给Item中的每个字段，最后使用yield返回即可。</p><p><img src="https://i.imgur.com/6PrClK0.png" alt=""></p><p>这样就获取了初始页面的内容了。 这里为什么用yield而不用return呢？这里要从parse()方法的工作机制来说：</p><blockquote><pre><code>因为使用的是yield，parse函数将会被当做一个生成器使用。scrapy会逐一获取parse方法中生成的结果，并判断该结果是一个什么样的类型；如果是request则加入爬取队列，如果是item类型则使用pipeline处理，其他类型则返回错误信息。scrapy取到第一部分的request不会立马就去发送这个request，只是把这个request放到队列里，然后接着从生成器里获取；取完第一部分的request，然后再获取第二部分的item，取到item了，就会放到对应的pipeline里处理；parse()方法作为回调函数(callback)赋值给了Request，指定parse()方法来处理这些请求 scrapy.Request(url, callback=self.parse)Request对象经过调度，执行生成 scrapy.http.response()的响应对象，并送回给parse()方法，直到调度器中没有Request（递归的思路）取完之后，parse()工作结束，引擎再根据队列和pipelines中的内容去执行相应的操作；程序在取得各个页面的items前，会先处理完之前所有的request队列里的请求，然后再提取items。</code></pre></blockquote><h2 id="五-处理后续的Request"><a href="#五-处理后续的Request" class="headerlink" title="五. 处理后续的Request"></a>五. 处理后续的Request</h2><p>上面知识完成了首页的抓取，那么下一页怎么来完成了，以前我写的爬虫都是分析了url请求的规律，然后构造请求再取获取页面内容。这里我们需要从当前页面找到信息来生成下一个请求，然后再下一个请求中寻找信息构造下下一个请求。就这样循环迭代，实现整个网站的爬取。</p><p><img src="https://i.imgur.com/C0tjdXD.png" alt=""></p><p>将网页下拉到底部，查看下一页按钮，链接为/page/2/， 完整的URL就是 <a href="http://quotes.toscrape.com/page/2/" target="_blank" rel="noopener">http://quotes.toscrape.com/page/2/</a><br>我们现在就可以构造RUL请求了，这里又要用到一个新函数<strong>scrapy.Requst</strong>。它有两个参数—-url， callback。  </p><blockquote><pre><code>url: 新的请求链接callback： 回调函数，当指定了该回调函数的请求完成后，获取到响应，引擎会将该响应作为参数传递给这个回调函数。 回调函数进行解析或者生成下一个请求，这里的回调函数就是上面的parse()函数。</code></pre></blockquote><p>这里由于parse()就是解析text，author，tags的方法，并且下一页面的结构与刚才解析的页面是一样的，所以我们可以再次使用parse来解析页面。接下来我们先利用选择器得到下一页的URL，生成新请求；在parse()方法后面追加如下代码即可：  </p><p><img src="https://i.imgur.com/BvG1S5X.png" alt=""></p><h2 id="六-运行"><a href="#六-运行" class="headerlink" title="六. 运行"></a>六. 运行</h2><p>然后，进入项目所在目录，直接运行 <strong>scrapy crawl quotes</strong>，Scrapy就开始进行抓取啦，当然我们还没设置中间件Middlewares和Piplines里面的内容，后面写其他项目会细细说明的。<br><img src="https://i.imgur.com/pI4YD37.png" alt=""></p><p>另外，可以在项目的根目录下新建main.py文件,在爬虫代码中设置好断点，用于调试 </p><blockquote><pre><code>from scrapy import cmdlinecmdline.execute(&apos;scrapy crawl quotes&apos;.split())</code></pre></blockquote><h2 id="七-保存结果-Item-Pipeline"><a href="#七-保存结果-Item-Pipeline" class="headerlink" title="七.保存结果(Item Pipeline)"></a>七.保存结果(Item Pipeline)</h2><p><strong>保存到文件</strong> </p><p>针对小项目数据，我们可以直接保存成文件格式。</p><p>scrapy保存信息的最简单的方法主要有以下几种，-o 输出指定格式的文件，命令如下：</p><ul><li><p>scrapy crawl quotes -o quotes.json # 输出json文件</p></li><li><p>scrapy crawl quotes -o quotes.csv  # 输出csv文件</p></li><li><p>scrapy crawl quotes -o quotes.xml  # 输出xml格式文件</p></li><li><p>scrapy crawl quotes -o <a href="ftp://user:passwd@ftp.xxxx.com/path/to/quotes.csv" target="_blank" rel="noopener">ftp://user:passwd@ftp.xxxx.com/path/to/quotes.csv</a>  </p></li></ul><p>同时还支持ftp， s3等远程输出。</p><p><strong>保存到数据库</strong><br>如果想输出到数据库中的话，以mongoDB为例，我们可以定义<strong>项目管道Item Pipelin</strong>e进行操作。<br>当Item在Spider中被收集之后，它将会被传递到Item Pipeline，这些Item Pipeline组件按定义的顺序处理Item。<br>每个Item Pipeline都是实现了简单方法的Python类，比如决定此Item是丢弃而存储。以下是item pipeline的常用处理操作：</p><blockquote><pre><code>1. 清理HTML数据2. 验证爬虫数据，检查爬虫字段3. 查重并丢弃重复内容4. 将爬取结果保存到数据库中</code></pre></blockquote><p>编写item pipeline很简单，item pipiline组件是一个独立的Python类，其中process_item()方法必须实现。<br>下面是一些方法示例：<br><img src="https://i.imgur.com/AmBVtAr.png" alt=""></p><p>下面为实际的代码：<br><img src="https://i.imgur.com/ITitSws.png" alt=""><br><img src="https://i.imgur.com/ViRiqt5.png" alt=""></p><p>这样我们便完成了一个入门案例了，正式踏入Scrapy框架的学习之路。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scrapy是一个用纯Python实现，为了爬取网站数据、提取结构性数据而编写的应用框架；我们只需要定制开发几个模块就可以轻松的实现一个爬虫，用来抓取网页内容以及各种图片，非常之方便。Scrapy 使用了Twisted多线程异步网络框架来处理网络通讯，可以加快我们的下载速度，不用自己去实现异步框架，并且包含了各种中间件接口，可以灵活的完成各种需求。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>破解验证码识别</title>
    <link href="http://yoursite.com/2018/08/20/%E7%A0%B4%E8%A7%A3%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB/"/>
    <id>http://yoursite.com/2018/08/20/破解验证码识别/</id>
    <published>2018-08-20T02:49:58.000Z</published>
    <updated>2018-08-22T04:21:28.874Z</updated>
    
    <content type="html"><![CDATA[<p>现在，很多网站采取各种各样的措施来反爬虫，其中之一就是使用验证码。当我们访问网页时，必须先通过验证码才能够访问页面。下面我们便来讲2种验证码的识别方式和一些思路。当然我们也可以直接使用付费的打码平台，那样可以增加识别的准确度，毕竟出了钱的嘛。哈哈！<br><a id="more"></a>  </p><h2 id="PIL库"><a href="#PIL库" class="headerlink" title="PIL库"></a>PIL库</h2><p>其实，验证码识别归根到底还是对各种各样图片的识别和操作，python中有很对图像处理的库，其中PIL就是其中之一。 所以在处理验证码识别之前，必须先了解PIL库。 下面附上其API源码地址，以及对应的学习博客。<br>源码地址：<a href="https://pillow-cn.readthedocs.io/zh_CN/latest/reference/index.html" target="_blank" rel="noopener">https://pillow-cn.readthedocs.io/zh_CN/latest/reference/index.html</a><br>参考博客：<a href="https://blog.csdn.net/louishao/article/details/69879981" target="_blank" rel="noopener">https://blog.csdn.net/louishao/article/details/69879981</a><br>下面我们就开始验证码识别之路了。</p><h2 id="图形验证码"><a href="#图形验证码" class="headerlink" title="图形验证码"></a>图形验证码</h2><p>以中国知网为例：<br><img src="https://i.imgur.com/KfnZxWP.png" alt=""></p><p>首先，我们先拿到上图中绿线标记的验证码，下载到本地项目文件中，<br>然后，编写如下代码:<br><img src="https://i.imgur.com/Dx7xscT.png" alt=""><br>输出结果为：486V， 可是实际图片为4S6V，这是因为验证码内多余线条干扰了图片的识别，像这类情况，还需要做出额外的处理，比如转灰度，二值化等。对应的方法如下：<br><img src="https://i.imgur.com/WagZ5W3.png" alt=""></p><p>当然，实际处理中并不是这样，一般我们会先对模糊图片进行灰度处理后，再设定二值化的阈值，实际处理如下。<br><img src="https://i.imgur.com/li0queT.png" alt=""></p><p>输出结果：4S6V<br>进行识别时，先设定好二值化阈值threshold，进行适当调试，直到图片能正常识别为止。</p><h2 id="滑动验证码"><a href="#滑动验证码" class="headerlink" title="滑动验证码"></a>滑动验证码</h2><p><strong>过程分析：</strong></p><p>滑动验证码主要的验证方式是拖动滑块，拼合图像；如图象完全拼合，则验证成功，即表单提交成功，否则需要重新验证。<br>如图：<br><img src="https://i.imgur.com/pYD8C6K.png" alt=""></p><p>下面，我们就以极验的验证码为例，来讲诉一下识别方法。<br>因为极验的验证码在拖动验证码后会生成一个加密的表单提交到后台，所有为了避免麻烦我们直接用selenium模拟浏览器行为来完成验证。<br>登陆网站：极验官网</p><blockquote><pre><code>目标站点：https://account.geetest.com/login</code></pre></blockquote><p><img src="https://i.imgur.com/sVfDi03.png" alt=""></p><p>首先，我们发现登陆界面有个智能按钮，一般来说，在输入邮箱之后，点击按钮就会弹出滑动验证窗口，然后我们在拖动验证码完成图像拼接，完成验证。<br>弹出滑块验证：<br><img src="https://i.imgur.com/u1iul7H.png" alt=""><br>拖动滑块，完成验证：<br><img src="https://i.imgur.com/dKjUoEK.png" alt=""></p><p>所以，滑块验证识别需要完成以下步骤：</p><blockquote><pre><code>1. 模拟点击验证按钮2. 识别滑块的缺口位置3. 模拟拖动滑块</code></pre></blockquote><p><strong>如何实现以上步骤呢？我们先需要将任务进行分解，看似只有三大步骤，其实里面坑还有很多的，稍后会做解释。</strong>  </p><p>第一步，输入账号，获取智能按钮，使用selenium模拟点击，获取带有缺口的图片。  </p><p>第二步，获取上面缺口图片中的完整图片。这里有个地方要注意，正常情况下我们在网页源代码里是找不到完整图的，因为它被隐藏了，必须执行javascript语句才能出现完整图。<br><img src="https://i.imgur.com/v8ouv0P.png" alt=""></p><p>我们将display参数改为block，opacity参数改为1，然后进行截图，就可以拿到完整的验证码图片了。</p><p>第三步，对比两张图片的所有RGB像素点，得到缺口位置。</p><p>第四步，模拟人的拖动习惯，这里也有坑，极验的验证码增加了机器轨迹识别，匀速移动，或者随机速度移动滑块都不能通过验证，所以我们将需要拖动的总位移分成一段一段小的轨迹，先匀加速拖动后匀减速拖动。</p><p>第五步，按照规定轨迹进行拖动，完成验证。</p><p>第六步，完成账号登陆。</p><p>过程分析完了，下面我们就来写代码试一下：<br>首先，我们先将整个代码的一个逻辑思路做一个大致的概括吧。<br><img src="https://i.imgur.com/Ayg9jgg.png" alt="">  </p><p>接下来，我们便来逐一完成main函数里要实现的功能了。</p><p><strong>代码示例：</strong><br><img src="https://i.imgur.com/vCAntIv.png" alt=""><br><img src="https://i.imgur.com/p7giBb3.png" alt=""><br><img src="https://i.imgur.com/EC60eS5.png" alt=""></p><p>通过以上代码我们便拿到了完整的验证码和带有缺口的验证码。<br>缺口图片：<br><img src="https://i.imgur.com/YHzDimh.png" alt="">  </p><p>完整图片：<br><img src="https://i.imgur.com/KNLDRsB.png" alt=""></p><p>这里我在调试的时候碰到一个坑，因为chrome中，location方法不滚动，直接返回相对整个html的坐标，我的电脑是15.6寸的，显示设置上布局的缩放大小被放大到1.25倍，导致location返回的坐标与验证码的坐标有误差。修改布局为100%后就解决了。 下面便是对比图片找出缺口位置。这里我们需要遍历图片的坐标点，获取像素点的RGB数据。  </p><p><strong>代码示例</strong><br><img src="https://i.imgur.com/TQzHllN.png" alt=""></p><p>接下来就是获取滑块的移动路径和模拟拖动行为了。<br><img src="https://i.imgur.com/Rlb2brE.png" alt=""><br><img src="https://i.imgur.com/uaEc7Jg.png" alt=""></p><p>执行代码：<br><img src="https://i.imgur.com/G8pc1GS.png" alt=""> </p><p>最后终于成功了，踩了这么多坑，终于完成了滑块验证码的破解。。。现在已经实现功能，因为还可能出现其他情况，接下来我们还需要完善一下代码，其实也可封装成一个类，测试的时候我们会发现，图片会弹出小怪兽被吃了，那是因为系统识别我们是机器行为，所以不通过，这里我们需要修改加速度参数，再增加一个回调。<br><img src="https://i.imgur.com/f9DPlNE.png" alt=""></p><p>代码执行效果：<br><img src="https://i.imgur.com/br02DBK.png" alt=""></p><p>这样我们就成功破解验证码，并登陆到网页界面了。。。忙活了一上午，吃饭去了。</p><p>源码地址：<a href="https://github.com/appleguardu/spider_projects/tree/master/Captcha" target="_blank" rel="noopener">https://github.com/appleguardu/spider_projects/tree/master/Captcha</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;现在，很多网站采取各种各样的措施来反爬虫，其中之一就是使用验证码。当我们访问网页时，必须先通过验证码才能够访问页面。下面我们便来讲2种验证码的识别方式和一些思路。当然我们也可以直接使用付费的打码平台，那样可以增加识别的准确度，毕竟出了钱的嘛。哈哈！&lt;br&gt;
    
    </summary>
    
      <category term="反爬虫" scheme="http://yoursite.com/categories/%E5%8F%8D%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="反爬虫" scheme="http://yoursite.com/tags/%E5%8F%8D%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Selenium爬取淘宝商品</title>
    <link href="http://yoursite.com/2018/08/19/selenium%E7%88%AC%E5%8F%96%E6%B7%98%E5%AE%9D%E5%95%86%E5%93%81/"/>
    <id>http://yoursite.com/2018/08/19/selenium爬取淘宝商品/</id>
    <published>2018-08-19T05:28:28.000Z</published>
    <updated>2018-08-19T14:48:46.678Z</updated>
    
    <content type="html"><![CDATA[<p>前一天学习了selenium的相关用法，今天打算用selenium来模拟浏览器操作，抓取数据，抓取目标就定为淘宝网吧，接下来我们就开工啦！<br><a id="more"></a>  </p><h2 id="目标设定"><a href="#目标设定" class="headerlink" title="目标设定"></a>目标设定</h2><p>目标定为抓取淘宝iPhone手机销售的相关信息：其中包括商品的图片，名字，最关键的价格，购买量，商铺，地点等信息。目标定好了，接下来我们就去分析网页规律了。  </p><h2 id="网页分析"><a href="#网页分析" class="headerlink" title="网页分析"></a>网页分析</h2><p>首先，确定抓取入口，也就是start url为淘宝搜索页面，url的链接为：  </p><blockquote><pre><code>https://s.taobao.com/search?q=iphone</code></pre></blockquote><p>当然，这是第一页的链接。但是我们要获取的是所有页面的，先下拉到页面底部，我们发现：<br><img src="https://i.imgur.com/B2oVHX4.png" alt="">  </p><p>用黑线圈起来的是一个分页导航，其中包括前5页的链接，下一页的链接，同时还有一个任意页码跳转的链接，页面总数为100。 若要获取每一页的内容， 我们的思路是将页码从1到100的顺序进行遍历一遍。 此时我们便又两种方式来进行遍历：  </p><blockquote><pre><code>1.点击下一页，等下一页面加载出来后进行爬取2.在页面跳转框逐一输入1~100，点击确定按钮后跳转到对应页码进行爬取  </code></pre></blockquote><p>对于方法一，有一个弊端就是如果爬取过程中出现异常，程序退出了，我在点击下一页时，可能无法快速切换到后续的页面；所以，在爬取过程中还需要记录当前页码数，如果点击下一页加载失败的话，还要增加异常检查等。 方法二，我们只需输入对应页面的数值就可以跳转，比较方便。<br>当页面成功加载出来后，我们利用selenium获取页面源代码，然后进行解析，就可以获取目标信息了。  </p><h2 id="获取商品列表"><a href="#获取商品列表" class="headerlink" title="获取商品列表"></a>获取商品列表</h2><p>首先，构造出要抓取的url：  </p><blockquote><pre><code>https://s.taobao.com/search?q=iphone </code></pre></blockquote><p>q指得是搜索的关键字，改变这个参数就可以搜索不同的商品，这里我们把定义成一个变量keyword。<br>接下来我们进行代码演练了。<br><img src="https://i.imgur.com/9zhFFeT.png" alt=""><br><img src="https://i.imgur.com/2Bu9bhH.png" alt=""></p><p>输出结果：<br><img src="https://i.imgur.com/4NA5EqV.png" alt=""></p><p>这样我们便拿到了第一页商品信息的源码啦，当然这个代码还不完善，还没添加跳转功能，因为page我们设计是从第一页抓取的，当page值为2时，我们就要翻页了，那么怎么进行翻页呢？首先获取页码输入框啊，然后获取确定按钮进行操作啦。<br><img src="https://i.imgur.com/Aepl0Mc.png" alt=""><br><img src="https://i.imgur.com/94knVze.png" alt=""></p><p>我们发现，当前页面都是高亮显示的，那么我们只需要判断高亮的页码数是当前页码数就证明页面跳转成功。这里可以使用等待条件：text_to_be_present_in_element,它会等待指定的文本出现在某一节点里面时即返回成功。<br><img src="https://i.imgur.com/djIRfga.png" alt=""></p><h2 id="解析商品列表"><a href="#解析商品列表" class="headerlink" title="解析商品列表"></a>解析商品列表</h2><p>既然上面已经能拿到页面商品的源代码了，那么我们接下来就可以对源码进行解析了。<br>分析可得，每个页面有44家店铺，在源代码里我们可以发现整个商品列表使用的CSS选择器是#mainsrp-itemlist .items .item, 它匹配了整个页面的44件商品，然后我们先用items()方法将其全部提取出来，他会返回一个生成器，再使用for循环进行遍历，得到每个item对象，然后用pyquery的find方法选取我们所需要的信息即可。<br>代码如下：<br><img src="https://i.imgur.com/JDIeNPe.png" alt=""></p><p>提取结果：<br><img src="https://i.imgur.com/gxIzM05.png" alt=""></p><p>哈哈，这样我们就已经拿到第一页所有商品的数据了，接下，我们要把100页的信息都爬取下来，这里可以直接使用for循环进行遍历页码。  </p><h2 id="爬取所有商品，保存数据库"><a href="#爬取所有商品，保存数据库" class="headerlink" title="爬取所有商品，保存数据库"></a>爬取所有商品，保存数据库</h2><p>先创建数据库：<br><img src="https://i.imgur.com/cV5FihW.png" alt=""></p><p>然后for循环遍历所有页面<br><img src="https://i.imgur.com/YYEc2ko.png" alt=""></p><p>最后的结果：<br><img src="https://i.imgur.com/tjwiYUs.png" alt=""><br>MongoDB数据库：<br><img src="https://i.imgur.com/BJyyvDe.png" alt=""></p><p>然后，所有页面的iphone数据都被我们爬取下来了。。。当然，代码还可以再精简一点。。。  </p><p>源码地址:<a href="https://github.com/appleguardu/spider_projects/blob/master/taobao_iphone.py" target="_blank" rel="noopener">https://github.com/appleguardu/spider_projects/blob/master/taobao_iphone.py</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前一天学习了selenium的相关用法，今天打算用selenium来模拟浏览器操作，抓取数据，抓取目标就定为淘宝网吧，接下来我们就开工啦！&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="selenium" scheme="http://yoursite.com/tags/selenium/"/>
    
  </entry>
  
  <entry>
    <title>爬虫的思路</title>
    <link href="http://yoursite.com/2018/08/19/%E7%88%AC%E8%99%AB%E7%9A%84%E6%80%9D%E8%B7%AF/"/>
    <id>http://yoursite.com/2018/08/19/爬虫的思路/</id>
    <published>2018-08-19T01:56:37.000Z</published>
    <updated>2018-08-19T02:27:31.726Z</updated>
    
    <content type="html"><![CDATA[<p>万事开头难，刚开始写爬虫程序的时候，不知从何下手；通过最近一段时间练习写爬虫程序，也渐渐有了一些简单的思路，在这里做个笔记，供日后翻阅。<br><a id="more"></a>  </p><h2 id="准备好目标URL"><a href="#准备好目标URL" class="headerlink" title="准备好目标URL"></a>准备好目标URL</h2><p>一般分为一下两种情况：  </p><p>1.准备start_url</p><blockquote><pre><code>页码总数不确定，规律不明显通过代码提取下一页地址使用xpath/css/re提取url寻找url地址，部分参数在当前页面中，比如当前页码数和总页码数，使用这两个参数构建url</code></pre></blockquote><p>2.准备url_list</p><blockquote><pre><code>页码总数清楚url的地址规律明显</code></pre></blockquote><h2 id="发送请求，获取响应"><a href="#发送请求，获取响应" class="headerlink" title="发送请求，获取响应"></a>发送请求，获取响应</h2><blockquote><pre><code>添加随机user-agent，添加随机代理ip，如果对方判断我们是爬虫之后应该添加更多的headers字段，包含cookie；cookie的处理可以使用session来解决准备一堆能用的cookie，组成cookie池。如果不登录:        准备刚刚开始能够成功请求的cookie，即接收对方网站设置在response的cookie如果登录:       准备多个账号       使用程序获取每个账号的cookie       请求登录之后才能访问的网站随机选择上一步获取的cookie</code></pre></blockquote><h2 id="提取数据"><a href="#提取数据" class="headerlink" title="提取数据"></a>提取数据</h2><p>1.确定数据的位置:  </p><p>如果数据在当前的url地址中,提取的是列表页中的数据;直接请求列表页的url地址，不用进入详情页,提取详情页的数据<br><strong>1.确定url; 2.发送请求; 3.提取数据; 4.返回</strong></p><p>如果数据不在当前的url地址中,在其他响应中(例如Ajax请求)，寻找数据的位置<br> <strong>在network中从上而下找,使用chrome中的过滤条件，选择除了js，css, image之外的按钮,使用chrome的serach all file，搜索数字和英文</strong></p><p>如果碰到加密的Ajax请求，可使用selenium等工具获取响应源码<br><strong>分析源码，获取目标数据</strong></p><p>2.数据的提取  </p><blockquote><pre><code>1.使用re,xpath，css selector从html中提取整块的数据，先分组，之后每一组在提取2.json数据，可使用get方法进行筛选后，在进行相应的提取</code></pre></blockquote><h2 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h2><blockquote><pre><code>保存在本地json ，csv，text,使用open方法或者os创建本地文件夹，将文件保存至本地。保存在数据库，关系型MySQL，提前创建好表，将数据按字段分类；非关系型，MongoDB， redis等。</code></pre></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;万事开头难，刚开始写爬虫程序的时候，不知从何下手；通过最近一段时间练习写爬虫程序，也渐渐有了一些简单的思路，在这里做个笔记，供日后翻阅。&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>selenium的使用</title>
    <link href="http://yoursite.com/2018/08/18/selenium%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2018/08/18/selenium的使用/</id>
    <published>2018-08-18T09:10:04.000Z</published>
    <updated>2018-08-19T01:37:32.562Z</updated>
    
    <content type="html"><![CDATA[<p>对于像淘宝，京东这样的网站，虽然它也是采用Ajax获取数据，但是Ajax接口中含有许多加密参数，我们很难去直接找出其规律；所以我们需要直接使用模拟浏览器运行的方式来实现，这样就可以做到在浏览器中看到的是什么样，抓取的源码就是什么样的啦，即可见可爬，系不系很可怕。<br><a id="more"></a>  </p><h2 id="Selenium"><a href="#Selenium" class="headerlink" title="Selenium"></a>Selenium</h2><p>selenium是一个自动化测试工具，使用它可以驱动浏览器执行特定的动作，如点击，下拉，填充表单等操作，同时可以获取浏览器当前呈现页面的源代码，做到可见可爬。<br>下面以chrome浏览器为例，来说一下selenium的使用方式。  </p><blockquote><pre><code>首先，配置谷歌浏览器对于版本的ChromeDriver，可以找资源去下载，然后将chromedriver.exe放在python文件夹下的Scripts文件夹里。然后，安装python的selenium库，自行百度；接下来我们就可以使用selenium啦!!!</code></pre></blockquote><p><img src="https://i.imgur.com/mZll8Ht.png" alt=""></p><h2 id="声明浏览器对象"><a href="#声明浏览器对象" class="headerlink" title="声明浏览器对象"></a>声明浏览器对象</h2><p>首先初始化浏览器对象，接着就可调用browser对象，让其模拟浏览器操作啦！<br><img src="https://i.imgur.com/gxAlhda.png" alt=""></p><h2 id="访问页面"><a href="#访问页面" class="headerlink" title="访问页面"></a>访问页面</h2><p>使用get()方法访问页面<br><img src="https://i.imgur.com/CUAkEEO.png" alt=""></p><p>运行代码后，会弹出一个淘宝界面，并输出网页的源代码。<br><img src="https://i.imgur.com/5NwK8Ag.jpg" alt=""></p><h2 id="节点操作"><a href="#节点操作" class="headerlink" title="节点操作"></a>节点操作</h2><p>当我们用selenium驱动浏览器模拟操作时，需要先获取其操作节点，比如输入框，按钮的位置。  </p><p><strong>单个节点</strong><br>以淘宝搜索框为例，先从源代码里找到其位置。<br><img src="https://i.imgur.com/ed1UMjD.png" alt=""></p><blockquote><pre><code>&lt;input id=&quot;q&quot; name=&quot;q&quot; aria-label=&quot;请输入搜索文字&quot; accesskey=&quot;s&quot; autofocus=&quot;autofocus&quot; autocomplete=&quot;off&quot; class=&quot;search-combobox-input&quot; aria-haspopup=&quot;true&quot; aria-combobox=&quot;list&quot; role=&quot;combobox&quot; x-webkit-grammar=&quot;builtin:translate&quot; tabindex=&quot;0&quot;&gt;</code></pre></blockquote><p>这个节点或者说是标签中有很多属性，比如id属性：q， name属性：q；那么我们可以根据css选择器或者xpath获取这个节点。  </p><blockquote><pre><code>1. find_element_by_id(&apos;q&apos;)  2. find_element_by_css_selector(&apos;#q&apos;)3. find_element_by_xpath(&apos;//*[@id=&quot;q&quot;]&apos;)以上方法都可获取搜索输入框，一般获取单个节点用以上方法就足够了，另外还可以根据name或者class_name来获取。  </code></pre></blockquote><p>selenium还有通用方法，find_element(By.ID, ‘q’),需要传入两个参数，一个时查找方式，一个是查找值。返回结果为webelement类型。</p><p><strong>多个节点</strong><br>如果我们需要查找满足条件的所有节点，就绪要用到find_elements()方法。返回结果为列表类型，列表在每个节点为webelement类型。查找方式与单个节点一致。  </p><p><strong>节点交互</strong><br>selenium可以驱动浏览器执行相应的操作，我们先找到这个节点，比如淘宝搜索框，然后调用方法：  </p><blockquote><pre><code>send_keys() 输入文字clear() 清空文字click() 点击按钮 还有很多其他的操作，可以去google查</code></pre></blockquote><p><img src="https://i.imgur.com/11otno6.png" alt="">  </p><p>代码运行效果：<br><img src="https://i.imgur.com/Q0duWFX.png" alt=""></p><h2 id="动作链"><a href="#动作链" class="headerlink" title="动作链"></a>动作链</h2><p>还有一些其他的操作，他们没有像上面那样是针对单个节点的，他们没有特定的对象，比如鼠标拖动，键盘按键等，这些动作就是动作链。  </p><p><img src="https://i.imgur.com/kleR7Xi.png" alt="">  </p><p>动作前：<br><img src="https://i.imgur.com/odEyGQP.png" alt=""></p><p>动作后：<br><img src="https://i.imgur.com/yApOg8G.png" alt=""></p><h2 id="执行JavaScript"><a href="#执行JavaScript" class="headerlink" title="执行JavaScript"></a>执行JavaScript</h2><p>对于某些操作，selenium API并没有提供；比如，下拉进度条，它可直接模拟运行JS脚本，此时可以使用<strong>execute_script()</strong>方法实现。<br><img src="https://i.imgur.com/9IILHeu.png" alt=""></p><p>执行效果：<br><img src="https://i.imgur.com/rsl9U5d.png" alt=""></p><h2 id="获取节点信息"><a href="#获取节点信息" class="headerlink" title="获取节点信息"></a>获取节点信息</h2><p>通过前面讲的page_source属性可以获取网页的源代码，那么我们都可以对获取的网页源码进行解析(re, BS, pyquery)。不过selenium自带相关的方法和属性来直接提取节点信息。  </p><p><strong>获取属性值</strong><br>先选中节点，再通过节点对象调用get_attribute(‘属性名’)就可获取这个属性对应的内容。<br><img src="https://i.imgur.com/A4Ts0H9.png" alt=""> </p><p><strong>获取文本值</strong><br>每个WeBElement节点都有text属性，直接调用这个属性就可以获取节点内部的文本信息，这相当于BS中的get_text()和pyquery中的text()方法。<br>加入获取的节点为p，那么p.text就是其文本内容。这里就不演示了  </p><p><strong>获取其他值</strong><br>另外，一个节点还有其他属性，方法都差不多。  </p><blockquote><pre><code>节点.id   # 获取节点id节点.location   # 获取节点在页面中的位置节点.tag_name   # 获取节点标签名称节点.size   # 获取节点宽高</code></pre></blockquote><h2 id="切换frame"><a href="#切换frame" class="headerlink" title="切换frame"></a>切换frame</h2><p>网页中有一种节点叫作 iframe， 即子frame，相当于页面的子页面，它的结构和外部网页的结构完全一致。selenium打开页面后，默认在父级的Frame里操作，若此时页面中还有子Frame，它是不能获取到子Frame里面的节点的，所以，此时需要使用<strong>switch_to.frame()</strong>方法来切换Frame。  </p><h2 id="延时等待"><a href="#延时等待" class="headerlink" title="延时等待"></a>延时等待</h2><p>在selenium中， get()方法会在网页框架加载结束后执行，此时若获取page_soucre，可能并不是浏览器完全加载的页面，如果某些页面有额外的Ajax请求，我们就不一定能捕获成功。 所以，这里需要延时等待一定时间，确保节点已经加载完全。<br><strong>隐式等待</strong><br>当使用隐式等待时，如果selenium没有在DOM中找到节点，将继续等待，超出设定时间后，则会抛出找不到节点的异常。<br><img src="https://i.imgur.com/Zq8XTVv.png" alt=""></p><p><strong>显式等待</strong><br>由于页面加载有时候会受到网络的影响，所以隐式等待这种设置固定时间并不是很靠谱；这里引入显式等待。<br>显式等待指定了要查找的节点，然后指定一个最长等待时间，如果在规定时间加载出来便返回该节点；如果没有，即抛出超时异常。  </p><p>首先，需要引入WebDriverWait这个对象，调用until方法，传入等待条件excepted_conditions。<br><strong>expected_conditions模块:</strong>  </p><p>以下两个条件类验证title，验证传入的参数title是否等于或包含于driver.title </p><blockquote><pre><code>title_is title_contains</code></pre></blockquote><p>以下两个条件验证元素是否出现，传入的参数都是元组类型的locator，如(By.ID, ‘kw’) ;顾名思义，一个只要一个符合条件的元素加载出来就通过；另一个必须所有符合条件的元素都加载出来才行 </p><blockquote><pre><code>presence_of_element_located presence_of_all_elements_located</code></pre></blockquote><p>以下三个条件验证元素是否可见，前两个传入参数是元组类型的locator，第三个传入WebElement ;第一个和第三个其实质是一样的 </p><blockquote><pre><code>visibility_of_element_located invisibility_of_element_located visibility_of</code></pre></blockquote><p>以下两个条件判断某段文本是否出现在某元素中，一个判断元素的text，一个判断元素的value </p><blockquote><pre><code>text_to_be_present_in_element text_to_be_present_in_element_value</code></pre></blockquote><p>以下条件判断frame是否可切入，可传入locator元组或者直接传入定位方式：id、name、index或WebElement </p><blockquote><pre><code>frame_to_be_available_and_switch_to_it</code></pre></blockquote><p>以下条件判断是否有alert出现 </p><blockquote><pre><code>alert_is_present</code></pre></blockquote><p>以下条件判断元素是否可点击，传入locator<br>element_to_be_clickable</p><p>以下四个条件判断元素是否被选中，第一个条件传入WebElement对象，第二个传入locator元组<br>第三个传入WebElement对象以及状态，相等返回True，否则返回False<br>第四个传入locator以及状态，相等返回True，否则返回False </p><blockquote><pre><code>element_to_be_selected element_located_to_be_selected element_selection_state_to_be element_located_selection_state_to_be</code></pre></blockquote><p>最后一个条件判断一个元素是否仍在DOM中，传入WebElement对象，可以判断页面是否刷新了 </p><blockquote><pre><code>staleness_of</code></pre></blockquote><p><img src="https://i.imgur.com/Xpj1Vj2.png" alt=""></p><h2 id="前进和后退"><a href="#前进和后退" class="headerlink" title="前进和后退"></a>前进和后退</h2><blockquote><pre><code>browser.back()   后退，浏览器中返回上一个页面browser.forward()   前进，下一个页面</code></pre></blockquote><h2 id="Cookies操作"><a href="#Cookies操作" class="headerlink" title="Cookies操作"></a>Cookies操作</h2><p>使用selenium还可以方便的对cookies进行操作<br><img src="https://i.imgur.com/GMq4nfL.png" alt=""></p><h2 id="选项卡管理"><a href="#选项卡管理" class="headerlink" title="选项卡管理"></a>选项卡管理</h2><p>在实际访问网页时，会开启一个个选项卡。在selenium中，我们也会对其进行操作。  </p><p><img src="https://i.imgur.com/ZrNbSSZ.png" alt=""></p><p>效果：<br><img src="https://i.imgur.com/P0NCssl.png" alt=""></p><h2 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h2><p>在使用selenium时，也会遇到异常情况；比如访问超时，节点无法找到等，所有也需要使用try。。。except来捕获各类异常。<br>下面举个例子：<br><img src="https://i.imgur.com/agPTE5p.png" alt="">  </p><p>那么到目前为止，selenium的常规用法基本上都讲了一遍了，若是想了解更多相关内容的话，可以参考以下几个链接：<br>selenium官方文档：<br><a href="http://selenium-python.readthedocs.io/installation.html#introduction" title="selenium官方文档" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/installation.html#introduction</a><br>博文参考：<br><a href="https://blog.csdn.net/wenboyu/article/details/78162667" target="_blank" rel="noopener">https://blog.csdn.net/wenboyu/article/details/78162667</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于像淘宝，京东这样的网站，虽然它也是采用Ajax获取数据，但是Ajax接口中含有许多加密参数，我们很难去直接找出其规律；所以我们需要直接使用模拟浏览器运行的方式来实现，这样就可以做到在浏览器中看到的是什么样，抓取的源码就是什么样的啦，即可见可爬，系不系很可怕。&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Ajax数据的爬取</title>
    <link href="http://yoursite.com/2018/08/16/Ajax%E6%95%B0%E6%8D%AE%E7%9A%84%E7%88%AC%E5%8F%96/"/>
    <id>http://yoursite.com/2018/08/16/Ajax数据的爬取/</id>
    <published>2018-08-16T02:53:33.000Z</published>
    <updated>2018-08-17T09:18:47.463Z</updated>
    
    <content type="html"><![CDATA[<p>不知道你在写爬虫时有没有遇到这种情况：在浏览器中可以看到正常显示的页面数据，但是用requests得到的html文件里却没有这些数据。这是因为浏览器中的页面时经过JavaScript处理数据后生成的，Ajax加载就是其中一种方式，下面我们便来讲一下如何在异步加载的网页中抓取有用数据。<br><a id="more"></a>  </p><h2 id="什么是Ajax"><a href="#什么是Ajax" class="headerlink" title="什么是Ajax"></a>什么是Ajax</h2><p><strong>Ajax</strong>，就是异步的JavaScript和XML。它是利用JavaScript在保证页面不被刷新，页面链接不改变的情况下，后台与服务器进行数据交互并更新部分网页的技术。  </p><h2 id="Ajax的实现过程"><a href="#Ajax的实现过程" class="headerlink" title="Ajax的实现过程"></a>Ajax的实现过程</h2><blockquote><pre><code>第一步，创建xmlhttprequest对象，var xmlhttp =new XMLHttpRequest（);XMLHttpRequest对象用来和服务器交换数据。第二步，使用xmlhttprequest对象的open（）和send（）方法发送资源请求给服务器。第三步，使用xmlhttprequest对象的responseText或responseXML属性获得服务器的响应。第四步，onreadystatechange函数，当发送请求到服务器，我们想要服务器响应执行一些功能就需要使用onreadystatechange函数，每次xmlhttprequest对象的readyState发生改变都会触发onreadystatechange函数。</code></pre></blockquote><p>真实的数据都是通过JavaScript向服务器发送ajax请求得到的，所以要抓取这些数据，必须搞清楚这些请求是如何发送的，以及发送到了哪里，参数有哪些等等。下面我们便来分析一下。</p><h2 id="Ajax的分析方法"><a href="#Ajax的分析方法" class="headerlink" title="Ajax的分析方法"></a>Ajax的分析方法</h2><p><strong>1.分析请求</strong>  </p><p>以微博为例，当我们拖动刷新额内容时，此时网页由Ajax加载的，页面url不会发生变化，下面我们就借助chrome浏览器来分析。<br>首先，打开我巴萨新闻的官方微博链接（本人巴萨球迷。。。）  </p><p><img src="https://i.imgur.com/56DDocS.png" alt=""><br>element选项可以查看网页源代码，但这不是我们要看的。<br>点击network选项卡，刷新一下页面，发现出现很多内容，那么这些内容便是页面在加载过程中浏览器与服务器之间发送请求和接受响应内容的记录。  </p><p><img src="https://i.imgur.com/MHF0oEP.png" alt=""></p><p>这里我们找到右边条目栏中有一个getIndex的请求，点击它，查看相应信息。<br><img src="https://i.imgur.com/gJAwlqS.png" alt=""><br>我们发现Request Headers里面有一个信息围为<strong>X-Request-with：XMLHttpRequest</strong>，这就表明此请求为Ajax请求；ajax其实有特殊的请求类型，叫作<strong>xhr</strong>，稍后我们会讲到。</p><p><img src="https://i.imgur.com/RU6k1BN.png" alt=""><br>preview选项可以查看响应内容，发现响应内容为json格式。</p><p><img src="https://i.imgur.com/C1IyRw2.png" alt=""><br>response选项卡中为真实的返回数据</p><p>现在我们回到chrome浏览器最开始发送的请求<br><img src="https://i.imgur.com/XAmdgHA.png" alt=""><br>原始链接返回的response代码知识执行了一些JS。所以我们看到的微博页面的真实数据不是原始页面，而是执行了页面中的JS代码后再次向后台发送Ajax请求后，浏览器拿到数据在进一步进行渲染出来的页面。  </p><p><strong>2.筛选Ajax请求</strong>  </p><p>前面提到ajax请求类型为xhr，这里我们点击network选项卡下的XHR选项，然后刷新页面发现，下面的请求都是Ajax请求。打开一条请求，我们会看到Request URL，request Headers，response headers, response body等内容。</p><p><img src="https://i.imgur.com/VE5CfXF.png" alt=""></p><p>对Ajax请求分析完毕后，现在我们就可以模拟这些请求，来抓取数据啦啦啦！！！</p><h2 id="抓取Ajax加载的数据"><a href="#抓取Ajax加载的数据" class="headerlink" title="抓取Ajax加载的数据"></a>抓取Ajax加载的数据</h2><p><strong>1.分析请求</strong>  </p><p><img src="https://i.imgur.com/9CzVySM.png" alt=""></p><p>我们选择其中一条请求，发现这是一个GET类型额请求，</p><blockquote><pre><code>https://m.weibo.cn/api/container/getIndex?type=uid&amp;value=1990303727&amp;containerid=1076031990303727&amp;page=2</code></pre></blockquote><p>URL包含4个参数，分别是：type，value，containerid，page。<br>然后我们分析一下规律后发现，type一直为uid，value值也不变，containerid就是107603加上value，唯一变化的参数就是page，顾名思义应该就是第几页的意思。请求分析完了。接下来看看响应内容。  </p><p><strong>2.分析响应</strong>  </p><p><img src="https://i.imgur.com/CCbittX.png" alt=""></p><p><img src="https://i.imgur.com/27cs9hW.png" alt=""></p><p>观察可知，响应内容为JSON格式，然后主要的信息全包含在cardlistInfo和cards里面。其中cards里面有一个mblog，包含的信息都是微博里面的点赞数，评论数，转发数等，还有page_Info主要包含的时博文内容这些信息。接下来我们就将这些格式化的内容提取出来。哈哈！  </p><h2 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h2><p>首先构造URL，发送请求，拿到json文本内容。<br><img src="https://i.imgur.com/4gqvJj8.png" alt=""></p><p>返回内容为：<br><img src="https://i.imgur.com/w80FzyK.png" alt=""></p><p>接着就是解析json文本，提取需要的数据了，我们就提取cards里面的mblog里面的一些内容，比如正文，点赞数，评论数，转发数。<br><img src="https://i.imgur.com/JP5arE0.png" alt=""></p><p>这样我们便抓取到第二页所需的微博信息啦：<br><img src="https://i.imgur.com/uLDc29w.png" alt=""></p><p>之前是抓取的一个页面的数据，如果要抓取所有页面，只需传入page的最大值，然后使用for循环即可。  </p><h2 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h2><p>我们将数据保存至mongodb中，由于是字典，我们直接进行存储即可<br>连接数据库：<br><img src="https://i.imgur.com/aUFTnAK.png" alt=""></p><p>保存：<br><img src="https://i.imgur.com/x74m1XQ.png" alt=""><br>运行结果：<br><img src="https://i.imgur.com/22jrCiD.png" alt=""></p><p>查看数据库：<br><img src="https://i.imgur.com/7enGi11.png" alt=""></p><p>这样我们便完成了Ajax加载的网页数据的抓取啦，在分析ajax请求时，我们还可以使用抓包工具Fiddler来进行分析。<br>另外，有些网站把ajax请求的所有参数全部加密了。我们根本没办法构造自己所需要的数据的请求。这种情况下就用<strong>selenium+phantomJS</strong>，调用浏览器内核，并利用phantomJS执行js来模拟人为操作以及触发页面中的js脚本。从填写表单到点击按钮再到滚动页面，全部都可以模拟，不考虑具体的请求和响应过程，只是完完整整的把人浏览页面获取数据的过程模拟一遍。 这个我们会在下篇博文来讲解。 </p><p>文末附上写的另一个小demo，今日头条街拍图片：<br><a href="https://github.com/appleguardu/spider_projects/blob/master/Ajax_spider/toutiao_today.py" target="_blank" rel="noopener">https://github.com/appleguardu/spider_projects/blob/master/Ajax_spider/toutiao_today.py</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不知道你在写爬虫时有没有遇到这种情况：在浏览器中可以看到正常显示的页面数据，但是用requests得到的html文件里却没有这些数据。这是因为浏览器中的页面时经过JavaScript处理数据后生成的，Ajax加载就是其中一种方式，下面我们便来讲一下如何在异步加载的网页中抓取有用数据。&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>爬取猫眼电影信息</title>
    <link href="http://yoursite.com/2018/08/14/%E7%88%AC%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1%E4%BF%A1%E6%81%AF/"/>
    <id>http://yoursite.com/2018/08/14/爬取猫眼电影信息/</id>
    <published>2018-08-14T07:52:36.000Z</published>
    <updated>2018-08-14T15:12:47.785Z</updated>
    
    <content type="html"><![CDATA[<p>本篇内容主要讲解的是一个基础爬虫实例——爬去猫眼电影排行榜前100的影片信息，通过这个实例，来理解一个爬虫的基本思路。<br><a id="more"></a>  </p><h2 id="抓取目标"><a href="#抓取目标" class="headerlink" title="抓取目标"></a>抓取目标</h2><p>确定需要爬去的信息：猫眼电影top100的影片信息，这些信息包括电影名称，上映时间，演员，图片等。</p><h2 id="网页分析"><a href="#网页分析" class="headerlink" title="网页分析"></a>网页分析</h2><p>我们发现榜单top100对应的url是maoyan.com/board/4 。这便是我们要抓取的目标站点。<br><img src="https://i.imgur.com/uP4n5f7.png" alt=""></p><p>首先我们看到，霸王别姬排名第一，页面中的有效信息包括电影名，主演，上映时间，地区，评分和图片等。接着我们继续翻页，观察页面url和内容的变化；<br><img src="https://i.imgur.com/1mniQsL.png" alt=""></p><p>可以看到页面的url变成了<a href="http://maoyan.com/board/4?offset=10，多了一个参数offset=10，目前页面显示内容为排行11~20的电影；我们再点击下一页，发现此时url变成了http://maoyan.com/board/4?offset=20，显示结果为排行21~30的电影，所以：" target="_blank" rel="noopener">http://maoyan.com/board/4?offset=10，多了一个参数offset=10，目前页面显示内容为排行11~20的电影；我们再点击下一页，发现此时url变成了http://maoyan.com/board/4?offset=20，显示结果为排行21~30的电影，所以：</a><br>网页显示规律为：offset表示偏移量，如果偏移量为n，那么显示电影排行为n+1到n+10，每页显示10部电影。如果要获取top100，则需要请求10次，每次抓取一个页面，提取出里面有用的信息。  </p><h2 id="抓取第一页"><a href="#抓取第一页" class="headerlink" title="抓取第一页"></a>抓取第一页</h2><p>首先我们来抓取第一页的内容。在此之前我们先创建一个主函数main(),用来执行主要操作。<br>第一步，获取网页内容。</p><p><img src="https://i.imgur.com/2AQs44l.png" alt=""></p><h2 id="解析页面"><a href="#解析页面" class="headerlink" title="解析页面"></a>解析页面</h2><p>获取到网页源代码后，就需要对页面进行解析，然后提取出我们想要的信息。我们先回到网页查看一下真实源码，在<strong>开发者模式下的Network监听组件</strong>中查看源代码。注意：不要在Elements选项卡中查看源代码，因为那里的源码可能是经过JavaScript操作，与原始请求不同，所以需要从Network下查看。<br>排行第一的源代码片段：<br><img src="https://i.imgur.com/ydbNLEA.png" alt=""> </p><p>经过分析发现，每部电影的信息都包含在一个<dd></dd>的节点中。  </p><blockquote><pre><code>排名对应节点： &lt;i class=&quot;board-index board-index-1&quot;&gt;1&lt;/i&gt;图片对应节点：&lt;img data-src=&quot;http://p1.meituan.net/movie/20803f59291c47e1e116c11963ce019e68711.jpg@160w_220h_1e_1c&quot; alt=&quot;霸王别姬&quot; class=&quot;board-img&quot; /&gt;电影名称：&lt;p class=&quot;name&quot;&gt;&lt;a href=&quot;/films/1203&quot; title=&quot;霸王别姬&quot; data-act=&quot;boarditem-click&quot; data-val=&quot;{movieId:1203}&quot;&gt;霸王别姬&lt;/a&gt;&lt;/p  主演：&lt;p class=&quot;star&quot;&gt;主演：张国荣,张丰毅,巩俐&lt;/p&gt;上映时间：&lt;p class=&quot;releasetime&quot;&gt;上映时间：1993-01-01(中国香港)&lt;/p&gt;</code></pre></blockquote><p>下面我们就用正则表达式来提取上述信息，当然也可以使用Xpath和css选择器，我这里为了巩固一下正则用法，就先用re来写，虽然麻烦点。下面定义一个解析的方法。<br><img src="https://i.imgur.com/sVlFvdE.png" alt=""></p><p>这样我们就得到了第一个页面的信息，以列表list的形式返回的，每部电影信息保存在一个元组中。<br><img src="https://i.imgur.com/tAlDjx9.png" alt=""></p><p>由于数据的格式比较乱，我们需要对其进行格式的整理，这里选择遍历数据列表，把每部电影信息保存为字典dict形式。现在将方法修改一下。<br><img src="https://i.imgur.com/gZUzIXx.png" alt=""></p><p>整理后的信息如下：<br><img src="https://i.imgur.com/HYgoqD7.png" alt=""></p><h2 id="保存提取信息"><a href="#保存提取信息" class="headerlink" title="保存提取信息"></a>保存提取信息</h2><p>由于上面我们将每部电影信息都保存为了字典类型，这样更为结构化；现在我们将他们写入到文本文件中。这里通过JSON库的dumps()方法将字典进行序列化，由于输出结果有中文，需要将ensure_ascii指定为False，这样可以保证中文输出，而不是Unicode编码格式。<br><img src="https://i.imgur.com/bilBPae.png" alt=""></p><h2 id="main-方法调用"><a href="#main-方法调用" class="headerlink" title="main()方法调用"></a>main()方法调用</h2><p>最后我们定义一个main()方法用来调用前面的方法，将单页的电影信息全部写入到文件中。<br><img src="https://i.imgur.com/wUBtzNw.png" alt=""></p><p>单页文件输出结果：<br><img src="https://i.imgur.com/RzwULTw.png" alt=""></p><p>这样，我们就成功完成了第一页电影信息的爬取，但是我们目标是爬取所有电影，接下来就来试试把</p><h2 id="分页爬取"><a href="#分页爬取" class="headerlink" title="分页爬取"></a>分页爬取</h2><p>之前已经分析过了，每次翻页之后，url都会有变化，多了一个参数offset，所以我们只需要构造一下URL即可。下面将代码修改一下：<br><img src="https://i.imgur.com/M3gGvdY.png" alt=""></p><p>输出结果：<br><img src="https://i.imgur.com/9gFXrP7.png" alt=""></p><h2 id="代码最后的修改"><a href="#代码最后的修改" class="headerlink" title="代码最后的修改"></a>代码最后的修改</h2><p>现在我们已经基本上完成了所有电影信息的抓取，但是部分代码还是有缺陷，比如请求部分未作异常处理，爬取速度太快，可能会被反爬措施封锁等，所以还需要将代码优化一下。<br>代码地址：<a href="https://github.com/appleguardu/spider_projects" target="_blank" rel="noopener">https://github.com/appleguardu/spider_projects</a></p><p>好了，一个简单爬虫基本上完成！！！继续加油！！！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇内容主要讲解的是一个基础爬虫实例——爬去猫眼电影排行榜前100的影片信息，通过这个实例，来理解一个爬虫的基本思路。&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>python爬虫基本流程</title>
    <link href="http://yoursite.com/2018/08/12/python%E7%88%AC%E8%99%AB%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B/"/>
    <id>http://yoursite.com/2018/08/12/python爬虫基本流程/</id>
    <published>2018-08-12T06:34:17.000Z</published>
    <updated>2018-08-12T06:38:00.064Z</updated>
    
    <content type="html"><![CDATA[<h2 id="何为爬虫"><a href="#何为爬虫" class="headerlink" title="何为爬虫"></a>何为爬虫</h2><p>爬虫：请求网站并提取数据的自动化程序<br><a id="more"></a>  </p><h2 id="爬虫的基本流程"><a href="#爬虫的基本流程" class="headerlink" title="爬虫的基本流程"></a>爬虫的基本流程</h2><p>1.. 发起请求<br>通过HTTP库(主要用到urllib和requests这两个库)向目标站点发起请求，即发送一个Request，请求可以包含headers等信息，等待服务器响应。  </p><blockquote><pre><code>Request：浏览器就发送消息给该网址所在的服务器，这个过程叫做HTTP Request。  Request介绍：1.请求方式：主要有GET、POST两种类型，另外还有HEAD、PUT、DELETE、OPTIONS等。2.请求URL：URL全称统一资源定位符，如一个网页文档、一张图片、一个视频等都可以用URL唯一来确定。3.请求头：包含请求时的头部信息，如User-Agent、Host、Cookies等信息。4.请求体：请求时额外携带的数据如表单提交时的表单数据</code></pre></blockquote><p>2.. 获取响应内容<br>如果服务器正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，类型可能是HtML，Json字符串，二进制数据(图片，视频)等类型。  </p><blockquote><pre><code>Response:服务器收到浏览器发送的消息后，能够根据浏览器发送消息的内容，做相应处理，然后把消息回传给浏览器。这个过程叫做HTTP Response。浏览器收到服务器的Response信息后，会对信息进行相应处理，然后展示。 Response介绍：1.响应状态：有多种响应状态，如200代表成功、301跳转、404找不到页面、502服务器错误。2.响应头：如内容类型、内容长度、服务器信息、设置Cookie等等。3.响应体：最主要的部分，包含了请求资源的内容，如网页HTML、图片二进制数据等。 </code></pre></blockquote><p>3.. 解析获取的内容<br>获取的内容若是HTML，可以用正则表达式，网页解析库(BS,XPath等)进行解析。可能是Json，可以直接转为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理。  </p><p>4.. 保存数据<br>保存形式多样，可以存为文本，也可以保存至数据库，或者保存特定格式的文件。  </p><blockquote><pre><code>1.文本：纯文本、Json、Xml等。2.关系型数据库：如MySQL、Oracle等具有结构化表结构形式存储。3.非关系型数据库：如MongoDB、Redis等Key-Value形式存储。4.二进制文件：如图片、视频、音频等等直接保存成特定格式即可。  </code></pre></blockquote><p>另外，爬虫过程还会遇到很多问题，例如JavaScript渲染的问题，网站的反爬措施，爬取速度太慢等等，这些问题会在后期更新博文的时候再做详细的讲解。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;何为爬虫&quot;&gt;&lt;a href=&quot;#何为爬虫&quot; class=&quot;headerlink&quot; title=&quot;何为爬虫&quot;&gt;&lt;/a&gt;何为爬虫&lt;/h2&gt;&lt;p&gt;爬虫：请求网站并提取数据的自动化程序&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python之数据结构与算法</title>
    <link href="http://yoursite.com/2018/08/09/Python%E4%B9%8B%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2018/08/09/Python之数据结构与算法/</id>
    <published>2018-08-09T12:24:47.000Z</published>
    <updated>2018-08-10T10:50:35.015Z</updated>
    
    <content type="html"><![CDATA[<p>要作为一名程序员，数据结构和算法是必备基本功；这篇文章主要是介绍Python常用的数据结构和算法，记录自己在学习中的一些心得体会。<br><a id="more"></a>  </p><h2 id="算法的概念"><a href="#算法的概念" class="headerlink" title="算法的概念"></a>算法的概念</h2><p>算法是计算机处理信息的本质，因为计算机程序本质上是一个算法来告诉计算机确切的步骤来执行一个指定的任务。一般地，当算法在处理信息时，会从输入设备或数据的存储地址读取数据，把结果写入输出设备或某个存储地址供以后再调用。  <strong>算法是独立存在的一种解决问题的方法和思想。</strong><br>算法的五大特性:  </p><blockquote><pre><code>1.输入: 算法具有0个或多个输入2.输出: 算法至少有1个或多个输出3.有穷性: 算法在有限的步骤之后会自动结束而不会无限循环，并且每一个步骤可以在可接受的时间内完成4.确定性：算法中的每一步都有确定的含义，不会出现二义性5.可行性：算法的每一步都是可行的，也就是说每一步都能够执行有限的次数完成</code></pre></blockquote><h2 id="算法效率衡量"><a href="#算法效率衡量" class="headerlink" title="算法效率衡量"></a>算法效率衡量</h2><p><strong>时间复杂度与“大O记法”</strong><br>定义：  </p><p><strong>大O记法：</strong>对于单调的整数函数f，如果存在一个整数函数g和实常数c&gt;0，使得对于充分大的n总有f(n)&lt;=c*g(n)，就说函数g是f的一个渐近函数（忽略常数），记为f(n)=O(g(n))。也就是说，在趋向无穷的极限意义下，函数f的增长速度受到函数g的约束，亦即函数f与函数g的特征相似。</p><p><strong>时间复杂度：</strong>假设存在函数g，使得算法A处理规模为n的问题示例所用时间为T(n)=O(g(n))，则称O(g(n))为算法A的渐近时间复杂度，简称时间复杂度，记为T(n)。  </p><p>估计看了概念还是一脸不知所措，哈哈，正常，我刚刚看的时候也是。<br>通俗的理解就是：对于算法的时间性质和空间性质，最重要的是其数量级和趋势，这些是分析算法效率的主要部分。而计量算法基本操作数量的规模函数中那些常量因子可以忽略不计。例如，可以认为3n2和100n2属于同一个量级，如果两个算法处理同样规模实例的代价分别为这两个函数，就认为它们的效率“差不多”，都为n2级。<br><strong>时间复杂度的计算规则：</strong>  </p><blockquote><pre><code>1.基本操作，即只有常数项，认为其时间复杂度为O(1)2.顺序结构，时间复杂度按加法进行计算3.循环结构，时间复杂度按乘法进行计算4.分支结构，时间复杂度取最大值5.判断一个算法的效率时，往往只需要关注操作数量的最高次项，其它次要项和常数项可以忽略</code></pre></blockquote><p>示例：<br><img src="https://i.imgur.com/3YRVUoa.png" alt=""><br><strong>常见时间复杂度</strong><br><img src="https://i.imgur.com/Sqq0QtA.png" alt=""></p><h2 id="数据结构的概念"><a href="#数据结构的概念" class="headerlink" title="数据结构的概念"></a>数据结构的概念</h2><p>在学习数据结构概念之前我们先想一个问题，假设如果现在有一个需求，要用Python中的数据类型来保存一个班的学生信息，而且要快速的通过学生姓名获取其信息。<br>实际上当我们在思考这个问题的时候，我们已经用到了数据结构。列表和字典都可以存储一个班的学生信息，但是想要在列表中获取一名同学的信息时，就要遍历这个列表，其时间复杂度为O(n)，而使用字典存储时，可将学生姓名作为字典的键，学生信息作为值，进而查询时不需要遍历便可快速获取到学生信息，其时间复杂度为O(1)。<br>那么我们为了解决问题，需要将数据保存下来，然后根据数据的存储方式来设计算法实现进行处理，那么数据的存储方式不同就会导致需要不同的算法进行处理。我们希望算法解决问题的效率越快越好，于是我们就需要考虑数据究竟如何保存的问题，这就是数据结构。<br><strong>概念</strong><br>数据是一个抽象的概念，将其进行分类后得到程序设计语言中的基本类型。如：int，float，char等。数据元素之间不是独立的，存在特定的关系，这些关系便是结构。数据结构指数据对象中数据元素之间的关系。</p><p>Python给我们提供了很多现成的数据结构类型，这些系统自己定义好的，不需要我们自己去定义的数据结构叫做Python的内置数据结构，比如列表、元组、字典。而有些数据组织方式，Python系统里面没有直接定义，需要我们自己去定义实现这些数据的组织方式，这些数据组织方式称之为Python的扩展数据结构，比如栈，队列等。</p><p><strong>小结：</strong><br><strong>算法是为了解决实际问题而设计的，数据结构是算法需要处理的问题载体。</strong><br><strong>数据结构只是静态的描述了数据元素之间的关系。</strong><br>这里讲一下常用的数据运算：插入，删除，修改，查找，排序，这也是后面内容要经常提到的。</p><h2 id="顺序表"><a href="#顺序表" class="headerlink" title="顺序表"></a>顺序表</h2><p>在程序中，经常需要将一组（通常是同为某个类型的）数据元素作为整体管理和使用，需要创建这种元素组，用变量记录它们，传进传出函数等。一组数据中包含的元素个数可能发生变化（可以增加或删除元素）。</p><p>对于这种需求，最简单的解决方案便是将这样一组元素看成一个序列，用元素在序列里的位置和顺序，表示实际应用中的某种有意义的信息，或者表示数据之间的某种关系。</p><p>这样的一组序列元素的组织形式，我们可以将其抽象为线性表。一个线性表是某类元素的一个集合，还记录着元素之间的一种顺序关系。线性表是最基本的数据结构之一，在实际程序中应用非常广泛，它还经常被用作更复杂的数据结构的实现基础。<br>根据线性表的实际存储方式，分为两种实现模型：</p><blockquote><pre><code>顺序表，将元素顺序地存放在一块连续的存储区里，元素间的顺序关系由它们的存储顺序自然表示。链表，将元素存放在通过链接构造起来的一系列存储块中。</code></pre></blockquote><p><strong>顺序表的基本形式</strong><br><img src="https://i.imgur.com/Nc4TasW.png" alt=""></p><p>图a表示的是顺序表的基本形式，数据元素本身连续存储，每个元素所占的存储单元大小固定相同，元素的下标是其逻辑地址，而元素存储的物理地址（实际内存地址）可以通过存储区的起始地址Lo(e0)加上逻辑地址（第i个元素）与存储单元大小（c）的乘积计算而得，即：</p><p>Lo(ei) = Lo(e0) + c*i</p><p>所以，访问指定元素时无需从头遍历，通过计算便可获得对应地址，其时间复杂度为O(1)。  </p><p>另外，如果<strong>元素的大小不统一</strong>，则须采用图b的元素外置的形式，将实际数据元素另行存储，而顺序表中各单元位置保存对应元素的地址信息（即链接）。由于每个链接所需的存储量相同，通过上述公式，可以计算出元素链接的存储位置，而后顺着链接找到实际存储的数据元素。注意，图b中的c不再是数据元素的大小，而是存储一个链接地址所需的存储量，这个量通常很小。</p><p><strong>顺序表的结构与实现</strong><br><img src="https://i.imgur.com/GQLMtHf.png" alt="">  </p><p>一个顺序表的完整信息包括两部分，一部分是表中的元素集合，另一部分是为实现正确操作而需记录的信息，即有关表的整体情况的信息，这部分信息主要包括元素存储区的容量和当前表中已有的元素个数两项。</p><p><strong>顺序表的两种基本实现方式</strong><br><img src="https://i.imgur.com/VrJl6Tn.png" alt="">  </p><p>图a为一体式结构，存储表信息的单元与元素存储区以连续的方式安排在一块存储区里，两部分数据的整体形成一个完整的顺序表对象。<br>一体式结构整体性强，易于管理。但是由于数据元素存储区域是表对象的一部分，顺序表创建后，元素存储区就固定了。</p><p>图b为分离式结构，表对象里只保存与整个表有关的信息（即容量和元素个数），实际数据元素存放在另一个独立的元素存储区里，通过链接与基本表对象关联。</p><p><strong>元素存储区替换</strong><br>一体式结构由于顺序表信息区与数据区连续存储在一起，所以若想更换数据区，则只能整体搬迁，即整个顺序表对象（指存储顺序表的结构信息的区域）改变了。</p><p>分离式结构若想更换数据区，只需将表信息区中的数据区链接地址更新即可，而该顺序表对象不变。<br><strong>元素存储区扩充</strong><br>采用分离式结构的顺序表，若将数据区更换为存储空间更大的区域，则可以在不改变表对象的前提下对其数据存储区进行了扩充，所有使用这个表的地方都不必修改。只要程序的运行环境（计算机系统）还有空闲存储，这种表结构就不会因为满了而导致操作无法进行。我们把采用这种技术实现的顺序表称为动态顺序表，因为其容量可以在使用中动态变化。  </p><p>扩充的两种策略:<br>1.每次扩充增加固定数目的存储位置，如每次扩充增加10个元素位置，这种策略可称为线性增长。<br>特点：节省空间，但是扩充操作频繁，操作次数多。</p><p>2.每次扩充容量加倍，如每次扩充增加一倍存储空间。<br>特点：减少了扩充操作的执行次数，但可能会浪费空间资源。以空间换时间，推荐的方式。</p><p><strong>顺序表的操作</strong><br><strong>1.增加元素</strong><br>如图所示，为顺序表增加新元素111的三种方式<br><img src="https://i.imgur.com/58Yjdx7.png" alt=""></p><blockquote><pre><code>a. 尾端加入元素，时间复杂度为O(1)b. 非保序的加入元素（不常见），时间复杂度为O(1)c. 保序的元素加入，时间复杂度为O(n)</code></pre></blockquote><p><strong>2.删除元素</strong><br><img src="https://i.imgur.com/ccmB4na.png" alt=""></p><blockquote><pre><code>a. 删除表尾元素，时间复杂度为O(1)b. 非保序的元素删除（不常见），时间复杂度为O(1)c. 保序的元素删除，时间复杂度为O(n)</code></pre></blockquote><h2 id="Python中的顺序表"><a href="#Python中的顺序表" class="headerlink" title="Python中的顺序表"></a>Python中的顺序表</h2><p>Python中的list和tuple两种类型采用了顺序表的实现技术，具有前面讨论的顺序表的所有性质。tuple是不可变类型，即不变的顺序表，因此不支持改变其内部状态的任何操作，而其他方面，则与list的性质类似。</p><p><strong>list的基本实现技术</strong><br>Python标准类型list就是一种元素个数可变的线性表，可以加入和删除元素，并在各种操作中维持已有元素的顺序（即保序），而且还具有以下行为特征：  </p><blockquote><pre><code>1.基于下标（位置）的高效元素访问和更新，时间复杂度应该是O(1)；为满足该特征，应该采用顺序表技术，表中元素保存在一块连续的存储区中。</code></pre></blockquote><blockquote><pre><code>2.允许任意加入元素，而且在不断加入元素的过程中，表对象的标识（函数id得到的值）不变。为满足该特征，就必须能更换元素存储区，并且为保证更换存储区时list对象的标识id不变，只能采用分离式实现技术。</code></pre></blockquote><p><img src="https://i.imgur.com/cV3MVv2.png" alt=""></p><p><img src="https://i.imgur.com/jJOvR99.png" alt=""><br>在Python的官方实现中，<strong>list就是一种采用分离式技术实现的动态顺序表。</strong>这就是为什么用list.append(x) （或 list.insert(len(list), x)，即尾部插入）比在指定位置插入元素效率高的原因。</p><p>在Python的官方实现中，list实现采用了如下的策略：在建立空表（或者很小的表）时，系统分配一块能容纳8个元素的存储区；在执行插入操作（insert或append）时，如果元素存储区满就换一块4倍大的存储区。但如果此时的表已经很大（目前的阀值为50000），则改变策略，采用加一倍的方法。引入这种改变策略的方式，是为了避免出现过多空闲的存储位置。</p><h2 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h2><p>由于顺序表的构建需要预先知道数据大小来申请连续的存储空间，而在进行扩充时又需要进行数据的搬迁，所以使用起来并不是很灵活。<br>链表结构可以充分利用计算机内存空间，实现灵活的内存动态管理。  </p><p><strong>链表的定义</strong><br>链表（Linked list）是一种常见的基础数据结构，是一种线性表，但是不像顺序表一样连续存储数据，而是在每一个节点（数据存储单元）里存放下一个节点的位置信息（即地址）。<br><img src="https://i.imgur.com/n5v7yDM.png" alt=""></p><p><strong>单向链表</strong><br>单向链表也叫单链表，是链表中最简单的一种形式，它的每个节点包含两个域，一个信息域（元素域）和一个链接域。这个链接指向链表中的下一个节点，而最后一个节点的链接域则指向一个空值。<br><img src="https://i.imgur.com/SzIvatC.png" alt="">  </p><blockquote><pre><code>1.表元素域elem用来存放具体的数据。2.链接域next用来存放下一个节点的位置（python中的标识）3.变量p指向链表的头节点（首节点）的位置，从p出发能找到表中的任意节点。</code></pre></blockquote><h2 id="链表的实现"><a href="#链表的实现" class="headerlink" title="链表的实现"></a>链表的实现</h2><p>像顺序表中list有很多方法，比如增删改查，排序，遍历等；那么在链表中也能实现这些功能，这里需要用python程序来实现。<br><strong>单链表的相关操作</strong>  </p><blockquote><pre><code>1. is_empty()        链表是否为空2. length()          链表长度3. travel()          遍历整个链表4. add(item)         链表头部添加元素5. append(item)      链表尾部添加元素6. insert(pos, item) 指定位置添加元素7. remove(item)      删除节点8. search(item)      查找节点是否存在  </code></pre></blockquote><p><strong>单链表的实现:</strong><br>链表是由很多节点组成，我们可以先把节点用一个Node类封装起来。  链表中节点之间通过将自身的next区指向下一节点的elem区来实现节点之间的链接。 例如下图中Node1与Node2之间的关系。<br><img src="https://i.imgur.com/9Gsl0JA.png" alt=""></p><p>链表实现的实例：<br><img src="https://i.imgur.com/2VVqSQs.png" alt=""><br><img src="https://i.imgur.com/etxxj3g.png" alt=""><br><img src="https://i.imgur.com/8kW2WnG.png" alt=""><br><img src="https://i.imgur.com/jtGFv51.png" alt=""><br><img src="https://i.imgur.com/YR2BUsv.png" alt=""></p><h2 id="链表与顺序表的对比"><a href="#链表与顺序表的对比" class="headerlink" title="链表与顺序表的对比"></a>链表与顺序表的对比</h2><p>链表失去了顺序表随机读取的优点，同时链表由于增加了结点的指针域，空间开销比较大，但对存储空间的使用要相对灵活。<br>链表与顺序表的各种操作复杂度如下所示：<br><img src="https://i.imgur.com/GQYdeju.png" alt="">  </p><p>链表和顺序表在插入和删除时进行的是完全不同的操作。链表的主要耗时操作是遍历查找，删除和插入操作本身的复杂度是O(1)。顺序表查找很快，主要耗时的操作是拷贝覆盖。因为除了目标元素在尾部的特殊情况，顺序表进行插入和删除时需要对操作点之后的所有元素进行前后移位操作，只能通过拷贝和覆盖的方法进行。</p><p>另外，出了单链表之外，还有单向循环链表和双向链表，这里只做一下介绍。实现的原理可以根据上面内容进行类比。  </p><p><strong>单向循环链表</strong><br>单链表的一个变形是单向循环链表，即链表中最后一个节点的next域不再为None，而是指向链表的头节点。<br><img src="https://i.imgur.com/ubeX9gK.png" alt=""></p><p>定义类时，节点的属性还是只有elem和next，链表需要指定头节点head。</p><p><strong>双向链表</strong><br>双向链表：每个节点有两个链接：一个指向前一个节点，当此节点为第一个节点时，指向空值；而另一个指向下一个节点，当此节点为最后一个节点时，指向空值。</p><p><img src="https://i.imgur.com/hJ2xsbR.png" alt=""></p><p>与单链表相比，他的节点多了一个指向前节点next的prev区属性。</p><h2 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h2><p>栈作为一种数据结构，既可以用顺序表实现，也可以用链表实现。栈的特性就像弹夹一样，子弹先进后出，栈里面的元素也是先进后出。<br><strong>栈的相关操作</strong>  </p><blockquote><pre><code>1. Stack() 创建一个新的空栈2. push(item) 添加一个新的元素item到栈顶3. pop() 弹出栈顶元素4. peek() 返回栈顶元素5. is_empty() 判断栈是否为空6. size() 返回栈的元素个数</code></pre></blockquote><p><strong>顺序表实现栈</strong><br><img src="https://i.imgur.com/iqY2vxB.png" alt=""></p><h2 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h2><p>队列（queue）是只允许在一端进行插入操作，而在另一端进行删除操作的线性表。</p><p>队列是一种先进先出的（First In First Out）的线性表，简称FIFO。允许插入的一端为队尾，允许删除的一端为队头。队列不允许在中间部位进行操作！假设队列是q=（a1，a2，……，an），那么a1就是队头元素，而an是队尾元素。这样我们就可以删除时，总是从a1开始，而插入时，总是在队列最后。这也比较符合我们通常生活中的习惯，排在第一个的优先出列，最后来的当然排在队伍最后。</p><p><strong>队列的实现</strong>   </p><blockquote><pre><code>队列相关操作: 1. Queue() 创建一个空的队列2. enqueue(item) 往队列中添加一个item元素3. dequeue() 从队列头部删除一个元素4. is_empty() 判断一个队列是否为空5. size() 返回队列的大小</code></pre></blockquote><p><img src="https://i.imgur.com/ifKJNTN.png" alt=""></p><p><strong>双端队列</strong><br>双端队列（deque，全名double-ended queue），是一种具有队列和栈的性质的数据结构。<br>双端队列中的元素可以从两端弹出，其限定插入和删除操作在表的两端进行。双端队列可以在队列任意一端入队和出队。<br><img src="https://i.imgur.com/IYWCdDG.png" alt=""></p><p><strong>相关操作</strong>  </p><blockquote><pre><code>1. Deque() 创建一个空的双端队列2. add_front(item) 从队头加入一个item元素3. add_rear(item) 从队尾加入一个item元素4. remove_front() 从队头删除一个item元素5. remove_rear() 从队尾删除一个item元素6. is_empty() 判断双端队列是否为空7. size() 返回队列的大小</code></pre></blockquote><p><img src="https://i.imgur.com/oAeUqKg.png" alt=""></p><h2 id="排序算法"><a href="#排序算法" class="headerlink" title="排序算法"></a>排序算法</h2><p>排序算法（Sorting algorithm）是一种能将一串数据依照特定顺序进行排列的一种算法。  </p><p><strong>1.冒泡排序</strong><br>冒泡排序（Bubble Sort）是一种简单的排序算法。它重复地遍历要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。遍历数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。</p><p>冒泡排序算法的运作如下：</p><blockquote><pre><code>比较相邻的元素。如果第一个比第二个大（升序），就交换他们两个。对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。针对所有的元素重复以上的步骤，除了最后一个。持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。</code></pre></blockquote><p>交换过程图示(第一次)：<br><img src="https://i.imgur.com/2Lb0Vnx.png" alt=""></p><p>那么我们需要进行n-1次冒泡过程，每次对应的比较次数如下图所示：<br><img src="https://i.imgur.com/ZYR7kGw.png" alt=""></p><p>代码示例：<br><img src="https://i.imgur.com/q2Poq9L.png" alt=""></p><p>时间复杂度：  </p><blockquote><pre><code>最优时间复杂度：O(n) （表示遍历一次发现没有任何可以交换的元素，排序结束。）  最坏时间复杂度：O(n2)。  稳定性：稳定  </code></pre></blockquote><p>冒泡排序的演示：  </p><p><img src="https://i.imgur.com/C9iBDuU.gif" alt=""></p><p><strong>2.选择排序</strong><br>选择排序（Selection sort）是一种简单直观的排序算法。它的工作原理如下。首先在未排序序列中找到最小元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。<br>选择排序中前面部分为有序固定的，后面时无序的，每次从后面无序部分找到最小值往前送。<br>原理动图：<br><img src="https://i.imgur.com/ExAbEid.gif" alt=""></p><p>代码示例：<br><img src="https://i.imgur.com/HN0QK6D.png" alt=""></p><p>时间复杂度</p><blockquote><pre><code>最优时间复杂度：O(n2)最坏时间复杂度：O(n2)稳定性：不稳定（考虑升序每次选择最大的情况）</code></pre></blockquote><p><strong>3.插入排序</strong><br>插入排序（Insertion Sort）是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。</p><p>原理示意：<br><img src="https://i.imgur.com/Vxu7QQq.gif" alt=""></p><p>代码示例：<br><img src="https://i.imgur.com/y5DKsYP.png" alt=""><br>这里可以将代码做一下优化，把内层的while循环换成for循环，去掉<code>i -= 1</code>和break。因为如果碰上最理想的情况，也就是目标序列时有序的，while循环还是需要判断，计算i -= 1，这里可以用<code>for i in range(j, 0, -1):</code></p><p>效果图：<br><img src="https://i.imgur.com/jcumXAQ.gif" alt=""></p><p><strong>4.快速排序</strong><br>快速排序（Quick sort），又称划分交换排序（partition-exchange sort），通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。</p><p>步骤为：</p><blockquote><pre><code>1.从数列中挑出一个元素，称为&quot;基准&quot;（pivot），2.重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。3.递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。</code></pre></blockquote><p><strong>代码示例</strong><br><img src="https://i.imgur.com/f9Fcr7Q.png" alt=""></p><p>时间复杂度</p><blockquote><pre><code>最优时间复杂度：O(nlogn)最坏时间复杂度：O(n2)稳定性：不稳定</code></pre></blockquote><p>另外，除了以上几种排序方法外，还有希尔排序和归并排序，这里就不一一说了。  </p><h2 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h2><p>搜索是在一个项目集合中找到一个特定项目的算法过程。搜索通常的答案是真的或假的，因为该项目是否存在。 搜索的几种常见方法：顺序查找、二分法查找、二叉树查找、哈希查找 。  </p><p><strong>二分法查找</strong><br>二分查找又称折半查找，优点是比较次数少，查找速度快，平均性能好；其缺点是要求待查表为有序表，且插入删除困难。因此，折半查找方法适用于不经常变动而查找频繁的有序列表。  </p><p>代码示例：<br><img src="https://i.imgur.com/Ka39vOj.png" alt=""></p><h2 id="树的概念"><a href="#树的概念" class="headerlink" title="树的概念"></a>树的概念</h2><p>树（tree）是一种抽象数据类型（ADT）或是实作这种抽象数据类型的数据结构，用来模拟具有树状结构性质的数据集合。它是由n（n&gt;=1）个有限节点组成一个具有层次关系的集合。把它叫做“树”是因为它看起来像一棵倒挂的树，也就是说它是根朝上，而叶朝下的。它具有以下的特点：</p><blockquote><pre><code>1.每个节点有零个或多个子节点；2.没有父节点的节点称为根节点；3.每一个非根节点有且只有一个父节点；4.除了根节点外，每个子节点可以分为多个不相交的子树；</code></pre></blockquote><p>树的结构示意图：</p><p><img src="https://i.imgur.com/m7YiNn8.png" alt=""></p><p><strong>二叉树的基本概念</strong><br>二叉树是每个节点最多有两个子树的树结构。通常子树被称作“左子树”（left subtree）和“右子树”（right subtree）  </p><p><strong>二叉树的性质(特性)</strong>  </p><blockquote><pre><code>性质1: 在二叉树的第i层上至多有2^(i-1)个结点（i&gt;0）性质2: 深度为k的二叉树至多有2^k - 1个结点（k&gt;0）性质3: 对于任意一棵二叉树，如果其叶结点数为N0，而度数为2的结点总数为N2，则N0=N2+1;性质4:具有n个结点的完全二叉树的深度必为 log2(n+1)性质5:对完全二叉树，若从上至下、从左至右编号，则编号为i 的结点，其左孩子编号必为2i，其右孩子编号必为2i＋1；其双亲的编号必为i/2（i＝1 时为根,除外）</code></pre></blockquote><p><strong>二叉树的遍历</strong><br>树的遍历是树的一种重要的运算。所谓遍历是指对树中所有结点的信息的访问，即依次对树中每个结点访问一次且仅访问一次，我们把这种对所有节点的访问称为遍历（traversal）。那么树的两种重要的遍历模式是深度优先遍历和广度优先遍历,深度优先一般用递归，广度优先一般用队列。一般情况下能用递归实现的算法大部分也能用堆栈来实现。</p><p><strong>深度优先遍历</strong>  </p><p>对于一颗二叉树，深度优先搜索(Depth First Search)是沿着树的深度遍历树的节点，尽可能深的搜索树的分支。<br>那么深度遍历有重要的三种方法。这三种方式常被用于访问树的节点，它们之间的不同在于访问每个节点的次序不同。这三种遍历分别叫做先序遍历（preorder），中序遍历（inorder）和后序遍历（postorder）。</p><blockquote><pre><code>1.先序遍历  根节点-&gt;左子树-&gt;右子树2.中序遍历  左子树-&gt;根节点-&gt;右子树3.左子树-&gt;右子树-&gt;根节点</code></pre><p><img src="https://i.imgur.com/WviXeBC.png" alt=""></p></blockquote><p>中序和后序只是把elem的顺序往下排。</p><p><strong>广度优先遍历(层次遍历)</strong>  </p><p>从树的root开始，从上到下从从左到右遍历整个树的节点<br><img src="https://i.imgur.com/IxhlZ91.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;要作为一名程序员，数据结构和算法是必备基本功；这篇文章主要是介绍Python常用的数据结构和算法，记录自己在学习中的一些心得体会。&lt;br&gt;
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python之json基础</title>
    <link href="http://yoursite.com/2018/08/08/python%E4%B9%8Bjson%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/2018/08/08/python之json数据解析/</id>
    <published>2018-08-08T05:27:17.000Z</published>
    <updated>2018-08-08T08:01:22.866Z</updated>
    
    <content type="html"><![CDATA[<p>当我们爬取异步加载的网页从上面获取的往往是json类型的文件，通常我们需要对这类数据进行处理后才能使用，下面就来说说python中对json数据的基本处理方法。<br><a id="more"></a>  </p><h2 id="什么是JSON"><a href="#什么是JSON" class="headerlink" title="什么是JSON"></a>什么是JSON</h2><p>JSON 指的是 JavaScript 对象表示法（JavaScript Object Notation）；是一种轻量级的文本数据交换格式；适用于进行数据交互的场景，比如网站前台与后台之间的数据交互。  </p><p>简单来说json就是javascript中的对象和数组，所以这两种结构就是对象和数组两种结构，通过这两种结构可以表示各种复杂的结构：  </p><blockquote><pre><code>1.对象：对象在js中表示为{ }括起来的内容，数据结构为 { key：value, key：value, ... }的键值对的结构，在面向对象的语言中，key为对象的属性，value为对应的属性值，所以很容易理解，取值方法为 对象.key 获取属性值，这个属性值的类型可以是数字、字符串、数组、对象这几种。</code></pre></blockquote><blockquote><pre><code>2.数组：数组在js中是中括号[ ]括起来的内容，数据结构为 [&quot;Python&quot;, &quot;javascript&quot;, &quot;C++&quot;, ...]，取值方式和所有语言中一样，使用索引获取，字段值的类型可以是 数字、字符串、数组、对象几种。</code></pre></blockquote><h2 id="JSON模块"><a href="#JSON模块" class="headerlink" title="JSON模块"></a>JSON模块</h2><p>Python中提供了json模块，总共有四个方法：dumps、dump、loads、load，用于字符串 和 python数据类型间进行转换。<br><strong>1.json.loads()</strong><br>对数据进行解码，把Json格式字符串 解码 转换成Python对象 。<br><img src="https://i.imgur.com/gFu4n1P.png" alt=""></p><p>实例：<br><img src="https://i.imgur.com/3zOg7fE.png" alt=""></p><p><strong>2.json.dumps()</strong><br>对数据进行编码，返回一个str对象，把一个Python对象编码转换成Json字符串。如果直接将dict类型的数据写入json文件中会发生报错，因此在将数据写入时需要用到该函数。<br><img src="https://i.imgur.com/foRbCB5.png" alt=""></p><p>实例：<br><img src="https://i.imgur.com/N5ids9K.png" alt=""></p><p><strong>3.json.dump()</strong><br>json.dump()用于将dict类型的数据转成str，并写入到json文件中  </p><p><img src="https://i.imgur.com/b06PiXX.png" alt=""></p><p><strong>4.json.load()</strong><br>json.load()直接从json文件中读取数据，并将str转为dict类型</p><p>实例：<br><img src="https://i.imgur.com/bBO2Wo7.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当我们爬取异步加载的网页从上面获取的往往是json类型的文件，通常我们需要对这类数据进行处理后才能使用，下面就来说说python中对json数据的基本处理方法。&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="json" scheme="http://yoursite.com/tags/json/"/>
    
  </entry>
  
  <entry>
    <title>leetcode02-两数相加</title>
    <link href="http://yoursite.com/2018/08/07/leetcode02-%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/"/>
    <id>http://yoursite.com/2018/08/07/leetcode02-两数相加/</id>
    <published>2018-08-07T13:11:58.000Z</published>
    <updated>2018-08-07T15:51:30.663Z</updated>
    
    <content type="html"><![CDATA[<p>题目二：<br>给定两个非空链表来表示两个非负整数。位数按照逆序方式存储，它们的每个节点只存储单个数字。将两数相加返回一个新的链表。你可以假设除了数字 0 之外，这两个数字都不会以零开头。<br><a id="more"></a><br>示例：</p><blockquote><pre><code>输入：(2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)输出：7 -&gt; 0 -&gt; 8原因：342 + 465 = 807</code></pre></blockquote><p>分析：<br>在这之前先把链表的知识复习一下，以单链表为例，它是链表中最简单的一种形式，它的每个节点包含两个域，一个信息域（元素域）和一个链接域。这个链接指向链表中的下一个节点，而最后一个节点的链接域则指向一个空值。<br><img src="https://i.imgur.com/SXv0O3O.png" alt=""></p><blockquote><pre><code>表元素域elem用来存放具体的数据。链接域next用来存放下一个节点的位置（python中的标识）变量p指向链表的头节点（首节点）的位置head，从p出发能找到表中的任意节点</code></pre></blockquote><p>链表的操作：</p><blockquote><pre><code>is_empty() 链表是否为空length() 链表长度travel() 遍历整个链表add(item) 链表头部添加元素append(item) 链表尾部添加元素insert(pos, item) 指定位置添加元素remove(item) 删除节点search(item) 查找节点是否存在</code></pre></blockquote><p>使用变量来跟踪进位，并从包含最低有效位的表头开始模拟逐位相加的过程。<br><img src="https://i.imgur.com/b7iQwXf.png" alt=""></p><p>我们首先从最低有效位也就是列表l1和l2的表头开始相加。由于每位数字都应当处于0到9的范围内，我们计算两个数字的和时可能会出现“溢出”。例如，5 + 7 = 12。在这种情况下，我们会将当前位的数值设置为 2，并将进位 carry = 1带入下一次迭代。进位 carry 必定是 0 或 1，这是因为两个数字相加（考虑到进位）可能出现的最大和为 9 + 9 + 1 = 19。<br>我们考虑到了存在的进位情况，但是还可能出现链表长度不等，即两个加数位数不等。我们将题目拆分为两个情况，以三位数加五位数为例，前三位数和前三位数我们将进位carry初始化为0，遍历列表 l1 和 l2 直至到达它们的尾端，新列表l3就等于l1+l2+carry；carry满足大于10时进1。第四位第五位则是另一种情况；具体就是：前三位数范围内，对应位进行相加操作时附加上低位的进位；第四位第五位只用将该位加上低位的进位即可（或者理解成另一个三位数的高位为0）<br>执行代码如下：<br><img src="https://i.imgur.com/PnJsCoU.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;题目二：&lt;br&gt;给定两个非空链表来表示两个非负整数。位数按照逆序方式存储，它们的每个节点只存储单个数字。将两数相加返回一个新的链表。你可以假设除了数字 0 之外，这两个数字都不会以零开头。&lt;br&gt;
    
    </summary>
    
      <category term="leetcode" scheme="http://yoursite.com/categories/leetcode/"/>
    
    
      <category term="leetcode" scheme="http://yoursite.com/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>JavaScript基础学习(二)</title>
    <link href="http://yoursite.com/2018/08/07/JavaScript%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0-%E4%BA%8C/"/>
    <id>http://yoursite.com/2018/08/07/JavaScript基础学习-二/</id>
    <published>2018-08-07T08:08:15.000Z</published>
    <updated>2018-08-08T07:59:30.478Z</updated>
    
    <content type="html"><![CDATA[<h2 id="定时器"><a href="#定时器" class="headerlink" title="定时器"></a>定时器</h2><p>定时器在javascript中的作用:1、制作动画;2、异步操作;3、函数缓冲与节流。<br><a id="more"></a>  </p><p><strong>1.1定时器类型及语法</strong>  </p><blockquote><pre><code>定时器：setTimeout  只执行一次的定时器 clearTimeout 关闭只执行一次的定时器setInterval  反复执行的定时器clearInterval 关闭反复执行的定时器</code></pre></blockquote><p>实例1：定时器制作时钟<br><img src="https://i.imgur.com/SWmiiF5.png" alt=""><br><img src="https://i.imgur.com/LusZz6q.png" alt=""></p><p>实例二：定时器制作倒计时<br><img src="https://i.imgur.com/WhsbnOW.png" alt="">  </p><h2 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h2><p><strong>2.1直接转换 parseInt() 与 parseFloat()</strong></p><blockquote><pre><code>alert(&apos;12&apos;+7); //弹出127alert( parseInt(&apos;12&apos;) + 7 );  //弹出19 alert( parseInt(5.6));  // 弹出5alert(&apos;5.6&apos;+2.3);  // 弹出5.62.3alert(parseFloat(&apos;5.6&apos;)+2.3);  // 弹出7.8999999999999995alert(0.1+0.2); //弹出 0.3000000000000004,改进方法如下alert((0.1*100+0.2*100)/100); //弹出0.3alert((parseFloat(&apos;5.6&apos;)*100+2.3*100)/100); //弹出7.9</code></pre></blockquote><p><strong>2.22、隐式转换 “==” 和 “-”</strong>  </p><blockquote><pre><code>if(&apos;3&apos;==3){alert(&apos;相等&apos;);} //弹出&apos;相等&apos;；‘==’会先转换成相同类型再比较</code></pre></blockquote><blockquote><pre><code>alert(&apos;10&apos;-3);  // 弹出7</code></pre></blockquote><p><strong>3.NaN 和 isNaN</strong><br>NaN: Not a Number<br>isNaN: 判断是否是非数字，是，则返回True。 </p><blockquote><pre><code>alert( parseInt(&apos;123abc&apos;) );  // 弹出123alert( parseInt(&apos;abc123&apos;) );  // 弹出NaN</code></pre></blockquote><h2 id="变量作用域"><a href="#变量作用域" class="headerlink" title="变量作用域"></a>变量作用域</h2><p>变量作用域指的是变量的作用范围，javascript中的变量分为全局变量和局部变量。  </p><blockquote><pre><code>全局变量：在函数之外定义的变量，为整个页面公用，函数内部外部都可以访问。  局部变量：在函数内部定义的变量，只能在定义该变量的函数内部访问，外部无法访问。  </code></pre></blockquote><p><img src="https://i.imgur.com/khRSBDv.png" alt=""></p><h2 id="封闭函数"><a href="#封闭函数" class="headerlink" title="封闭函数"></a>封闭函数</h2><p>封闭函数是javascript中匿名函数的另外一种写法，创建一个一开始就执行而不用命名的函数。 </p><p><img src="https://i.imgur.com/jHRoYFq.png" alt=""></p><h2 id="变量方式定义函数"><a href="#变量方式定义函数" class="headerlink" title="变量方式定义函数"></a>变量方式定义函数</h2><p><img src="https://i.imgur.com/UrF0DwV.png" alt=""></p><h2 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h2><p>函数嵌套函数，内部函数可以引用外部函数的参数和变量，参数和变量不会被垃圾回收机制收回 。  </p><p><img src="https://i.imgur.com/b5pM7hE.png" alt=""><br><strong>使用封闭函数简写闭包</strong><br><img src="https://i.imgur.com/eu3enth.png" alt=""></p><p><strong>闭包的作用</strong><br>1.将一个变量长期驻扎在内存当中，可用于循环中存索引值<br><img src="https://i.imgur.com/ruhKqYk.png" alt=""></p><p>2.私有变量计数器，外部无法访问，只能通过对应的函数来访问；避免全局变量的污染。<br><img src="https://i.imgur.com/Yc6kQVb.png" alt=""></p><h2 id="内置对象"><a href="#内置对象" class="headerlink" title="内置对象"></a>内置对象</h2><p>1.document</p><blockquote><pre><code>document.referrer  //获取上一个跳转页面的地址(需要服务器环境)  </code></pre></blockquote><p>2.location</p><blockquote><pre><code>window.location.href  //获取或者重定url地址window.location.search //获取地址参数部分window.location.hash //获取页面锚点或者叫哈希值</code></pre></blockquote><p>3.Math</p><blockquote><pre><code>Math.random 获取0-1的随机数Math.floor 向下取整Math.ceil 向上取整</code></pre></blockquote><h2 id="面向对象"><a href="#面向对象" class="headerlink" title="面向对象"></a>面向对象</h2><p><strong>面向过程与面向对象编程</strong><br>1、面向过程：所有的工作都是现写现用。</p><p>2、面向对象：是一种编程思想，许多功能事先已经编写好了，在使用时，只需要关注功能的运用，而不需要这个功能的具体实现过程。</p><p><strong>javascript对象</strong><br>将相关的变量和函数组合成一个整体，这个整体叫做对象，对象中的变量叫做属性，变量中的函数叫做方法。javascript中的对象类似字典。</p><p><strong>8.1创建对象的方法</strong><br>1、单体<br><img src="https://i.imgur.com/KtEySXa.png" alt=""></p><p>2、工厂模式<br><img src="https://i.imgur.com/35AshCy.png" alt=""></p><p>3、原型模式<br><img src="https://i.imgur.com/hKOYvdj.png" alt=""></p><p>4、继承<br><img src="https://i.imgur.com/Y8sPzmq.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;定时器&quot;&gt;&lt;a href=&quot;#定时器&quot; class=&quot;headerlink&quot; title=&quot;定时器&quot;&gt;&lt;/a&gt;定时器&lt;/h2&gt;&lt;p&gt;定时器在javascript中的作用:1、制作动画;2、异步操作;3、函数缓冲与节流。&lt;br&gt;
    
    </summary>
    
      <category term="javascript" scheme="http://yoursite.com/categories/javascript/"/>
    
    
      <category term="javascript" scheme="http://yoursite.com/tags/javascript/"/>
    
  </entry>
  
</feed>
