<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>AppleGuard的博客</title>
  
  <subtitle>天下事有难易乎？为之，则难者亦易矣；不为，则易者亦难矣。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-09-16T11:40:22.495Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>huhua</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MySQL的一些优化</title>
    <link href="http://yoursite.com/2018/09/16/MySQL%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BC%98%E5%8C%96/"/>
    <id>http://yoursite.com/2018/09/16/MySQL的一些优化/</id>
    <published>2018-09-16T10:46:55.000Z</published>
    <updated>2018-09-16T11:40:22.495Z</updated>
    
    <content type="html"><![CDATA[<p>MySQL单表记录数过大时，增删改查性能都会急剧下降，我们可以采取一些措施来优化其性能。<br><a id="more"></a>  </p><h2 id="单表优化"><a href="#单表优化" class="headerlink" title="单表优化"></a>单表优化</h2><p>除非单表数据未来会一直不断上涨，否则不要一开始就考虑拆分，拆分会带来逻辑、部署、运维的各种复杂度，一般以整型值为主的表在千万级以下，字符串为主的表在五百万以下是没有太大问题的。而事实上很多时候MySQL单表的性能依然有不少优化空间，甚至能正常支撑千万级以上的数据量.</p><h4 id="字段"><a href="#字段" class="headerlink" title="字段"></a>字段</h4><ul><li>尽量使用TINYINT、SMALLINT、MEDIUM_INT作为整数类型而非INT，如果非负则加上UNSIGNED</li><li>VARCHAR的长度只分配真正需要的空间</li><li>使用枚举或整数代替字符串类型</li><li>尽量使用TIMESTAMP而非DATETIME，</li><li>单表不要有太多字段，建议在20以内</li><li>避免使用NULL字段，很难查询优化且占用额外索引空间</li><li>用整型来存IP<h4 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h4></li></ul><p>创建索引<br>这是最基本的索引，它没有任何限制。它有以下几种创建方式：</p><blockquote><pre><code>CREATE INDEX indexName ON mytable(username(length)); 如果是CHAR，VARCHAR类型，length可以小于字段实际长度；如果是BLOB和TEXT类型，必须指定 length。</code></pre></blockquote><blockquote><pre><code>修改表结构(添加索引)ALTER table tableName ADD INDEX indexName(columnName)</code></pre></blockquote><blockquote><pre><code>创建表的时候直接指定CREATE TABLE mytable(       ID INT NOT NULL,        username VARCHAR(16) NOT NULL,       INDEX [indexName] (username(length))      );  删除索引的语法DROP INDEX [indexName] ON mytable; </code></pre></blockquote><ul><li>索引并不是越多越好，要根据查询有针对性的创建，考虑在WHERE和ORDER BY命令上涉及的列建立索引，可根据EXPLAIN来查看是否用了索引还是全表扫描</li><li>应尽量避免在WHERE子句中对字段进行NULL值判断，否则将导致引擎放弃使用索引而进行全表扫</li><li>值分布很稀少的字段不适合建索引，例如”性别”这种只有两三个值的字段</li><li>字符字段只建前缀索引</li><li>字符字段最好不要做主键</li><li>不用外键，由程序保证约束</li><li>尽量不用UNIQUE，由程序保证约束</li><li>使用多列索引时主意顺序和查询条件保持一致，同时删除不必要的单列索引</li></ul><h4 id="查询SQL"><a href="#查询SQL" class="headerlink" title="查询SQL"></a>查询SQL</h4><ul><li>可通过开启慢查询日志来找出较慢的SQL</li><li>不做列运算：SELECT id WHERE age + 1 = 10，任何对列的操作都将导致表扫描，它包括数据库教程函数、计算表达式等等，查询时要尽可能将操作移至等号右边</li><li>sql语句尽可能简单：一条sql只能在一个cpu运算；大语句拆小语句，减少锁时间；一条大sql可以堵死整个库</li><li>不用SELECT *</li><li>OR改写成IN：OR的效率是n级别，IN的效率是log(n)级别，in的个数建议控制在200以内</li><li>不用函数和触发器，在应用程序实xian</li><li>避免%xxx式查询</li><li>少用JOIN</li><li>使用同类型进行比较，比如用’123’和’123’比，123和123比</li><li>尽量避免在WHERE子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描</li><li>对于连续数值，使用BETWEEN不用IN：SELECT id FROM t WHERE num BETWEEN 1 AND 5</li><li>列表数据不要拿全表，要使用LIMIT来分页，每页数量也不要太大</li></ul><h4 id="引擎"><a href="#引擎" class="headerlink" title="引擎"></a>引擎</h4><p>目前广泛使用的是MyISAM和InnoDB两种引擎：</p><p>MyISAM</p><p>MyISAM引擎是MySQL 5.1及之前版本的默认引擎，它的特点是：</p><ul><li>不支持行锁，读取时对需要读到的所有表加锁，写入时则对表加排它锁</li><li>不支持事务</li><li>不支持外键</li><li>不支持崩溃后的安全恢复</li><li>在表有读取查询的同时，支持往表中插入新纪录</li><li>支持BLOB和TEXT的前500个字符索引，支持全文索引</li><li>支持延迟更新索引，极大提升写入性能</li><li>对于不会进行修改的表，支持压缩表，极大减少磁盘空间占用</li></ul><p>InnoDB</p><p>InnoDB在MySQL 5.5后成为默认索引，它的特点是：</p><ul><li>支持行锁，采用MVCC来支持高并发</li><li>支持事务</li><li>支持外键</li><li>支持崩溃后的安全恢复</li><li>不支持全文索引</li></ul><p>总体来讲，MyISAM适合SELECT密集型的表，而InnoDB适合INSERT和UPDATE密集型的表</p><h2 id="读写分离"><a href="#读写分离" class="headerlink" title="读写分离"></a>读写分离</h2><p>也是目前常用的优化，从库读主库写，一般不要采用双主或多主引入很多复杂性，尽量采用文中的其他方案来提高性能。同时目前很多拆分的解决方案同时也兼顾考虑了读写分离.  </p><h2 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h2><p>缓存可以发生在这些层次：</p><ul><li>MySQL内部：在系统调优参数介绍了相关设置</li><li>数据访问层：比如MyBatis针对SQL语句做缓存，而Hibernate可以精确到单个记录，这里缓存的对象主要是持久化对象Persistence Object</li><li>应用服务层：这里可以通过编程手段对缓存做到更精准的控制和更多的实现策略，这里缓存的对<br>是数据传输对象Data Transfer Object</li><li>Web层：针对web页面做缓存</li><li>浏览器客户端：用户端的缓存</li></ul><p>可以根据实际情况在一个层次或多个层次结合加入缓存。这里重点介绍下服务层的缓存实现，目前主要有两种方式：</p><p>直写式（Write Through）：在数据写入数据库后，同时更新缓存，维持数据库与缓存的一致性。这也是当前大多数应用缓存框架如Spring Cache的工作方式。这种实现非常简单，同步好，但效率一般。<br>回写式（Write Back）：当有数据要写入数据库时，只会更新缓存，然后异步批量的将缓存数据同步到数据库上。这种实现比较复杂，需要较多的应用逻辑，同时可能会产生数据库与缓存的不同步，但效率非常高。</p><h2 id="表分区"><a href="#表分区" class="headerlink" title="表分区"></a>表分区</h2><p>MySQL在5.1版引入的分区是一种简单的水平拆分，用户需要在建表的时候加上分区参数，对应用是透明的无需修改代码</p><p>对用户来说，分区表是一个独立的逻辑表，但是底层由多个物理子表组成，实现分区的代码实际上是通过对一组底层表的对象封装，但对SQL层来说是一个完全封装底层的黑盒子。MySQL实现分区的方式也意味着索引也是按照分区的子表定义，没有全局索引</p><p><img src="https://i.imgur.com/4UenpUK.jpg" alt=""></p><p>用户的SQL语句是需要针对分区表做优化，SQL条件中要带上分区条件的列，从而使查询定位到少量的分区上，否则就会扫描全部分区，可以通过EXPLAIN PARTITIONS来查看某条SQL语句会落在那些分区上，从而进行SQL优化，如下图5条记录落在两个分区上</p><p><img src="https://i.imgur.com/cRydn4u.png" alt=""></p><p>分区的好处是：</p><ul><li>可以让单表存储更多的数据</li><li>分区表的数据更容易维护，可以通过清楚整个分区批量删除大量数据，也可以增加新的分区来支持新插入的数据。另外，还可以对一个独立分区进行优化、检查、修复等操作</li><li>部分查询能够从查询条件确定只落在少数分区上，速度会很快</li><li>分区表的数据还可以分布在不同的物理设备上，从而高效利用多个硬件设备</li><li>使用分区表赖避免某些特殊瓶颈，例如InnoDB单个索引的互斥访问、ext3文件系统的inode锁竞争</li><li>可以备份和恢复单个分区</li></ul><p>分区的限制和缺点：</p><ul><li>一个表最多只能有1024个分区</li><li>如果分区字段中有主键或者唯一索引的列，那么所有主键列和唯一索引列都必须包含进来</li><li>分区表无法使用外键约束</li><li>NULL值会使分区过滤无效</li><li>所有分区必须使用相同的存储引擎</li></ul><p>分区的类型：</p><ul><li>RANGE分区：基于属于一个给定连续区间的列值，把多行分配给分区</li><li>LIST分区：类似于按RANGE分区，区别在于LIST分区是基于列值匹配一个离散值集合中的某个值来进行选择</li><li>HASH分区：基于用户定义的表达式的返回值来进行选择的分区，该表达式使用将要插入到表中的这些行的列值进行计算。这个函数可以包含MySQL中有效的、产生非负整数值的任何表达式</li><li>KEY分区：类似于按HASH分区，区别在于KEY分区只支持计算一列或多列，且MySQL服务器提供其自身的哈希函数。必须有一列或多列包含整数值</li></ul><p>分区适合的场景有：</p><p>最适合的场景数据的时间序列性比较强，则可以按时间来分区，如下所示：</p><p><img src="https://i.imgur.com/49ATKFU.png" alt=""></p><p>查询时加上时间范围条件效率会非常高。</p><h2 id="垂直拆分"><a href="#垂直拆分" class="headerlink" title="垂直拆分"></a>垂直拆分</h2><p>垂直分库是根据数据库里面的数据表的相关性进行拆分，比如：一个数据库里面既存在用户数据，又存在订单数据，那么垂直拆分可以把用户数据放到用户库、把订单数据放到订单库。垂直分表是对数据表进行垂直拆分的一种方式，常见的是把一个多字段的大表按常用字段和非常用字段进行拆分，每个表里面的数据记录数一般情况下是相同的，只是字段不一样，使用主键关联</p><p>比如原始的用户表是：</p><p><img src="https://i.imgur.com/tANr4Cp.jpg" alt=""></p><p>垂直拆分后是：</p><p><img src="https://i.imgur.com/DxH5uvc.jpg" alt=""></p><p>垂直拆分的优点是：</p><ul><li>可以使得行数据变小，一个数据块(Block)就能存放更多的数据，在查询时就会减少I/O次数(每次查询时读取的Block 就少)</li><li>可以达到最大化利用Cache的目的，具体在垂直拆分的时候可以将不常变的字段放一起，将经常改变的放一起</li><li>数据维护简单</li></ul><p>缺点是：</p><ul><li>主键出现冗余，需要管理冗余列</li><li>会引起表连接JOIN操作（增加CPU开销）可以通过在业务服务器上进行join来减少数据库压力</li><li>依然存在单表数据量过大的问题（需要水平拆分）</li><li>事务处理复杂</li></ul><h2 id="水平拆分"><a href="#水平拆分" class="headerlink" title="水平拆分"></a>水平拆分</h2><p>水平拆分是通过某种策略将数据分片来存储，分库内分表和分库两部分，每片数据会分散到不同的MySQL表或库，达到分布式的效果，能够支持非常大的数据量。前面的表分区本质上也是一种特殊的库内分表。  </p><p>库内分表，仅仅是单纯的解决了单一表数据过大的问题，由于没有把表的数据分布到不同的机器上，因此对于减轻MySQL服务器的压力来说，并没有太大的作用，大家还是竞争同一个物理机上的IO、CPU、网络，这个就要通过分库来解决</p><p>前面垂直拆分的用户表如果进行水平拆分，结果是：  </p><p><img src="https://i.imgur.com/Uw3IfpV.jpg" alt=""></p><p>实际情况中往往会是垂直拆分和水平拆分的结合，即将Users_A_M和Users_N_Z再拆成Users和UserExtras，这样一共四张表。</p><p>水平拆分的优点是:</p><ul><li>不存在单库大数据和高并发的性能瓶颈</li><li>应用端改造较少</li><li>提高了系统的稳定性和负载能力</li></ul><p>缺点是：</p><ul><li>分片事务一致性难以解决</li><li>跨节点Join性能差，逻辑复杂</li><li>数据多次扩展难度跟维护量极大</li></ul><h4 id="分片原则"><a href="#分片原则" class="headerlink" title="分片原则"></a>分片原则</h4><ul><li>能不分就不分，参考单表优化</li><li>分片数量尽量少，分片尽量均匀分布在多个数据结点上，因为一个查询SQL跨分片越多，则总体性能越差，虽然要好于所有数据在一个分片的结果，只在必要的时候进行扩容，增加分片数量</li><li>分片规则需要慎重选择做好提前规划，分片规则的选择，需要考虑数据的增长模式，数据的访问模式，分片关联性问题，以及分片扩容问题，最近的分片策略为范围分片，枚举分片，一致性Hash分片，这几种分片都有利于扩容</li><li>尽量不要在一个事务中的SQL跨越多个分片，分布式事务一直是个不好处理的问题</li><li>查询条件尽量优化，尽量避免Select * 的方式，大量数据结果集下，会消耗大量带宽和CPU资源，查询尽量避免返回大量结果集，并且尽量为频繁使用的查询语句建立索引。</li><li>通过数据冗余和表分区赖降低跨库Join的可能</li></ul><p>这里特别强调一下分片规则的选择问题，如果某个表的数据有明显的时间特征，比如订单、交易记录等，则他们通常比较合适用时间范围分片，因为具有时效性的数据，我们往往关注其近期的数据，查询条件中往往带有时间字段进行过滤，比较好的方案是，当前活跃的数据，采用跨度比较短的时间段进行分片，而历史性的数据，则采用比较长的跨度存储。</p><p>总体上来说，分片的选择是取决于最频繁的查询SQL的条件，因为不带任何Where语句的查询SQL，会遍历所有的分片，性能相对最差，因此这种SQL越多，对系统的影响越大，所以我们要尽量避免这种SQL的产生。</p><h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>由于水平拆分牵涉的逻辑比较复杂，当前也有了不少比较成熟的解决方案。这些方案分为两大类：客户端架构和代理架构。</p><p><strong>客户端架构</strong></p><p>通过修改数据访问层，如JDBC、Data Source、MyBatis，通过配置来管理多个数据源，直连数据库，并在模块内完成数据的分片整合，一般以Jar包的方式呈现</p><p>这是一个客户端架构的例子：</p><p><img src="https://i.imgur.com/Agf5sLW.jpg" alt=""></p><p>可以看到分片的实现是和应用服务器在一起的，通过修改Spring JDBC层来实现</p><p>客户端架构的优点是：</p><ul><li>应用直连数据库，降低外围系统依赖所带来的宕机风险</li><li>集成成本低，无需额外运维的组件</li></ul><p>缺点是：</p><ul><li>限于只能在数据库访问层上做文章，扩展性一般，对于比较复杂的系统可能会力不从心</li><li>将分片逻辑的压力放在应用服务器上，造成额外风险</li></ul><p><strong>代理架构</strong></p><p>通过独立的中间件来统一管理所有数据源和数据分片整合，后端数据库集群对前端应用程序透明，需要独立部署和运维代理组件</p><p>这是一个代理架构的例子：</p><p><img src="https://i.imgur.com/3qo732p.jpg" alt=""></p><p>代理组件为了分流和防止单点，一般以集群形式存在，同时可能需要Zookeeper之类的服务组件来管理</p><p>代理架构的优点是：</p><ul><li>能够处理非常复杂的需求，不受数据库访问层原来实现的限制，扩展性强</li><li>对于应用服务器透明且没有增加任何额外负载</li></ul><p>缺点是：</p><ul><li>需部署和运维独立的代理中间件，成本高</li><li>应用需经过代理来连接数据库，网络上多了一跳，性能有损失且有额外风险</li></ul><p>各方案比较：  </p><p><img src="https://i.imgur.com/lF4mQje.jpg" alt=""></p><p>如此多的方案，如何进行选择？可以按以下思路来考虑：</p><ul><li>确定是使用代理架构还是客户端架构。中小型规模或是比较简单的场景倾向于选择客户端架构，复杂场景或大规模系统倾向选择代理架构</li><li>具体功能是否满足，比如需要跨节点ORDER BY，那么支持该功能的优先考虑</li><li>不考虑一年内没有更新的产品，说明开发停滞，甚至无人维护和技术支持</li><li>最好按大公司-&gt;社区-&gt;小公司-&gt;个人这样的出品方顺序来选择</li><li>选择口碑较好的，比如github星数、使用者数量质量和使用者反馈</li><li>开源的优先，往往项目有特殊需求可能需要改动源代码</li></ul><p>按照上述思路，推荐以下选择：</p><ul><li>客户端架构：ShardingJDBC</li><li>代理架构：MyCat或者Atlas</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MySQL单表记录数过大时，增删改查性能都会急剧下降，我们可以采取一些措施来优化其性能。&lt;br&gt;
    
    </summary>
    
      <category term="数据库" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="MySQL" scheme="http://yoursite.com/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy抓取装修图片</title>
    <link href="http://yoursite.com/2018/07/26/Scrapy%E6%8A%93%E5%8F%96%E8%A3%85%E4%BF%AE%E5%9B%BE%E7%89%87/"/>
    <id>http://yoursite.com/2018/07/26/Scrapy抓取装修图片/</id>
    <published>2018-07-26T12:35:00.000Z</published>
    <updated>2018-09-01T16:08:34.191Z</updated>
    
    <content type="html"><![CDATA[<p>scrapy框架也学了几天了，这次来试一下抓取360的家居装修图，加强对框架的理解和掌握。<br><a id="more"></a>  </p><h1 id="抓取分析"><a href="#抓取分析" class="headerlink" title="抓取分析"></a>抓取分析</h1><h2 id="网页分析"><a href="#网页分析" class="headerlink" title="网页分析"></a>网页分析</h2><p>目标站点定位： <a href="http://image.so.com/" target="_blank" rel="noopener">http://image.so.com/</a><br>打开浏览器后，我们在发现图片是通过发送Ajax请求加载的，所以我们直接找到对应的请求接口，然后访问，获取里面的json数据，数据中包含的图片的链接地址和其他信息，另外，我们拿到图片链接后，再去请求，便可以下载图片到本地。基本思路就是这样了。下面开始做吧。<br><img src="https://i.imgur.com/ZsIbFIH.png" alt=""></p><h1 id="url分析"><a href="#url分析" class="headerlink" title="url分析"></a>url分析</h1><p>寻找url规律：<br><img src="https://i.imgur.com/uA6x16d.png" alt=""></p><p>sn为偏移量，每页会显示30张图片，其他参数不变，那么我们可以构造一个参数字典，然后通过urlencode方法转成URL的格式，这样就构造出了需要访问的url。</p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h1><p>首先，创建项目，并生成一个Spider。</p><h2 id="编写items文件，确定抓取目标"><a href="#编写items文件，确定抓取目标" class="headerlink" title="编写items文件，确定抓取目标"></a>编写items文件，确定抓取目标</h2><p><img src="https://i.imgur.com/qvDOYkY.png" alt=""><br>这里，我们打算将图片信息放到mongodb和mysql中，先在这里定义集合和表名称，等会方便插入数据。  </p><h2 id="编写spider文件，生成数据和请求。"><a href="#编写spider文件，生成数据和请求。" class="headerlink" title="编写spider文件，生成数据和请求。"></a>编写spider文件，生成数据和请求。</h2><p><img src="https://i.imgur.com/REXVhXc.png" alt=""><br>这里，我们只要遍历json文件中的list字段就可以获取相应的信息。</p><h2 id="编写管道文件Pipeline，处理数据"><a href="#编写管道文件Pipeline，处理数据" class="headerlink" title="编写管道文件Pipeline，处理数据"></a>编写管道文件Pipeline，处理数据</h2><ul><li><p>存储到MongoDB<br><img src="https://i.imgur.com/foSb5dd.png" alt=""></p></li><li><p>存储到mysql<br>这里我们先提前在mysql中创建好相应的数据库和表，用来保存接下来的信息，这里就不多讲了。<br><img src="https://i.imgur.com/MWervfP.png" alt="">  </p></li><li><p>存储图片到本地<br>这里需要用到Scrapy专门处理图片的Pipeline，我们先定义一个当前目录下的文件存储路径 <strong>IMAGES_STORE = ‘.\images’</strong>,之后我们下载的图片会保存在这里。<br><img src="https://i.imgur.com/yJ1Fvgo.png" alt=""></p></li><li><p>setting文件配置<br>接下来我们需要配置setting.py里面的信息了。<br>数据库信息，设置最大抓取页数，请求头啥的。<br><img src="https://i.imgur.com/ZsksNAZ.png" alt=""></p></li></ul><p>管道文件<br><img src="https://i.imgur.com/xxEIa6B.png" alt=""></p><h2 id="运行程序"><a href="#运行程序" class="headerlink" title="运行程序"></a>运行程序</h2><p><img src="https://i.imgur.com/yDqIwWr.png" alt=""></p><p>这样，scrapy便开始下载图片和图片信息了，刚开始出现了几次报错，有参数的类型错误，还有初始化Mysql类属性时，命名造成了干涉等，导致后面方法报错。  </p><p>下面效果展示：<br>MONGO存储：<br><img src="https://i.imgur.com/au1ZCLy.png" alt=""></p><p>MySQL存储：<br><img src="https://i.imgur.com/NtIm5Me.png" alt=""></p><p>本地存储：<br><img src="https://i.imgur.com/AysaEQB.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;scrapy框架也学了几天了，这次来试一下抓取360的家居装修图，加强对框架的理解和掌握。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>破解验证码识别</title>
    <link href="http://yoursite.com/2018/07/24/%E7%A0%B4%E8%A7%A3%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB/"/>
    <id>http://yoursite.com/2018/07/24/破解验证码识别/</id>
    <published>2018-07-24T02:49:58.000Z</published>
    <updated>2018-09-01T16:08:17.448Z</updated>
    
    <content type="html"><![CDATA[<p>现在，很多网站采取各种各样的措施来反爬虫，其中之一就是使用验证码。当我们访问网页时，必须先通过验证码才能够访问页面。下面我们便来讲2种验证码的识别方式和一些思路。当然我们也可以直接使用付费的打码平台，那样可以增加识别的准确度，毕竟出了钱的嘛。哈哈！<br><a id="more"></a>  </p><h2 id="PIL库"><a href="#PIL库" class="headerlink" title="PIL库"></a>PIL库</h2><p>其实，验证码识别归根到底还是对各种各样图片的识别和操作，python中有很对图像处理的库，其中PIL就是其中之一。 所以在处理验证码识别之前，必须先了解PIL库。 下面附上其API源码地址，以及对应的学习博客。<br>源码地址：<a href="https://pillow-cn.readthedocs.io/zh_CN/latest/reference/index.html" target="_blank" rel="noopener">https://pillow-cn.readthedocs.io/zh_CN/latest/reference/index.html</a><br>参考博客：<a href="https://blog.csdn.net/louishao/article/details/69879981" target="_blank" rel="noopener">https://blog.csdn.net/louishao/article/details/69879981</a><br>下面我们就开始验证码识别之路了。</p><h2 id="图形验证码"><a href="#图形验证码" class="headerlink" title="图形验证码"></a>图形验证码</h2><p>以中国知网为例：<br><img src="https://i.imgur.com/KfnZxWP.png" alt=""></p><p>首先，我们先拿到上图中绿线标记的验证码，下载到本地项目文件中，<br>然后，编写如下代码:<br><img src="https://i.imgur.com/Dx7xscT.png" alt=""><br>输出结果为：486V， 可是实际图片为4S6V，这是因为验证码内多余线条干扰了图片的识别，像这类情况，还需要做出额外的处理，比如转灰度，二值化等。对应的方法如下：<br><img src="https://i.imgur.com/WagZ5W3.png" alt=""></p><p>当然，实际处理中并不是这样，一般我们会先对模糊图片进行灰度处理后，再设定二值化的阈值，实际处理如下。<br><img src="https://i.imgur.com/li0queT.png" alt=""></p><p>输出结果：4S6V<br>进行识别时，先设定好二值化阈值threshold，进行适当调试，直到图片能正常识别为止。</p><h2 id="滑动验证码"><a href="#滑动验证码" class="headerlink" title="滑动验证码"></a>滑动验证码</h2><p><strong>过程分析：</strong></p><p>滑动验证码主要的验证方式是拖动滑块，拼合图像；如图象完全拼合，则验证成功，即表单提交成功，否则需要重新验证。<br>如图：<br><img src="https://i.imgur.com/pYD8C6K.png" alt=""></p><p>下面，我们就以极验的验证码为例，来讲诉一下识别方法。<br>因为极验的验证码在拖动验证码后会生成一个加密的表单提交到后台，所有为了避免麻烦我们直接用selenium模拟浏览器行为来完成验证。<br>登陆网站：极验官网</p><blockquote><pre><code>目标站点：https://account.geetest.com/login</code></pre></blockquote><p><img src="https://i.imgur.com/sVfDi03.png" alt=""></p><p>首先，我们发现登陆界面有个智能按钮，一般来说，在输入邮箱之后，点击按钮就会弹出滑动验证窗口，然后我们在拖动验证码完成图像拼接，完成验证。<br>弹出滑块验证：<br><img src="https://i.imgur.com/u1iul7H.png" alt=""><br>拖动滑块，完成验证：<br><img src="https://i.imgur.com/dKjUoEK.png" alt=""></p><p>所以，滑块验证识别需要完成以下步骤：</p><blockquote><pre><code>1. 模拟点击验证按钮2. 识别滑块的缺口位置3. 模拟拖动滑块</code></pre></blockquote><p><strong>如何实现以上步骤呢？我们先需要将任务进行分解，看似只有三大步骤，其实里面坑还有很多的，稍后会做解释。</strong>  </p><p>第一步，输入账号，获取智能按钮，使用selenium模拟点击，获取带有缺口的图片。  </p><p>第二步，获取上面缺口图片中的完整图片。这里有个地方要注意，正常情况下我们在网页源代码里是找不到完整图的，因为它被隐藏了，必须执行javascript语句才能出现完整图。<br><img src="https://i.imgur.com/v8ouv0P.png" alt=""></p><p>我们将display参数改为block，opacity参数改为1，然后进行截图，就可以拿到完整的验证码图片了。</p><p>第三步，对比两张图片的所有RGB像素点，得到缺口位置。</p><p>第四步，模拟人的拖动习惯，这里也有坑，极验的验证码增加了机器轨迹识别，匀速移动，或者随机速度移动滑块都不能通过验证，所以我们将需要拖动的总位移分成一段一段小的轨迹，先匀加速拖动后匀减速拖动。</p><p>第五步，按照规定轨迹进行拖动，完成验证。</p><p>第六步，完成账号登陆。</p><p>过程分析完了，下面我们就来写代码试一下：<br>首先，我们先将整个代码的一个逻辑思路做一个大致的概括吧。<br><img src="https://i.imgur.com/Ayg9jgg.png" alt="">  </p><p>接下来，我们便来逐一完成main函数里要实现的功能了。</p><p><strong>代码示例：</strong><br><img src="https://i.imgur.com/vCAntIv.png" alt=""><br><img src="https://i.imgur.com/p7giBb3.png" alt=""><br><img src="https://i.imgur.com/EC60eS5.png" alt=""></p><p>通过以上代码我们便拿到了完整的验证码和带有缺口的验证码。<br>缺口图片：<br><img src="https://i.imgur.com/YHzDimh.png" alt="">  </p><p>完整图片：<br><img src="https://i.imgur.com/KNLDRsB.png" alt=""></p><p>这里我在调试的时候碰到一个坑，因为chrome中，location方法不滚动，直接返回相对整个html的坐标，我的电脑是15.6寸的，显示设置上布局的缩放大小被放大到1.25倍，导致location返回的坐标与验证码的坐标有误差。修改布局为100%后就解决了。 下面便是对比图片找出缺口位置。这里我们需要遍历图片的坐标点，获取像素点的RGB数据。  </p><p><strong>代码示例</strong><br><img src="https://i.imgur.com/TQzHllN.png" alt=""></p><p>接下来就是获取滑块的移动路径和模拟拖动行为了。<br><img src="https://i.imgur.com/Rlb2brE.png" alt=""><br><img src="https://i.imgur.com/uaEc7Jg.png" alt=""></p><p>执行代码：<br><img src="https://i.imgur.com/G8pc1GS.png" alt=""> </p><p>最后终于成功了，踩了这么多坑，终于完成了滑块验证码的破解。。。现在已经实现功能，因为还可能出现其他情况，接下来我们还需要完善一下代码，其实也可封装成一个类，测试的时候我们会发现，图片会弹出小怪兽被吃了，那是因为系统识别我们是机器行为，所以不通过，这里我们需要修改加速度参数，再增加一个回调。<br><img src="https://i.imgur.com/f9DPlNE.png" alt=""></p><p>代码执行效果：<br><img src="https://i.imgur.com/br02DBK.png" alt=""></p><p>这样我们就成功破解验证码，并登陆到网页界面了。。。忙活了一上午，吃饭去了。</p><p>源码地址：<a href="https://github.com/appleguardu/spider_projects/tree/master/Captcha" target="_blank" rel="noopener">https://github.com/appleguardu/spider_projects/tree/master/Captcha</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;现在，很多网站采取各种各样的措施来反爬虫，其中之一就是使用验证码。当我们访问网页时，必须先通过验证码才能够访问页面。下面我们便来讲2种验证码的识别方式和一些思路。当然我们也可以直接使用付费的打码平台，那样可以增加识别的准确度，毕竟出了钱的嘛。哈哈！&lt;br&gt;
    
    </summary>
    
      <category term="反爬虫" scheme="http://yoursite.com/categories/%E5%8F%8D%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="反爬虫" scheme="http://yoursite.com/tags/%E5%8F%8D%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>python之json基础</title>
    <link href="http://yoursite.com/2018/07/19/python%E4%B9%8Bjson%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/2018/07/19/python之json数据解析/</id>
    <published>2018-07-19T05:27:17.000Z</published>
    <updated>2018-09-01T16:07:40.743Z</updated>
    
    <content type="html"><![CDATA[<p>当我们爬取异步加载的网页从上面获取的往往是json类型的文件，通常我们需要对这类数据进行处理后才能使用，下面就来说说python中对json数据的基本处理方法。<br><a id="more"></a>  </p><h2 id="什么是JSON"><a href="#什么是JSON" class="headerlink" title="什么是JSON"></a>什么是JSON</h2><p>JSON 指的是 JavaScript 对象表示法（JavaScript Object Notation）；是一种轻量级的文本数据交换格式；适用于进行数据交互的场景，比如网站前台与后台之间的数据交互。  </p><p>简单来说json就是javascript中的对象和数组，所以这两种结构就是对象和数组两种结构，通过这两种结构可以表示各种复杂的结构：  </p><blockquote><pre><code>1.对象：对象在js中表示为{ }括起来的内容，数据结构为 { key：value, key：value, ... }的键值对的结构，在面向对象的语言中，key为对象的属性，value为对应的属性值，所以很容易理解，取值方法为 对象.key 获取属性值，这个属性值的类型可以是数字、字符串、数组、对象这几种。</code></pre></blockquote><blockquote><pre><code>2.数组：数组在js中是中括号[ ]括起来的内容，数据结构为 [&quot;Python&quot;, &quot;javascript&quot;, &quot;C++&quot;, ...]，取值方式和所有语言中一样，使用索引获取，字段值的类型可以是 数字、字符串、数组、对象几种。</code></pre></blockquote><h2 id="JSON模块"><a href="#JSON模块" class="headerlink" title="JSON模块"></a>JSON模块</h2><p>Python中提供了json模块，总共有四个方法：dumps、dump、loads、load，用于字符串 和 python数据类型间进行转换。<br><strong>1.json.loads()</strong><br>对数据进行解码，把Json格式字符串 解码 转换成Python对象 。<br><img src="https://i.imgur.com/gFu4n1P.png" alt=""></p><p>实例：<br><img src="https://i.imgur.com/3zOg7fE.png" alt=""></p><p><strong>2.json.dumps()</strong><br>对数据进行编码，返回一个str对象，把一个Python对象编码转换成Json字符串。如果直接将dict类型的数据写入json文件中会发生报错，因此在将数据写入时需要用到该函数。<br><img src="https://i.imgur.com/foRbCB5.png" alt=""></p><p>实例：<br><img src="https://i.imgur.com/N5ids9K.png" alt=""></p><p><strong>3.json.dump()</strong><br>json.dump()用于将dict类型的数据转成str，并写入到json文件中  </p><p><img src="https://i.imgur.com/b06PiXX.png" alt=""></p><p><strong>4.json.load()</strong><br>json.load()直接从json文件中读取数据，并将str转为dict类型</p><p>实例：<br><img src="https://i.imgur.com/bBO2Wo7.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当我们爬取异步加载的网页从上面获取的往往是json类型的文件，通常我们需要对这类数据进行处理后才能使用，下面就来说说python中对json数据的基本处理方法。&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="json" scheme="http://yoursite.com/tags/json/"/>
    
  </entry>
  
  <entry>
    <title>Ajax数据的爬取</title>
    <link href="http://yoursite.com/2018/07/18/Ajax%E6%95%B0%E6%8D%AE%E7%9A%84%E7%88%AC%E5%8F%96/"/>
    <id>http://yoursite.com/2018/07/18/Ajax数据的爬取/</id>
    <published>2018-07-18T02:53:33.000Z</published>
    <updated>2018-09-01T16:07:21.360Z</updated>
    
    <content type="html"><![CDATA[<p>不知道你在写爬虫时有没有遇到这种情况：在浏览器中可以看到正常显示的页面数据，但是用requests得到的html文件里却没有这些数据。这是因为浏览器中的页面时经过JavaScript处理数据后生成的，Ajax加载就是其中一种方式，下面我们便来讲一下如何在异步加载的网页中抓取有用数据。<br><a id="more"></a>  </p><h2 id="什么是Ajax"><a href="#什么是Ajax" class="headerlink" title="什么是Ajax"></a>什么是Ajax</h2><p><strong>Ajax</strong>，就是异步的JavaScript和XML。它是利用JavaScript在保证页面不被刷新，页面链接不改变的情况下，后台与服务器进行数据交互并更新部分网页的技术。  </p><h2 id="Ajax的实现过程"><a href="#Ajax的实现过程" class="headerlink" title="Ajax的实现过程"></a>Ajax的实现过程</h2><blockquote><pre><code>第一步，创建xmlhttprequest对象，var xmlhttp =new XMLHttpRequest（);XMLHttpRequest对象用来和服务器交换数据。第二步，使用xmlhttprequest对象的open（）和send（）方法发送资源请求给服务器。第三步，使用xmlhttprequest对象的responseText或responseXML属性获得服务器的响应。第四步，onreadystatechange函数，当发送请求到服务器，我们想要服务器响应执行一些功能就需要使用onreadystatechange函数，每次xmlhttprequest对象的readyState发生改变都会触发onreadystatechange函数。</code></pre></blockquote><p>真实的数据都是通过JavaScript向服务器发送ajax请求得到的，所以要抓取这些数据，必须搞清楚这些请求是如何发送的，以及发送到了哪里，参数有哪些等等。下面我们便来分析一下。</p><h2 id="Ajax的分析方法"><a href="#Ajax的分析方法" class="headerlink" title="Ajax的分析方法"></a>Ajax的分析方法</h2><p><strong>1.分析请求</strong>  </p><p>以微博为例，当我们拖动刷新额内容时，此时网页由Ajax加载的，页面url不会发生变化，下面我们就借助chrome浏览器来分析。<br>首先，打开我巴萨新闻的官方微博链接（本人巴萨球迷。。。）  </p><p><img src="https://i.imgur.com/56DDocS.png" alt=""><br>element选项可以查看网页源代码，但这不是我们要看的。<br>点击network选项卡，刷新一下页面，发现出现很多内容，那么这些内容便是页面在加载过程中浏览器与服务器之间发送请求和接受响应内容的记录。  </p><p><img src="https://i.imgur.com/MHF0oEP.png" alt=""></p><p>这里我们找到右边条目栏中有一个getIndex的请求，点击它，查看相应信息。<br><img src="https://i.imgur.com/gJAwlqS.png" alt=""><br>我们发现Request Headers里面有一个信息围为<strong>X-Request-with：XMLHttpRequest</strong>，这就表明此请求为Ajax请求；ajax其实有特殊的请求类型，叫作<strong>xhr</strong>，稍后我们会讲到。</p><p><img src="https://i.imgur.com/RU6k1BN.png" alt=""><br>preview选项可以查看响应内容，发现响应内容为json格式。</p><p><img src="https://i.imgur.com/C1IyRw2.png" alt=""><br>response选项卡中为真实的返回数据</p><p>现在我们回到chrome浏览器最开始发送的请求<br><img src="https://i.imgur.com/XAmdgHA.png" alt=""><br>原始链接返回的response代码知识执行了一些JS。所以我们看到的微博页面的真实数据不是原始页面，而是执行了页面中的JS代码后再次向后台发送Ajax请求后，浏览器拿到数据在进一步进行渲染出来的页面。  </p><p><strong>2.筛选Ajax请求</strong>  </p><p>前面提到ajax请求类型为xhr，这里我们点击network选项卡下的XHR选项，然后刷新页面发现，下面的请求都是Ajax请求。打开一条请求，我们会看到Request URL，request Headers，response headers, response body等内容。</p><p><img src="https://i.imgur.com/VE5CfXF.png" alt=""></p><p>对Ajax请求分析完毕后，现在我们就可以模拟这些请求，来抓取数据啦啦啦！！！</p><h2 id="抓取Ajax加载的数据"><a href="#抓取Ajax加载的数据" class="headerlink" title="抓取Ajax加载的数据"></a>抓取Ajax加载的数据</h2><p><strong>1.分析请求</strong>  </p><p><img src="https://i.imgur.com/9CzVySM.png" alt=""></p><p>我们选择其中一条请求，发现这是一个GET类型额请求，</p><blockquote><pre><code>https://m.weibo.cn/api/container/getIndex?type=uid&amp;value=1990303727&amp;containerid=1076031990303727&amp;page=2</code></pre></blockquote><p>URL包含4个参数，分别是：type，value，containerid，page。<br>然后我们分析一下规律后发现，type一直为uid，value值也不变，containerid就是107603加上value，唯一变化的参数就是page，顾名思义应该就是第几页的意思。请求分析完了。接下来看看响应内容。  </p><p><strong>2.分析响应</strong>  </p><p><img src="https://i.imgur.com/CCbittX.png" alt=""></p><p><img src="https://i.imgur.com/27cs9hW.png" alt=""></p><p>观察可知，响应内容为JSON格式，然后主要的信息全包含在cardlistInfo和cards里面。其中cards里面有一个mblog，包含的信息都是微博里面的点赞数，评论数，转发数等，还有page_Info主要包含的时博文内容这些信息。接下来我们就将这些格式化的内容提取出来。哈哈！  </p><h2 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h2><p>首先构造URL，发送请求，拿到json文本内容。<br><img src="https://i.imgur.com/4gqvJj8.png" alt=""></p><p>返回内容为：<br><img src="https://i.imgur.com/w80FzyK.png" alt=""></p><p>接着就是解析json文本，提取需要的数据了，我们就提取cards里面的mblog里面的一些内容，比如正文，点赞数，评论数，转发数。<br><img src="https://i.imgur.com/JP5arE0.png" alt=""></p><p>这样我们便抓取到第二页所需的微博信息啦：<br><img src="https://i.imgur.com/uLDc29w.png" alt=""></p><p>之前是抓取的一个页面的数据，如果要抓取所有页面，只需传入page的最大值，然后使用for循环即可。  </p><h2 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h2><p>我们将数据保存至mongodb中，由于是字典，我们直接进行存储即可<br>连接数据库：<br><img src="https://i.imgur.com/aUFTnAK.png" alt=""></p><p>保存：<br><img src="https://i.imgur.com/x74m1XQ.png" alt=""><br>运行结果：<br><img src="https://i.imgur.com/22jrCiD.png" alt=""></p><p>查看数据库：<br><img src="https://i.imgur.com/7enGi11.png" alt=""></p><p>这样我们便完成了Ajax加载的网页数据的抓取啦，在分析ajax请求时，我们还可以使用抓包工具Fiddler来进行分析。<br>另外，有些网站把ajax请求的所有参数全部加密了。我们根本没办法构造自己所需要的数据的请求。这种情况下就用<strong>selenium+phantomJS</strong>，调用浏览器内核，并利用phantomJS执行js来模拟人为操作以及触发页面中的js脚本。从填写表单到点击按钮再到滚动页面，全部都可以模拟，不考虑具体的请求和响应过程，只是完完整整的把人浏览页面获取数据的过程模拟一遍。 这个我们会在下篇博文来讲解。 </p><p>文末附上写的另一个小demo，今日头条街拍图片：<br><a href="https://github.com/appleguardu/spider_projects/blob/master/Ajax_spider/toutiao_today.py" target="_blank" rel="noopener">https://github.com/appleguardu/spider_projects/blob/master/Ajax_spider/toutiao_today.py</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不知道你在写爬虫时有没有遇到这种情况：在浏览器中可以看到正常显示的页面数据，但是用requests得到的html文件里却没有这些数据。这是因为浏览器中的页面时经过JavaScript处理数据后生成的，Ajax加载就是其中一种方式，下面我们便来讲一下如何在异步加载的网页中抓取有用数据。&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Selenium爬取淘宝商品</title>
    <link href="http://yoursite.com/2018/07/13/selenium%E7%88%AC%E5%8F%96%E6%B7%98%E5%AE%9D%E5%95%86%E5%93%81/"/>
    <id>http://yoursite.com/2018/07/13/selenium爬取淘宝商品/</id>
    <published>2018-07-13T05:28:28.000Z</published>
    <updated>2018-09-01T16:06:56.803Z</updated>
    
    <content type="html"><![CDATA[<p>前一天学习了selenium的相关用法，今天打算用selenium来模拟浏览器操作，抓取数据，抓取目标就定为淘宝网吧，接下来我们就开工啦！<br><a id="more"></a>  </p><h2 id="目标设定"><a href="#目标设定" class="headerlink" title="目标设定"></a>目标设定</h2><p>目标定为抓取淘宝iPhone手机销售的相关信息：其中包括商品的图片，名字，最关键的价格，购买量，商铺，地点等信息。目标定好了，接下来我们就去分析网页规律了。  </p><h2 id="网页分析"><a href="#网页分析" class="headerlink" title="网页分析"></a>网页分析</h2><p>首先，确定抓取入口，也就是start url为淘宝搜索页面，url的链接为：  </p><blockquote><pre><code>https://s.taobao.com/search?q=iphone</code></pre></blockquote><p>当然，这是第一页的链接。但是我们要获取的是所有页面的，先下拉到页面底部，我们发现：<br><img src="https://i.imgur.com/B2oVHX4.png" alt="">  </p><p>用黑线圈起来的是一个分页导航，其中包括前5页的链接，下一页的链接，同时还有一个任意页码跳转的链接，页面总数为100。 若要获取每一页的内容， 我们的思路是将页码从1到100的顺序进行遍历一遍。 此时我们便又两种方式来进行遍历：  </p><blockquote><pre><code>1.点击下一页，等下一页面加载出来后进行爬取2.在页面跳转框逐一输入1~100，点击确定按钮后跳转到对应页码进行爬取  </code></pre></blockquote><p>对于方法一，有一个弊端就是如果爬取过程中出现异常，程序退出了，我在点击下一页时，可能无法快速切换到后续的页面；所以，在爬取过程中还需要记录当前页码数，如果点击下一页加载失败的话，还要增加异常检查等。 方法二，我们只需输入对应页面的数值就可以跳转，比较方便。<br>当页面成功加载出来后，我们利用selenium获取页面源代码，然后进行解析，就可以获取目标信息了。  </p><h2 id="获取商品列表"><a href="#获取商品列表" class="headerlink" title="获取商品列表"></a>获取商品列表</h2><p>首先，构造出要抓取的url：  </p><blockquote><pre><code>https://s.taobao.com/search?q=iphone </code></pre></blockquote><p>q指得是搜索的关键字，改变这个参数就可以搜索不同的商品，这里我们把定义成一个变量keyword。<br>接下来我们进行代码演练了。<br><img src="https://i.imgur.com/9zhFFeT.png" alt=""><br><img src="https://i.imgur.com/2Bu9bhH.png" alt=""></p><p>输出结果：<br><img src="https://i.imgur.com/4NA5EqV.png" alt=""></p><p>这样我们便拿到了第一页商品信息的源码啦，当然这个代码还不完善，还没添加跳转功能，因为page我们设计是从第一页抓取的，当page值为2时，我们就要翻页了，那么怎么进行翻页呢？首先获取页码输入框啊，然后获取确定按钮进行操作啦。<br><img src="https://i.imgur.com/Aepl0Mc.png" alt=""><br><img src="https://i.imgur.com/94knVze.png" alt=""></p><p>我们发现，当前页面都是高亮显示的，那么我们只需要判断高亮的页码数是当前页码数就证明页面跳转成功。这里可以使用等待条件：text_to_be_present_in_element,它会等待指定的文本出现在某一节点里面时即返回成功。<br><img src="https://i.imgur.com/djIRfga.png" alt=""></p><h2 id="解析商品列表"><a href="#解析商品列表" class="headerlink" title="解析商品列表"></a>解析商品列表</h2><p>既然上面已经能拿到页面商品的源代码了，那么我们接下来就可以对源码进行解析了。<br>分析可得，每个页面有44家店铺，在源代码里我们可以发现整个商品列表使用的CSS选择器是#mainsrp-itemlist .items .item, 它匹配了整个页面的44件商品，然后我们先用items()方法将其全部提取出来，他会返回一个生成器，再使用for循环进行遍历，得到每个item对象，然后用pyquery的find方法选取我们所需要的信息即可。<br>代码如下：<br><img src="https://i.imgur.com/JDIeNPe.png" alt=""></p><p>提取结果：<br><img src="https://i.imgur.com/gxIzM05.png" alt=""></p><p>哈哈，这样我们就已经拿到第一页所有商品的数据了，接下，我们要把100页的信息都爬取下来，这里可以直接使用for循环进行遍历页码。  </p><h2 id="爬取所有商品，保存数据库"><a href="#爬取所有商品，保存数据库" class="headerlink" title="爬取所有商品，保存数据库"></a>爬取所有商品，保存数据库</h2><p>先创建数据库：<br><img src="https://i.imgur.com/cV5FihW.png" alt=""></p><p>然后for循环遍历所有页面<br><img src="https://i.imgur.com/YYEc2ko.png" alt=""></p><p>最后的结果：<br><img src="https://i.imgur.com/tjwiYUs.png" alt=""><br>MongoDB数据库：<br><img src="https://i.imgur.com/BJyyvDe.png" alt=""></p><p>然后，所有页面的iphone数据都被我们爬取下来了。。。当然，代码还可以再精简一点。。。  </p><p>源码地址:<a href="https://github.com/appleguardu/spider_projects/blob/master/taobao_iphone.py" target="_blank" rel="noopener">https://github.com/appleguardu/spider_projects/blob/master/taobao_iphone.py</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前一天学习了selenium的相关用法，今天打算用selenium来模拟浏览器操作，抓取数据，抓取目标就定为淘宝网吧，接下来我们就开工啦！&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="selenium" scheme="http://yoursite.com/tags/selenium/"/>
    
  </entry>
  
  <entry>
    <title>selenium的使用</title>
    <link href="http://yoursite.com/2018/07/11/selenium%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2018/07/11/selenium的使用/</id>
    <published>2018-07-11T09:10:04.000Z</published>
    <updated>2018-09-01T16:06:41.136Z</updated>
    
    <content type="html"><![CDATA[<p>对于像淘宝，京东这样的网站，虽然它也是采用Ajax获取数据，但是Ajax接口中含有许多加密参数，我们很难去直接找出其规律；所以我们需要直接使用模拟浏览器运行的方式来实现，这样就可以做到在浏览器中看到的是什么样，抓取的源码就是什么样的啦，即可见可爬，系不系很可怕。<br><a id="more"></a>  </p><h2 id="Selenium"><a href="#Selenium" class="headerlink" title="Selenium"></a>Selenium</h2><p>selenium是一个自动化测试工具，使用它可以驱动浏览器执行特定的动作，如点击，下拉，填充表单等操作，同时可以获取浏览器当前呈现页面的源代码，做到可见可爬。<br>下面以chrome浏览器为例，来说一下selenium的使用方式。  </p><blockquote><pre><code>首先，配置谷歌浏览器对于版本的ChromeDriver，可以找资源去下载，然后将chromedriver.exe放在python文件夹下的Scripts文件夹里。然后，安装python的selenium库，自行百度；接下来我们就可以使用selenium啦!!!</code></pre></blockquote><p><img src="https://i.imgur.com/mZll8Ht.png" alt=""></p><h2 id="声明浏览器对象"><a href="#声明浏览器对象" class="headerlink" title="声明浏览器对象"></a>声明浏览器对象</h2><p>首先初始化浏览器对象，接着就可调用browser对象，让其模拟浏览器操作啦！<br><img src="https://i.imgur.com/gxAlhda.png" alt=""></p><h2 id="访问页面"><a href="#访问页面" class="headerlink" title="访问页面"></a>访问页面</h2><p>使用get()方法访问页面<br><img src="https://i.imgur.com/CUAkEEO.png" alt=""></p><p>运行代码后，会弹出一个淘宝界面，并输出网页的源代码。<br><img src="https://i.imgur.com/5NwK8Ag.jpg" alt=""></p><h2 id="节点操作"><a href="#节点操作" class="headerlink" title="节点操作"></a>节点操作</h2><p>当我们用selenium驱动浏览器模拟操作时，需要先获取其操作节点，比如输入框，按钮的位置。  </p><p><strong>单个节点</strong><br>以淘宝搜索框为例，先从源代码里找到其位置。<br><img src="https://i.imgur.com/ed1UMjD.png" alt=""></p><blockquote><pre><code>&lt;input id=&quot;q&quot; name=&quot;q&quot; aria-label=&quot;请输入搜索文字&quot; accesskey=&quot;s&quot; autofocus=&quot;autofocus&quot; autocomplete=&quot;off&quot; class=&quot;search-combobox-input&quot; aria-haspopup=&quot;true&quot; aria-combobox=&quot;list&quot; role=&quot;combobox&quot; x-webkit-grammar=&quot;builtin:translate&quot; tabindex=&quot;0&quot;&gt;</code></pre></blockquote><p>这个节点或者说是标签中有很多属性，比如id属性：q， name属性：q；那么我们可以根据css选择器或者xpath获取这个节点。  </p><blockquote><pre><code>1. find_element_by_id(&apos;q&apos;)  2. find_element_by_css_selector(&apos;#q&apos;)3. find_element_by_xpath(&apos;//*[@id=&quot;q&quot;]&apos;)以上方法都可获取搜索输入框，一般获取单个节点用以上方法就足够了，另外还可以根据name或者class_name来获取。  </code></pre></blockquote><p>selenium还有通用方法，find_element(By.ID, ‘q’),需要传入两个参数，一个时查找方式，一个是查找值。返回结果为webelement类型。</p><p><strong>多个节点</strong><br>如果我们需要查找满足条件的所有节点，就绪要用到find_elements()方法。返回结果为列表类型，列表在每个节点为webelement类型。查找方式与单个节点一致。  </p><p><strong>节点交互</strong><br>selenium可以驱动浏览器执行相应的操作，我们先找到这个节点，比如淘宝搜索框，然后调用方法：  </p><blockquote><pre><code>send_keys() 输入文字clear() 清空文字click() 点击按钮 还有很多其他的操作，可以去google查</code></pre></blockquote><p><img src="https://i.imgur.com/11otno6.png" alt="">  </p><p>代码运行效果：<br><img src="https://i.imgur.com/Q0duWFX.png" alt=""></p><h2 id="动作链"><a href="#动作链" class="headerlink" title="动作链"></a>动作链</h2><p>还有一些其他的操作，他们没有像上面那样是针对单个节点的，他们没有特定的对象，比如鼠标拖动，键盘按键等，这些动作就是动作链。  </p><p><img src="https://i.imgur.com/kleR7Xi.png" alt="">  </p><p>动作前：<br><img src="https://i.imgur.com/odEyGQP.png" alt=""></p><p>动作后：<br><img src="https://i.imgur.com/yApOg8G.png" alt=""></p><h2 id="执行JavaScript"><a href="#执行JavaScript" class="headerlink" title="执行JavaScript"></a>执行JavaScript</h2><p>对于某些操作，selenium API并没有提供；比如，下拉进度条，它可直接模拟运行JS脚本，此时可以使用<strong>execute_script()</strong>方法实现。<br><img src="https://i.imgur.com/9IILHeu.png" alt=""></p><p>执行效果：<br><img src="https://i.imgur.com/rsl9U5d.png" alt=""></p><h2 id="获取节点信息"><a href="#获取节点信息" class="headerlink" title="获取节点信息"></a>获取节点信息</h2><p>通过前面讲的page_source属性可以获取网页的源代码，那么我们都可以对获取的网页源码进行解析(re, BS, pyquery)。不过selenium自带相关的方法和属性来直接提取节点信息。  </p><p><strong>获取属性值</strong><br>先选中节点，再通过节点对象调用get_attribute(‘属性名’)就可获取这个属性对应的内容。<br><img src="https://i.imgur.com/A4Ts0H9.png" alt=""> </p><p><strong>获取文本值</strong><br>每个WeBElement节点都有text属性，直接调用这个属性就可以获取节点内部的文本信息，这相当于BS中的get_text()和pyquery中的text()方法。<br>加入获取的节点为p，那么p.text就是其文本内容。这里就不演示了  </p><p><strong>获取其他值</strong><br>另外，一个节点还有其他属性，方法都差不多。  </p><blockquote><pre><code>节点.id   # 获取节点id节点.location   # 获取节点在页面中的位置节点.tag_name   # 获取节点标签名称节点.size   # 获取节点宽高</code></pre></blockquote><h2 id="切换frame"><a href="#切换frame" class="headerlink" title="切换frame"></a>切换frame</h2><p>网页中有一种节点叫作 iframe， 即子frame，相当于页面的子页面，它的结构和外部网页的结构完全一致。selenium打开页面后，默认在父级的Frame里操作，若此时页面中还有子Frame，它是不能获取到子Frame里面的节点的，所以，此时需要使用<strong>switch_to.frame()</strong>方法来切换Frame。  </p><h2 id="延时等待"><a href="#延时等待" class="headerlink" title="延时等待"></a>延时等待</h2><p>在selenium中， get()方法会在网页框架加载结束后执行，此时若获取page_soucre，可能并不是浏览器完全加载的页面，如果某些页面有额外的Ajax请求，我们就不一定能捕获成功。 所以，这里需要延时等待一定时间，确保节点已经加载完全。<br><strong>隐式等待</strong><br>当使用隐式等待时，如果selenium没有在DOM中找到节点，将继续等待，超出设定时间后，则会抛出找不到节点的异常。<br><img src="https://i.imgur.com/Zq8XTVv.png" alt=""></p><p><strong>显式等待</strong><br>由于页面加载有时候会受到网络的影响，所以隐式等待这种设置固定时间并不是很靠谱；这里引入显式等待。<br>显式等待指定了要查找的节点，然后指定一个最长等待时间，如果在规定时间加载出来便返回该节点；如果没有，即抛出超时异常。  </p><p>首先，需要引入WebDriverWait这个对象，调用until方法，传入等待条件excepted_conditions。<br><strong>expected_conditions模块:</strong>  </p><p>以下两个条件类验证title，验证传入的参数title是否等于或包含于driver.title </p><blockquote><pre><code>title_is title_contains</code></pre></blockquote><p>以下两个条件验证元素是否出现，传入的参数都是元组类型的locator，如(By.ID, ‘kw’) ;顾名思义，一个只要一个符合条件的元素加载出来就通过；另一个必须所有符合条件的元素都加载出来才行 </p><blockquote><pre><code>presence_of_element_located presence_of_all_elements_located</code></pre></blockquote><p>以下三个条件验证元素是否可见，前两个传入参数是元组类型的locator，第三个传入WebElement ;第一个和第三个其实质是一样的 </p><blockquote><pre><code>visibility_of_element_located invisibility_of_element_located visibility_of</code></pre></blockquote><p>以下两个条件判断某段文本是否出现在某元素中，一个判断元素的text，一个判断元素的value </p><blockquote><pre><code>text_to_be_present_in_element text_to_be_present_in_element_value</code></pre></blockquote><p>以下条件判断frame是否可切入，可传入locator元组或者直接传入定位方式：id、name、index或WebElement </p><blockquote><pre><code>frame_to_be_available_and_switch_to_it</code></pre></blockquote><p>以下条件判断是否有alert出现 </p><blockquote><pre><code>alert_is_present</code></pre></blockquote><p>以下条件判断元素是否可点击，传入locator<br>element_to_be_clickable</p><p>以下四个条件判断元素是否被选中，第一个条件传入WebElement对象，第二个传入locator元组<br>第三个传入WebElement对象以及状态，相等返回True，否则返回False<br>第四个传入locator以及状态，相等返回True，否则返回False </p><blockquote><pre><code>element_to_be_selected element_located_to_be_selected element_selection_state_to_be element_located_selection_state_to_be</code></pre></blockquote><p>最后一个条件判断一个元素是否仍在DOM中，传入WebElement对象，可以判断页面是否刷新了 </p><blockquote><pre><code>staleness_of</code></pre></blockquote><p><img src="https://i.imgur.com/Xpj1Vj2.png" alt=""></p><h2 id="前进和后退"><a href="#前进和后退" class="headerlink" title="前进和后退"></a>前进和后退</h2><blockquote><pre><code>browser.back()   后退，浏览器中返回上一个页面browser.forward()   前进，下一个页面</code></pre></blockquote><h2 id="Cookies操作"><a href="#Cookies操作" class="headerlink" title="Cookies操作"></a>Cookies操作</h2><p>使用selenium还可以方便的对cookies进行操作<br><img src="https://i.imgur.com/GMq4nfL.png" alt=""></p><h2 id="选项卡管理"><a href="#选项卡管理" class="headerlink" title="选项卡管理"></a>选项卡管理</h2><p>在实际访问网页时，会开启一个个选项卡。在selenium中，我们也会对其进行操作。  </p><p><img src="https://i.imgur.com/ZrNbSSZ.png" alt=""></p><p>效果：<br><img src="https://i.imgur.com/P0NCssl.png" alt=""></p><h2 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h2><p>在使用selenium时，也会遇到异常情况；比如访问超时，节点无法找到等，所有也需要使用try。。。except来捕获各类异常。<br>下面举个例子：<br><img src="https://i.imgur.com/agPTE5p.png" alt="">  </p><p>那么到目前为止，selenium的常规用法基本上都讲了一遍了，若是想了解更多相关内容的话，可以参考以下几个链接：<br>selenium官方文档：<br><a href="http://selenium-python.readthedocs.io/installation.html#introduction" title="selenium官方文档" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/installation.html#introduction</a><br>博文参考：<br><a href="https://blog.csdn.net/wenboyu/article/details/78162667" target="_blank" rel="noopener">https://blog.csdn.net/wenboyu/article/details/78162667</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于像淘宝，京东这样的网站，虽然它也是采用Ajax获取数据，但是Ajax接口中含有许多加密参数，我们很难去直接找出其规律；所以我们需要直接使用模拟浏览器运行的方式来实现，这样就可以做到在浏览器中看到的是什么样，抓取的源码就是什么样的啦，即可见可爬，系不系很可怕。&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>爬取猫眼电影信息</title>
    <link href="http://yoursite.com/2018/07/05/%E7%88%AC%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1%E4%BF%A1%E6%81%AF/"/>
    <id>http://yoursite.com/2018/07/05/爬取猫眼电影信息/</id>
    <published>2018-07-05T07:52:36.000Z</published>
    <updated>2018-09-01T16:05:11.731Z</updated>
    
    <content type="html"><![CDATA[<p>本篇内容主要讲解的是一个基础爬虫实例——爬去猫眼电影排行榜前100的影片信息，通过这个实例，来理解一个爬虫的基本思路。<br><a id="more"></a>  </p><h2 id="抓取目标"><a href="#抓取目标" class="headerlink" title="抓取目标"></a>抓取目标</h2><p>确定需要爬去的信息：猫眼电影top100的影片信息，这些信息包括电影名称，上映时间，演员，图片等。</p><h2 id="网页分析"><a href="#网页分析" class="headerlink" title="网页分析"></a>网页分析</h2><p>我们发现榜单top100对应的url是maoyan.com/board/4 。这便是我们要抓取的目标站点。<br><img src="https://i.imgur.com/uP4n5f7.png" alt=""></p><p>首先我们看到，霸王别姬排名第一，页面中的有效信息包括电影名，主演，上映时间，地区，评分和图片等。接着我们继续翻页，观察页面url和内容的变化；<br><img src="https://i.imgur.com/1mniQsL.png" alt=""></p><p>可以看到页面的url变成了<a href="http://maoyan.com/board/4?offset=10，多了一个参数offset=10，目前页面显示内容为排行11~20的电影；我们再点击下一页，发现此时url变成了http://maoyan.com/board/4?offset=20，显示结果为排行21~30的电影，所以：" target="_blank" rel="noopener">http://maoyan.com/board/4?offset=10，多了一个参数offset=10，目前页面显示内容为排行11~20的电影；我们再点击下一页，发现此时url变成了http://maoyan.com/board/4?offset=20，显示结果为排行21~30的电影，所以：</a><br>网页显示规律为：offset表示偏移量，如果偏移量为n，那么显示电影排行为n+1到n+10，每页显示10部电影。如果要获取top100，则需要请求10次，每次抓取一个页面，提取出里面有用的信息。  </p><h2 id="抓取第一页"><a href="#抓取第一页" class="headerlink" title="抓取第一页"></a>抓取第一页</h2><p>首先我们来抓取第一页的内容。在此之前我们先创建一个主函数main(),用来执行主要操作。<br>第一步，获取网页内容。</p><p><img src="https://i.imgur.com/2AQs44l.png" alt=""></p><h2 id="解析页面"><a href="#解析页面" class="headerlink" title="解析页面"></a>解析页面</h2><p>获取到网页源代码后，就需要对页面进行解析，然后提取出我们想要的信息。我们先回到网页查看一下真实源码，在<strong>开发者模式下的Network监听组件</strong>中查看源代码。注意：不要在Elements选项卡中查看源代码，因为那里的源码可能是经过JavaScript操作，与原始请求不同，所以需要从Network下查看。<br>排行第一的源代码片段：<br><img src="https://i.imgur.com/ydbNLEA.png" alt=""> </p><p>经过分析发现，每部电影的信息都包含在一个<dd></dd>的节点中。  </p><blockquote><pre><code>排名对应节点： &lt;i class=&quot;board-index board-index-1&quot;&gt;1&lt;/i&gt;图片对应节点：&lt;img data-src=&quot;http://p1.meituan.net/movie/20803f59291c47e1e116c11963ce019e68711.jpg@160w_220h_1e_1c&quot; alt=&quot;霸王别姬&quot; class=&quot;board-img&quot; /&gt;电影名称：&lt;p class=&quot;name&quot;&gt;&lt;a href=&quot;/films/1203&quot; title=&quot;霸王别姬&quot; data-act=&quot;boarditem-click&quot; data-val=&quot;{movieId:1203}&quot;&gt;霸王别姬&lt;/a&gt;&lt;/p  主演：&lt;p class=&quot;star&quot;&gt;主演：张国荣,张丰毅,巩俐&lt;/p&gt;上映时间：&lt;p class=&quot;releasetime&quot;&gt;上映时间：1993-01-01(中国香港)&lt;/p&gt;</code></pre></blockquote><p>下面我们就用正则表达式来提取上述信息，当然也可以使用Xpath和css选择器，我这里为了巩固一下正则用法，就先用re来写，虽然麻烦点。下面定义一个解析的方法。<br><img src="https://i.imgur.com/sVlFvdE.png" alt=""></p><p>这样我们就得到了第一个页面的信息，以列表list的形式返回的，每部电影信息保存在一个元组中。<br><img src="https://i.imgur.com/tAlDjx9.png" alt=""></p><p>由于数据的格式比较乱，我们需要对其进行格式的整理，这里选择遍历数据列表，把每部电影信息保存为字典dict形式。现在将方法修改一下。<br><img src="https://i.imgur.com/gZUzIXx.png" alt=""></p><p>整理后的信息如下：<br><img src="https://i.imgur.com/HYgoqD7.png" alt=""></p><h2 id="保存提取信息"><a href="#保存提取信息" class="headerlink" title="保存提取信息"></a>保存提取信息</h2><p>由于上面我们将每部电影信息都保存为了字典类型，这样更为结构化；现在我们将他们写入到文本文件中。这里通过JSON库的dumps()方法将字典进行序列化，由于输出结果有中文，需要将ensure_ascii指定为False，这样可以保证中文输出，而不是Unicode编码格式。<br><img src="https://i.imgur.com/bilBPae.png" alt=""></p><h2 id="main-方法调用"><a href="#main-方法调用" class="headerlink" title="main()方法调用"></a>main()方法调用</h2><p>最后我们定义一个main()方法用来调用前面的方法，将单页的电影信息全部写入到文件中。<br><img src="https://i.imgur.com/wUBtzNw.png" alt=""></p><p>单页文件输出结果：<br><img src="https://i.imgur.com/RzwULTw.png" alt=""></p><p>这样，我们就成功完成了第一页电影信息的爬取，但是我们目标是爬取所有电影，接下来就来试试把</p><h2 id="分页爬取"><a href="#分页爬取" class="headerlink" title="分页爬取"></a>分页爬取</h2><p>之前已经分析过了，每次翻页之后，url都会有变化，多了一个参数offset，所以我们只需要构造一下URL即可。下面将代码修改一下：<br><img src="https://i.imgur.com/M3gGvdY.png" alt=""></p><p>输出结果：<br><img src="https://i.imgur.com/9gFXrP7.png" alt=""></p><h2 id="代码最后的修改"><a href="#代码最后的修改" class="headerlink" title="代码最后的修改"></a>代码最后的修改</h2><p>现在我们已经基本上完成了所有电影信息的抓取，但是部分代码还是有缺陷，比如请求部分未作异常处理，爬取速度太快，可能会被反爬措施封锁等，所以还需要将代码优化一下。<br>代码地址：<a href="https://github.com/appleguardu/spider_projects" target="_blank" rel="noopener">https://github.com/appleguardu/spider_projects</a></p><p>好了，一个简单爬虫基本上完成！！！继续加油！！！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇内容主要讲解的是一个基础爬虫实例——爬去猫眼电影排行榜前100的影片信息，通过这个实例，来理解一个爬虫的基本思路。&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>python爬虫基本流程</title>
    <link href="http://yoursite.com/2018/07/02/python%E7%88%AC%E8%99%AB%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B/"/>
    <id>http://yoursite.com/2018/07/02/python爬虫基本流程/</id>
    <published>2018-07-02T06:34:17.000Z</published>
    <updated>2018-09-01T16:04:45.177Z</updated>
    
    <content type="html"><![CDATA[<h2 id="何为爬虫"><a href="#何为爬虫" class="headerlink" title="何为爬虫"></a>何为爬虫</h2><p>爬虫：请求网站并提取数据的自动化程序<br><a id="more"></a>  </p><h2 id="爬虫的基本流程"><a href="#爬虫的基本流程" class="headerlink" title="爬虫的基本流程"></a>爬虫的基本流程</h2><p>1.. 发起请求<br>通过HTTP库(主要用到urllib和requests这两个库)向目标站点发起请求，即发送一个Request，请求可以包含headers等信息，等待服务器响应。  </p><blockquote><pre><code>Request：浏览器就发送消息给该网址所在的服务器，这个过程叫做HTTP Request。  Request介绍：1.请求方式：主要有GET、POST两种类型，另外还有HEAD、PUT、DELETE、OPTIONS等。2.请求URL：URL全称统一资源定位符，如一个网页文档、一张图片、一个视频等都可以用URL唯一来确定。3.请求头：包含请求时的头部信息，如User-Agent、Host、Cookies等信息。4.请求体：请求时额外携带的数据如表单提交时的表单数据</code></pre></blockquote><p>2.. 获取响应内容<br>如果服务器正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，类型可能是HtML，Json字符串，二进制数据(图片，视频)等类型。  </p><blockquote><pre><code>Response:服务器收到浏览器发送的消息后，能够根据浏览器发送消息的内容，做相应处理，然后把消息回传给浏览器。这个过程叫做HTTP Response。浏览器收到服务器的Response信息后，会对信息进行相应处理，然后展示。 Response介绍：1.响应状态：有多种响应状态，如200代表成功、301跳转、404找不到页面、502服务器错误。2.响应头：如内容类型、内容长度、服务器信息、设置Cookie等等。3.响应体：最主要的部分，包含了请求资源的内容，如网页HTML、图片二进制数据等。 </code></pre></blockquote><p>3.. 解析获取的内容<br>获取的内容若是HTML，可以用正则表达式，网页解析库(BS,XPath等)进行解析。可能是Json，可以直接转为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理。  </p><p>4.. 保存数据<br>保存形式多样，可以存为文本，也可以保存至数据库，或者保存特定格式的文件。  </p><blockquote><pre><code>1.文本：纯文本、Json、Xml等。2.关系型数据库：如MySQL、Oracle等具有结构化表结构形式存储。3.非关系型数据库：如MongoDB、Redis等Key-Value形式存储。4.二进制文件：如图片、视频、音频等等直接保存成特定格式即可。  </code></pre></blockquote><p>另外，爬虫过程还会遇到很多问题，例如JavaScript渲染的问题，网站的反爬措施，爬取速度太慢等等，这些问题会在后期更新博文的时候再做详细的讲解。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;何为爬虫&quot;&gt;&lt;a href=&quot;#何为爬虫&quot; class=&quot;headerlink&quot; title=&quot;何为爬虫&quot;&gt;&lt;/a&gt;何为爬虫&lt;/h2&gt;&lt;p&gt;爬虫：请求网站并提取数据的自动化程序&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>爬虫的思路</title>
    <link href="http://yoursite.com/2018/07/01/%E7%88%AC%E8%99%AB%E7%9A%84%E6%80%9D%E8%B7%AF/"/>
    <id>http://yoursite.com/2018/07/01/爬虫的思路/</id>
    <published>2018-07-01T01:56:37.000Z</published>
    <updated>2018-09-01T16:04:00.947Z</updated>
    
    <content type="html"><![CDATA[<p>万事开头难，刚开始写爬虫程序的时候，不知从何下手；通过最近一段时间练习写爬虫程序，也渐渐有了一些简单的思路，在这里做个笔记，供日后翻阅。<br><a id="more"></a>  </p><h2 id="准备好目标URL"><a href="#准备好目标URL" class="headerlink" title="准备好目标URL"></a>准备好目标URL</h2><p>一般分为一下两种情况：  </p><p>1.准备start_url</p><blockquote><pre><code>页码总数不确定，规律不明显通过代码提取下一页地址使用xpath/css/re提取url寻找url地址，部分参数在当前页面中，比如当前页码数和总页码数，使用这两个参数构建url</code></pre></blockquote><p>2.准备url_list</p><blockquote><pre><code>页码总数清楚url的地址规律明显</code></pre></blockquote><h2 id="发送请求，获取响应"><a href="#发送请求，获取响应" class="headerlink" title="发送请求，获取响应"></a>发送请求，获取响应</h2><blockquote><pre><code>添加随机user-agent，添加随机代理ip，如果对方判断我们是爬虫之后应该添加更多的headers字段，包含cookie；cookie的处理可以使用session来解决准备一堆能用的cookie，组成cookie池。如果不登录:        准备刚刚开始能够成功请求的cookie，即接收对方网站设置在response的cookie如果登录:       准备多个账号       使用程序获取每个账号的cookie       请求登录之后才能访问的网站随机选择上一步获取的cookie</code></pre></blockquote><h2 id="提取数据"><a href="#提取数据" class="headerlink" title="提取数据"></a>提取数据</h2><p>1.确定数据的位置:  </p><p>如果数据在当前的url地址中,提取的是列表页中的数据;直接请求列表页的url地址，不用进入详情页,提取详情页的数据<br><strong>1.确定url; 2.发送请求; 3.提取数据; 4.返回</strong></p><p>如果数据不在当前的url地址中,在其他响应中(例如Ajax请求)，寻找数据的位置<br> <strong>在network中从上而下找,使用chrome中的过滤条件，选择除了js，css, image之外的按钮,使用chrome的serach all file，搜索数字和英文</strong></p><p>如果碰到加密的Ajax请求，可使用selenium等工具获取响应源码<br><strong>分析源码，获取目标数据</strong></p><p>2.数据的提取  </p><blockquote><pre><code>1.使用re,xpath，css selector从html中提取整块的数据，先分组，之后每一组在提取2.json数据，可使用get方法进行筛选后，在进行相应的提取</code></pre></blockquote><h2 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h2><blockquote><pre><code>保存在本地json ，csv，text,使用open方法或者os创建本地文件夹，将文件保存至本地。保存在数据库，关系型MySQL，提前创建好表，将数据按字段分类；非关系型，MongoDB， redis等。</code></pre></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;万事开头难，刚开始写爬虫程序的时候，不知从何下手；通过最近一段时间练习写爬虫程序，也渐渐有了一些简单的思路，在这里做个笔记，供日后翻阅。&lt;br&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架学习之CrawlSpider</title>
    <link href="http://yoursite.com/2018/06/17/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E4%B9%8BCrawlSpider/"/>
    <id>http://yoursite.com/2018/06/17/Scrapy框架学习之CrawlSpider/</id>
    <published>2018-06-17T07:56:17.000Z</published>
    <updated>2018-09-01T16:01:16.887Z</updated>
    
    <content type="html"><![CDATA[<p>CrawlSpider是Spider的派生类，Spider类的设计原则是只爬取start_url列表中的网页，而CrawlSpider类定义了一些规则(rule)来提供跟进link的方便的机制，从爬取的网页中获取link并继续爬取。<br><a id="more"></a>  </p><h2 id="源码解读"><a href="#源码解读" class="headerlink" title="源码解读"></a>源码解读</h2><p>快速创建方式：scrapy genspider -t crawl tencent tencent.com </p><p><img src="https://i.imgur.com/xtMx6Ki.png" alt=""><br><img src="https://i.imgur.com/6amTlHs.png" alt=""></p><h2 id="CrawlSpider的属性和方法"><a href="#CrawlSpider的属性和方法" class="headerlink" title="CrawlSpider的属性和方法"></a>CrawlSpider的属性和方法</h2><p><strong>LinkExtractors</strong>  </p><blockquote><pre><code>class scrapy.linkextractors.LinkExtractor</code></pre></blockquote><p>Link Extractors 的目的很简单: 提取链接｡</p><p>每个LinkExtractor有唯一的公共方法是 extract_links()，它接收一个 Response 对象，并返回一个 scrapy.link.Link 对象。</p><p>Link Extractors要实例化一次，并且 extract_links 方法会根据不同的 response 调用多次提取链接｡</p><blockquote><pre><code>class scrapy.linkextractors.LinkExtractor(    allow = (),    deny = (),    allow_domains = (),    deny_domains = (),    deny_extensions = None,    restrict_xpaths = (),    tags = (&apos;a&apos;,&apos;area&apos;),       attrs = (&apos;href&apos;),       canonicalize = True,    unique = True,    process_value = None)</code></pre></blockquote><p>主要参数：</p><ul><li><p>allow：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。</p></li><li><p>deny：与这个正则表达式(或正则表达式列表)不匹配的URL一定不提取。</p></li><li><p>allow_domains：会被提取的链接的domains。</p></li><li><p>deny_domains：一定不会被提取链接的domains。</p></li><li><p>restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接。</p></li></ul><p><strong>rules</strong><br>在rules中包含一个或多个Rule对象，每个Rule对爬取网站的动作定义了特定操作。如果多个rule匹配了相同的链接，则根据规则在本集合中被定义的顺序，第一个会被使用。  </p><blockquote><pre><code>class scrapy.spiders.Rule(    link_extractor,     callback = None,     cb_kwargs = None,     follow = None,     process_links = None,     process_request = None)</code></pre></blockquote><p>主要参数：  </p><ul><li><p>link_extractor：是一个Link Extractor对象，用于定义需要提取的链接。</p></li><li><p>callback： 从link_extractor中每获取到链接时，参数所指定的值作为回调函数，该回调函数接受一个response作为其第一个参数。</p></li></ul><p>注意：当编写爬虫规则时，避免使用parse作为回调函数。由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了 parse方法，crawl spider将会运行失败。</p><ul><li><p>follow：是一个布尔(boolean)值，指定了根据该规则从response提取的链接是否需要跟进。 如果callback为None，follow 默认设置为True ，否则默认为False。</p></li><li><p>process_links：指定该spider中哪个的函数将会被调用，从link_extractor中获取到链接列表时将会调用该函数。该方法主要用来过滤。</p></li><li><p>process_request：指定该spider中哪个的函数将会被调用， 该规则提取到每个request时都会调用该函数。 (用来过滤request)</p></li></ul><p>下面写一个实例：<br><img src="https://i.imgur.com/QTBG9mC.png" alt=""></p><h2 id="Logging"><a href="#Logging" class="headerlink" title="Logging"></a>Logging</h2><p>Scrapy提供了log功能，可以通过 logging 模块使用。</p><p>可以修改配置文件settings.py，任意位置添加下面两行，效果会清爽很多。</p><blockquote><pre><code>LOG_FILE = &quot;TencentSpider.log&quot;LOG_LEVEL = &quot;INFO&quot;</code></pre></blockquote><p>Log levels<br>Scrapy提供5层logging级别:</p><ul><li>CRITICAL - 严重错误(critical)</li><li>ERROR - 一般错误(regular errors)</li><li>WARNING - 警告信息(warning messages)</li><li>INFO - 一般信息(informational messages)</li><li>DEBUG - 调试信息(debugging messages)</li></ul><p>logging设置<br>通过在setting.py中进行以下设置可以被用来配置logging:</p><ul><li>LOG_ENABLED 默认: True，启用logging</li><li>LOG_ENCODING 默认: ‘utf-8’，logging使用的编码</li><li>LOG_FILE 默认: None，在当前目录里创建logging输出文件的文件名</li><li>LOG_LEVEL 默认: ‘DEBUG’，log的最低级别</li><li>LOG_STDOUT 默认: False 如果为 True，进程所有的标准输出(及错误)将会被重定向到log中。例如，执行 print “hello” ，其将会在Scrapy log中显示。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CrawlSpider是Spider的派生类，Spider类的设计原则是只爬取start_url列表中的网页，而CrawlSpider类定义了一些规则(rule)来提供跟进link的方便的机制，从爬取的网页中获取link并继续爬取。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架学习之Downloader Middlewares</title>
    <link href="http://yoursite.com/2018/06/15/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E4%B9%8BDownloader-Middlewares/"/>
    <id>http://yoursite.com/2018/06/15/Scrapy框架学习之Downloader-Middlewares/</id>
    <published>2018-06-15T08:52:03.000Z</published>
    <updated>2018-09-01T16:00:58.679Z</updated>
    
    <content type="html"><![CDATA[<p>Scrapy下载中间件是处于引擎(crawler.engine)和下载(crawler.engine.download())之间的一层组件，可以有多个下载中间件被加载运行；通常用来做异常处理和反爬处理。<br><a id="more"></a>  </p><h2 id="设置下载中间件"><a href="#设置下载中间件" class="headerlink" title="设置下载中间件"></a>设置下载中间件</h2><ul><li><p>当引擎传递请求给下载器的过程中，下载中间件可以对请求进行处理 （例如增加http header信息，增加proxy信息等）；</p></li><li><p>在下载器完成http请求，传递响应给引擎的过程中， 下载中间件可以对响应进行处理（例如进行gzip的解压等）</p></li></ul><p>要激活下载器中间件组件，将其加入到 DOWNLOADER_MIDDLEWARES 设置中。 该设置是一个字典(dict)，键为中间件类的路径，值为其中间件的顺序(order)。</p><p>这里是一个例子:</p><blockquote><pre><code>DOWNLOADER_MIDDLEWARES = {           &apos;mySpider.middlewares.MyDownloaderMiddleware&apos;: 543,  }</code></pre></blockquote><p>源码示例：<br><img src="https://i.imgur.com/jAeXWuy.png" alt=""></p><p>方法介绍：<br><strong>1. process_request(self, request, spider)</strong> </p><p>当每个request通过下载中间件时，该方法被调用。</p><ul><li>process_request() 必须返回以下其中之一：一个 None 、一个 Response 对象、一个 Request 对象或 raise IgnoreRequest:</li></ul><p>如果其返回 None ，Scrapy将继续处理该request，执行其他的中间件的相应方法，直到合适的下载器处理函数(download handler)被调用， 该request被执行(其response被下载)。</p><p>如果其返回 Response 对象，Scrapy将不会调用 任何 其他的 process_request() 或 process_exception() 方法，或相应地下载函数； 其将返回该response。 已安装的中间件的 process_response() 方法则会在每个response返回时被调用。</p><p>如果其返回 Request 对象，Scrapy则停止调用 process_request方法并重新调度返回的request。当新返回的request被执行后， 相应地中间件链将会根据下载的response被调用。</p><p>如果其raise一个 IgnoreRequest 异常，则安装的下载中间件的 process_exception() 方法会被调用。如果没有任何一个方法处理该异常， 则request的errback(Request.errback)方法会被调用。如果没有代码处理抛出的异常， 则该异常被忽略且不记录(不同于其他异常那样)。  </p><ul><li>参数:  </li></ul><p>request (Request 对象) – 处理的request</p><p>spider (Spider 对象) – 该request对应的spider</p><p><strong>2. process_response(self, request, response, spider)</strong></p><p>当下载器完成http请求，传递响应给引擎的时候调用</p><ul><li>process_request() 必须返回以下其中之一: 返回一个 Response 对象、 返回一个 Request 对象或raise一个 IgnoreRequest 异常。</li></ul><p>如果其返回一个 Response (可以与传入的response相同，也可以是全新的对象)， 该response会被在链中的其他中间件的 process_response() 方法处理。</p><p>如果其返回一个 Request 对象，则中间件链停止， 返回的request会被重新调度下载。处理类似于 process_request() 返回request所做的那样。</p><p>如果其抛出一个 IgnoreRequest 异常，则调用request的errback(Request.errback)。 如果没有代码处理抛出的异常，则该异常被忽略且不记录(不同于其他异常那样)。</p><ul><li>参数:</li></ul><p>request (Request 对象) – response所对应的request</p><p>response (Response 对象) – 被处理的response</p><p>spider (Spider 对象) – response所对应的spider</p><h2 id="几种scrapy中通常用来反爬虫的几种措施"><a href="#几种scrapy中通常用来反爬虫的几种措施" class="headerlink" title="几种scrapy中通常用来反爬虫的几种措施"></a>几种scrapy中通常用来反爬虫的几种措施</h2><p>这里因为暂时还没有项目涉及到，我就不写了，引用网上的资料参考。  </p><ul><li>创建middlewares.py文件</li></ul><p>Scrapy代理IP、Uesr-Agent的切换都是通过DOWNLOADER_MIDDLEWARES进行控制，我们在settings.py同级目录下创建middlewares.py文件，包装所有请求。<br><img src="https://i.imgur.com/KrjxbBW.png" alt=""></p><p>为什么HTTP代理要使用base64编码：</p><p>HTTP代理的原理很简单，就是通过HTTP协议与代理服务器建立连接，协议信令中包含要连接到的远程主机的IP和端口号，如果有需要身份验证的话还需要加上授权信息，服务器收到信令后首先进行身份验证，通过后便与远程主机建立连接，连接成功之后会返回给客户端200，表示验证通过，就这么简单，</p><p>下面是具体的信令格式：</p><blockquote><pre><code>CONNECT 59.64.128.198:21 HTTP/1.1Host: 59.64.128.198:21Proxy-Authorization: Basic bGV2I1TU5OTIzUser-Agent: OpenFetion</code></pre></blockquote><p>其中Proxy-Authorization是身份验证信息，Basic后面的字符串是用户名和密码组合后进行base64编码的结果，也就是对username:password进行base64编码。</p><blockquote><pre><code>HTTP/1.0 200 Connection established</code></pre><p>OK，客户端收到收面的信令后表示成功建立连接，接下来要发送给远程主机的数据就可以发送给代理服务器了，代理服务器建立连接后会在根据IP地址和端口号对应的连接放入缓存，收到信令后再根据IP地址和端口号从缓存中找到对应的连接，将数据通过该连接转发出去。</p></blockquote><p><strong>修改settings.py配置USER_AGENTS和PROXIES</strong></p><ul><li>添加USER_AGENTS：</li></ul><p><img src="https://i.imgur.com/E2dgdX0.png" alt=""></p><ul><li>添加代理IP设置PROXIES<br>免费代理IP可以网上搜索，或者付费购买一批可用的私密代理IP</li></ul><p><img src="https://i.imgur.com/eICOPL8.png" alt=""></p><ul><li>除非特殊需要，禁用cookies，防止某些网站根据Cookie来封锁爬虫</li></ul><blockquote><pre><code>COOKIES_ENABLED = False</code></pre></blockquote><ul><li>设置下载延迟</li></ul><blockquote><pre><code>DOWNLOAD_DELAY = 3</code></pre></blockquote><ul><li>最后设置setting.py里的DOWNLOADER_MIDDLEWARES，添加自己编写的下载中间件类</li></ul><blockquote><pre><code>DOWNLOADER_MIDDLEWARES = {      &apos;mySpider.middlewares.MyCustomDownloaderMiddleware&apos;: 543,      &apos;mySpider.middlewares.RandomUserAgent&apos;: 1,      &apos;mySpider.middlewares.ProxyMiddleware&apos;: 100}</code></pre></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scrapy下载中间件是处于引擎(crawler.engine)和下载(crawler.engine.download())之间的一层组件，可以有多个下载中间件被加载运行；通常用来做异常处理和反爬处理。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架学习之Item Pipeline</title>
    <link href="http://yoursite.com/2018/06/12/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E4%B9%8BItem-Pipeline/"/>
    <id>http://yoursite.com/2018/06/12/Scrapy框架学习之Item-Pipeline/</id>
    <published>2018-06-12T06:56:51.000Z</published>
    <updated>2018-09-01T16:00:30.642Z</updated>
    
    <content type="html"><![CDATA[<p>当Item在Spider中被收集之后，它将会被传递到Item Pipeline，这些Item Pipeline组件按定义的顺序处理Item。<br><a id="more"></a></p><h1 id="Item-Pipeline"><a href="#Item-Pipeline" class="headerlink" title="Item Pipeline"></a>Item Pipeline</h1><p>每个Item Pipeline都是实现了简单方法的Python类，比如决定此Item是丢弃而存储。以下是item pipeline的一些典型应用：</p><ul><li>验证爬取的数据(检查item包含某些字段，比如说name字段)</li><li>查重(并丢弃)</li><li>将爬取结果保存到文件或者数据库中</li></ul><h2 id="编写item-pipeline"><a href="#编写item-pipeline" class="headerlink" title="编写item pipeline"></a>编写item pipeline</h2><p>item pipiline组件是一个独立的Python类，其中process_item()方法必须实现:  </p><p>常用方法举例：<br><img src="https://i.imgur.com/6Drjw3H.png" alt=""></p><p>我的实例：pipeline将所有(从所有’spider’中)爬取到的item，存储到一个独立地items.json 文件，每行包含一个序列化为’JSON’格式的’item’:<br><img src="https://i.imgur.com/2Ssebh0.png" alt=""></p><h2 id="启用一个Item-Pipeline组件"><a href="#启用一个Item-Pipeline组件" class="headerlink" title="启用一个Item Pipeline组件"></a>启用一个Item Pipeline组件</h2><p>为了启用Item Pipeline组件，必须将它的类添加到 settings.py文件ITEM_PIPELINES 配置。  </p><p><img src="https://i.imgur.com/sbi6xsH.png" alt=""></p><p>分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内（0-1000随意设置，数值越低，组件的优先级越高）。</p><h2 id="启动爬虫"><a href="#启动爬虫" class="headerlink" title="启动爬虫"></a>启动爬虫</h2><p>我们启动爬虫，就可以得到一个json文件。  </p><p><img src="https://i.imgur.com/sDXT1kj.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当Item在Spider中被收集之后，它将会被传递到Item Pipeline，这些Item Pipeline组件按定义的顺序处理Item。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架学习之Spider</title>
    <link href="http://yoursite.com/2018/06/10/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E4%B9%8BSpider/"/>
    <id>http://yoursite.com/2018/06/10/Scrapy框架学习之Spider/</id>
    <published>2018-06-10T02:29:45.000Z</published>
    <updated>2018-09-01T16:00:12.614Z</updated>
    
    <content type="html"><![CDATA[<p>Spider类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。 换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。<br><a id="more"></a>  </p><h1 id="Spider类"><a href="#Spider类" class="headerlink" title="Spider类"></a>Spider类</h1><p>class scrapy.Spider是最基本的类，所有编写的爬虫必须继承这个类。</p><p>主要用到的函数及调用顺序为：</p><p>__init__() : 初始化爬虫名字和start_urls列表</p><p>start_requests() 调用make_requests_from url():生成Requests对象交给Scrapy下载并返回response</p><p>parse() : 解析response，并返回Item或Requests（需指定回调函数）。Item传给Item pipline持久化 ， 而Requests交由Scrapy下载，并由指定的回调函数处理（默认parse())，一直进行循环，直到处理完所有的数据为止。</p><h2 id="Spider类的源码理解"><a href="#Spider类的源码理解" class="headerlink" title="Spider类的源码理解"></a>Spider类的源码理解</h2><p><img src="https://i.imgur.com/DK3qWhu.png" alt=""><br><img src="https://i.imgur.com/reWWTRh.png" alt=""></p><h2 id="主要属性和方法"><a href="#主要属性和方法" class="headerlink" title="主要属性和方法"></a>主要属性和方法</h2><ul><li>name</li></ul><p>定义spider名字的字符串。</p><p>例如，如果spider爬取 mywebsite.com ，该spider通常会被命名为 mywebsite</p><ul><li>allowed_domains</li></ul><p>包含了spider允许爬取的域名(domain)的列表，可选。</p><ul><li>start_urls</li></ul><p>初始URL元祖/列表。当没有制定特定的URL时，spider将从该列表中开始进行爬取。</p><ul><li>start_requests(self)</li></ul><p>该方法必须返回一个可迭代对象(iterable)。该对象包含了spider用于爬取（默认实现是使用 start_urls 的url）的第一个Request。</p><p>当spider启动爬取并且未指定start_urls时，该方法被调用。</p><ul><li>parse(self, response)</li></ul><p>当请求url返回网页没有指定回调函数时，默认的Request对象回调函数。用来处理网页返回的response，以及生成Item或者Request对象。</p><ul><li>log(self, message[, level, component])</li></ul><p>使用 scrapy.log.msg() 方法记录(log)message。 </p><p>下面附上自己写的spider类实例：   </p><p><strong>items.py文件：确定爬取内容</strong></p><p><img src="https://i.imgur.com/cN7SeIG.png" alt=""></p><p><strong>spider文件：parse()方法</strong>  </p><p><img src="https://i.imgur.com/D0PQrz4.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spider类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。 换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架学习之scrapy shell和selector</title>
    <link href="http://yoursite.com/2018/06/05/Scrapy%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E4%B9%8Bscrapy-shell%E5%92%8Cselector/"/>
    <id>http://yoursite.com/2018/06/05/Scrapy框架学习之scrapy-shell和selector/</id>
    <published>2018-06-05T12:10:32.000Z</published>
    <updated>2018-09-01T15:59:47.655Z</updated>
    
    <content type="html"><![CDATA[<p>Scrapy终端是一个交互终端，我们可以在未启动spider的情况下尝试及调试代码，也可以用来测试XPath或CSS表达式，查看他们的工作方式，方便我们爬取的网页中提取的数据。<br><a id="more"></a>  </p><h1 id="Scrapy-Shell"><a href="#Scrapy-Shell" class="headerlink" title="Scrapy Shell"></a>Scrapy Shell</h1><p>进入项目的根目录，执行下列命令来启动shell:</p><blockquote><pre><code>scrapy shell &quot;http://www.itcast.cn/channel/teacher.shtml&quot;</code></pre></blockquote><p><img src="https://i.imgur.com/F45RBJ9.png" alt=""></p><p>Scrapy Shell根据下载的页面会自动创建一些方便使用的对象，例如 Response 对象，以及 Selector 对象 (对HTML及XML内容)。</p><p>当shell载入后，将得到一个包含response数据的本地 response 变量，输入 response.body将输出response的包体，输出 response.headers 可以看到response的报头。</p><p>输入 response.selector 时， 将获取到一个response 初始化的类 Selector 的对象，此时可以通过使用 response.selector.xpath()或response.selector.css() 来对 response 进行查询。<br>Xpath对象：<br><img src="https://i.imgur.com/z6EN1U1.png" alt=""></p><h1 id="Selectors选择器"><a href="#Selectors选择器" class="headerlink" title="Selectors选择器"></a>Selectors选择器</h1><p>Scrapy Selectors 内置 XPath 和 CSS Selector 表达式机制；Selector有四个基本的方法，最常用的还是xpath:</p><ul><li>xpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表</li><li>extract(): 序列化该节点为Unicode字符串并返回list</li><li>extract_first(): 取出上述节点中的第一个元素，可以传入默认值。</li><li>css(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表，语法同 BeautifulSoup4 </li><li>re(): 根据传入的正则表达式对数据进行提取，返回Unicode字符串list列表  </li></ul><p>XPath表达式的一些例子及对应的含义:</p><blockquote><pre><code>/html/head/title: 选择&lt;HTML&gt;文档中 &lt;head&gt; 标签内的 &lt;title&gt; 元素/html/head/title/text(): 选择上面提到的 &lt;title&gt; 元素的文字//td: 选择所有的 &lt;td&gt; 元素//div[@class=&quot;mine&quot;]: 选择所有具有 class=&quot;mine&quot; 属性的 div 元素</code></pre></blockquote><p>css表达式的一些例子及对应的含义:  </p><blockquote><pre><code>a[href=&quot;image.html&quot;] img: href熟悉为image.html的a节点下的img节点a[href=&quot;image.html&quot;]::text: 获取href为image.html属性a节点下的文本a[href=&quot;image.html&quot;] img::attr(src): 上述img节点下src的属性 </code></pre></blockquote><p>另外，Selector还支持正则re，可以与xpath， css搭配使用</p><blockquote><pre><code>response.xpath(&apos;//a/text()&apos;).re(&apos;name:\s(.*)&apos;) 注意response不能直接调用re，可以先使用xpath和css匹配后，在使用re</code></pre></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scrapy终端是一个交互终端，我们可以在未启动spider的情况下尝试及调试代码，也可以用来测试XPath或CSS表达式，查看他们的工作方式，方便我们爬取的网页中提取的数据。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架学习之入门案例</title>
    <link href="http://yoursite.com/2018/06/03/Scrapy%E6%A1%86%E6%9E%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2018/06/03/Scrapy框架基础学习/</id>
    <published>2018-06-03T12:50:37.000Z</published>
    <updated>2018-09-01T15:59:24.063Z</updated>
    
    <content type="html"><![CDATA[<p>Scrapy是一个用纯Python实现，为了爬取网站数据、提取结构性数据而编写的应用框架；我们只需要定制开发几个模块就可以轻松的实现一个爬虫，用来抓取网页内容以及各种图片，非常之方便。Scrapy 使用了Twisted多线程异步网络框架来处理网络通讯，可以加快我们的下载速度，不用自己去实现异步框架，并且包含了各种中间件接口，可以灵活的完成各种需求。<br><a id="more"></a>  </p><h1 id="Scrapy架构图"><a href="#Scrapy架构图" class="headerlink" title="Scrapy架构图"></a>Scrapy架构图</h1><p>——————————–<img src="https://i.imgur.com/vNUUr2S.png" alt=""></p><p>它可以分为以下几个部分：  </p><ul><li><strong>Scrapy Engine(引擎):</strong> 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。</li><li><strong>Scheduler(调度器):</strong> 它负责接受<code>引擎</code>发送过来的Request请求，并按照一定的方式进行整理排列，入队，当<code>引擎</code>需要时，交还给<code>引擎</code>。</li><li><strong>Downloader（下载器)：</strong>负责下载<code>Scrapy Engine(引擎)</code>发送的所有Requests请求，并将其获取到的Responses交还给<code>Scrapy Engine(引擎)</code>，由<code>引擎</code>交给<code>Spider</code>来处理。</li><li><strong>spiders（爬虫）：</strong>它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给<code>引擎</code>，再次进入<code>Scheduler(调度器)</code>。</li><li><strong>Item Pipeline(管道)：</strong>它负责处理<code>Spider</code>中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方.</li><li><strong>Downloader Middlewares（下载中间件）：</strong>你可以当作是一个可以自定义扩展下载功能的组件。</li><li><strong>Spider Middlewares（Spider中间件）：</strong>你可以理解为是一个可以自定扩展和操作<code>引擎</code>和<code>Spider</code>中间<code>通信</code>的功能组件（比如进入<code>Spider</code>的Responses;和从<code>Spider</code>出去的Requests）。</li></ul><h1 id="Scrapy的运作流程"><a href="#Scrapy的运作流程" class="headerlink" title="Scrapy的运作流程"></a>Scrapy的运作流程</h1><p>代码写好，程序开始运行…</p><ol><li><code>引擎</code>：Hi！<code>Spider</code>, 你要处理哪一个网站？</li><li><code>Spider</code>：老大要我处理xxxx.com。</li><li><code>引擎</code>：你把第一个需要处理的URL给我吧。</li><li><code>Spider</code>：给你，第一个URL是xxxxxxx.com。</li><li><code>引擎</code>：Hi！<code>调度器</code>，我这有request请求你帮我排序入队一下。</li><li><code>调度器</code>：好的，正在处理你等一下。</li><li><code>引擎</code>：Hi！<code>调度器</code>，把你处理好的request请求给我。</li><li><code>调度器</code>：给你，这是我处理好的request。</li><li><code>引擎</code>：Hi！下载器，你按照老大的<code>下载中间件</code>的设置帮我下载一下这个request请求</li><li><code>下载器</code>：好的！给你，这是下载好的东西。（如果失败：sorry，这个request下载失败了。然后<code>引擎</code>告诉<code>调度器</code>，这个request下载失败了，你记录一下，我们待会儿再下载）</li><li><code>引擎</code>：Hi！<code>Spider</code>，这是下载好的东西，并且已经按照老大的<code>下载中间件</code>处理过了，你自己处理一下（注意！这儿responses默认是交给<code>def parse()</code>这个函数处理的）</li><li><code>Spider</code>：（处理完毕数据之后对于需要跟进的URL），Hi！<code>引擎</code>，我这里有两个结果，这个是我需要跟进的URL，还有这个是我获取到的Item数据。</li><li><code>引擎</code>：Hi ！<code>管道</code>我这儿有个item你帮我处理一下！<code>调度器</code>！这是需要跟进URL你帮我处理下。然后从第四步开始循环，直到获取完老大需要全部信息。</li><li><code>管道`</code>调度器`：好的，现在就做！</li></ol><p><strong>注意！只有当</strong> <code>调度器</code> <strong>中不存在任何request了，整个程序才会停止，（也就是说，对于下载失败的URL，Scrapy也会重新下载。）</strong></p><h1 id="Scrapy-爬虫步骤："><a href="#Scrapy-爬虫步骤：" class="headerlink" title="Scrapy 爬虫步骤："></a>Scrapy 爬虫步骤：</h1><ul><li>新建项目 (scrapy startproject xxx)：在终端中执行命令，新建一个爬虫项目，会创建一个xxx文件夹</li><li>明确目标 （编写items.py）：明确你想要抓取的目标，定义爬取的数据结构</li><li>制作爬虫 （spiders/xxspider.py）：制作爬虫开始爬取网页</li><li>存储内容 （pipelines.py）：设计管道存储爬取内容</li></ul><p>下面来就来讲一个简单的案例来熟悉以下scrapy的操作：  </p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ul><li>创建一个Scrapy项目</li><li>定义提取的结构化数据(Item)</li><li>编写爬取网站的 Spider 并提取出结构化数据(Item)</li><li>编写 Item Pipelines 来存储提取到的Item(即结构化数据)</li></ul><h2 id="一-新建项目-scrapy-startproject-studyScrapy"><a href="#一-新建项目-scrapy-startproject-studyScrapy" class="headerlink" title="一. 新建项目(scrapy startproject studyScrapy)"></a>一. 新建项目(scrapy startproject studyScrapy)</h2><ul><li><p>在开始爬取之前，必须创建一个新的Scrapy项目。进入自定义的项目目录中，比如我的是<strong>F:\spider_projects</strong> 运行下列命令：</p><blockquote><pre><code>scrapy startproject studyScrapy</code></pre></blockquote></li><li><p>其中， studyScrapy 为项目名称，可以看到将会创建一个 studyScrapy 的文件夹，目录结构大致如下：<br><img src="https://i.imgur.com/blWwfGE.png" alt=""></p></li></ul><p>下面来简单介绍一下各个主要文件的作用：  </p><ul><li><p>studyScrapy/spiders/ ：存储爬虫代码的文件夹</p></li><li><p>studyScrapy/ ：该项目的Python模块，需要从这里引入；即import</p></li><li><p>studyScrapy/items.py ：定义项目的目标文件，即需要爬取的数据</p></li><li><p>studyScrapy/middlewares.py： 定义爬取时的中间件</p></li><li><p>studyScrapy/pipelines.py ：项目的管道文件，</p></li><li><p>studyScrapy/settings.py ：项目的设置文件</p></li><li><p>scrapy.cfg ：scrapy部署时的配置文件</p></li></ul><h2 id="二-创建QuotesSpider类-studyScrapy-spiders"><a href="#二-创建QuotesSpider类-studyScrapy-spiders" class="headerlink" title="二. 创建QuotesSpider类(studyScrapy/spiders)"></a>二. 创建QuotesSpider类(studyScrapy/spiders)</h2><p>我们打算抓取谚语网：<a href="http://quotes.toscrape.com" target="_blank" rel="noopener">http://quotes.toscrape.com</a> 。使用命令行生成一个Spider：  </p><blockquote><pre><code>scrapy genspider quotes quotes.toscrape.com</code></pre></blockquote><p>cd 到studyScrapy文件目录下，执行上述语句，然后我们在spiders文件下发现一个quotes.py文件。</p><p><img src="https://i.imgur.com/n2EJ8hO.png" alt=""></p><p>这个类必须继承scrapy.Spider， 这里有三个属性，和一个方法。</p><ul><li><code>name = &quot;&quot;</code>：这个爬虫的识别名称，必须是唯一的，在不同的爬虫必须定义不同的名字。</li><li><code>allow_domains = []</code>是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页，不存在的URL会被忽略。</li><li><code>start_urls = []</code>：爬取的URL的列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成。</li><li><code>parse(self, response)</code>：解析的方法，每个初始URL完成下载后将被调用，调用的时候传入从每一个URL传回的Response对象来作为唯一参数，主要作用如下：<ol><li>负责解析返回的网页数据(response.body)，提取结构化数据(生成item)</li><li>生成需要下一页的URL请求；</li><li></li></ol></li></ul><h2 id="三-创建Item类"><a href="#三-创建Item类" class="headerlink" title="三. 创建Item类"></a>三. 创建Item类</h2><ol><li>打开studyScrapy目录下的items.py</li><li>Item 定义结构化数据字段，用来保存爬取到的数据，有点像Python中的dict，但是提供了一些额外的保护机制来减少错误。</li><li>创建Item需要继承scrapy.Item类，并且定义类型为scrapy.Field的字段。</li><li>接下来，创建一个QuoteItem类，定义我们要获取的内容字段，text， author， tags。</li></ol><p>此时，将items.py修改如下：</p><p><img src="https://i.imgur.com/cOhEe1R.png" alt=""></p><h2 id="四-修改parse-方法，解析Response"><a href="#四-修改parse-方法，解析Response" class="headerlink" title="四. 修改parse()方法，解析Response"></a>四. 修改parse()方法，解析Response</h2><p>parse方法中的参数response是start_urls里面的链接爬取的结果；所以我们可直接对response变量中的内容进行解析。<br>首先我们看一下网页结构，每一页都有class为quote的区块，每个区块里包含了text，author，tags，我们先找出所有quote，在提取出里面的信息。<br><img src="https://i.imgur.com/yBzpX75.png" alt=""></p><p>这里需要使用上面创建的Item，可以将它理解为一个字典，不过在声明的时候需要实例化。然后再将图中的结果解析出来赋值给Item中的每个字段，最后使用yield返回即可。</p><p><img src="https://i.imgur.com/6PrClK0.png" alt=""></p><p>这样就获取了初始页面的内容了。 这里为什么用yield而不用return呢？这里要从parse()方法的工作机制来说：</p><blockquote><pre><code>因为使用的是yield，parse函数将会被当做一个生成器使用。scrapy会逐一获取parse方法中生成的结果，并判断该结果是一个什么样的类型；如果是request则加入爬取队列，如果是item类型则使用pipeline处理，其他类型则返回错误信息。scrapy取到第一部分的request不会立马就去发送这个request，只是把这个request放到队列里，然后接着从生成器里获取；取完第一部分的request，然后再获取第二部分的item，取到item了，就会放到对应的pipeline里处理；parse()方法作为回调函数(callback)赋值给了Request，指定parse()方法来处理这些请求 scrapy.Request(url, callback=self.parse)Request对象经过调度，执行生成 scrapy.http.response()的响应对象，并送回给parse()方法，直到调度器中没有Request（递归的思路）取完之后，parse()工作结束，引擎再根据队列和pipelines中的内容去执行相应的操作；程序在取得各个页面的items前，会先处理完之前所有的request队列里的请求，然后再提取items。</code></pre></blockquote><h2 id="五-处理后续的Request"><a href="#五-处理后续的Request" class="headerlink" title="五. 处理后续的Request"></a>五. 处理后续的Request</h2><p>上面知识完成了首页的抓取，那么下一页怎么来完成了，以前我写的爬虫都是分析了url请求的规律，然后构造请求再取获取页面内容。这里我们需要从当前页面找到信息来生成下一个请求，然后再下一个请求中寻找信息构造下下一个请求。就这样循环迭代，实现整个网站的爬取。</p><p><img src="https://i.imgur.com/C0tjdXD.png" alt=""></p><p>将网页下拉到底部，查看下一页按钮，链接为/page/2/， 完整的URL就是 <a href="http://quotes.toscrape.com/page/2/" target="_blank" rel="noopener">http://quotes.toscrape.com/page/2/</a><br>我们现在就可以构造RUL请求了，这里又要用到一个新函数<strong>scrapy.Requst</strong>。它有两个参数—-url， callback。  </p><blockquote><pre><code>url: 新的请求链接callback： 回调函数，当指定了该回调函数的请求完成后，获取到响应，引擎会将该响应作为参数传递给这个回调函数。 回调函数进行解析或者生成下一个请求，这里的回调函数就是上面的parse()函数。</code></pre></blockquote><p>这里由于parse()就是解析text，author，tags的方法，并且下一页面的结构与刚才解析的页面是一样的，所以我们可以再次使用parse来解析页面。接下来我们先利用选择器得到下一页的URL，生成新请求；在parse()方法后面追加如下代码即可：  </p><p><img src="https://i.imgur.com/BvG1S5X.png" alt=""></p><h2 id="六-运行"><a href="#六-运行" class="headerlink" title="六. 运行"></a>六. 运行</h2><p>然后，进入项目所在目录，直接运行 <strong>scrapy crawl quotes</strong>，Scrapy就开始进行抓取啦，当然我们还没设置中间件Middlewares和Piplines里面的内容，后面写其他项目会细细说明的。<br><img src="https://i.imgur.com/pI4YD37.png" alt=""></p><p>另外，可以在项目的根目录下新建main.py文件,在爬虫代码中设置好断点，用于调试 </p><blockquote><pre><code>from scrapy import cmdlinecmdline.execute(&apos;scrapy crawl quotes&apos;.split())</code></pre></blockquote><h2 id="七-保存结果-Item-Pipeline"><a href="#七-保存结果-Item-Pipeline" class="headerlink" title="七.保存结果(Item Pipeline)"></a>七.保存结果(Item Pipeline)</h2><p><strong>保存到文件</strong> </p><p>针对小项目数据，我们可以直接保存成文件格式。</p><p>scrapy保存信息的最简单的方法主要有以下几种，-o 输出指定格式的文件，命令如下：</p><ul><li><p>scrapy crawl quotes -o quotes.json # 输出json文件</p></li><li><p>scrapy crawl quotes -o quotes.csv  # 输出csv文件</p></li><li><p>scrapy crawl quotes -o quotes.xml  # 输出xml格式文件</p></li><li><p>scrapy crawl quotes -o <a href="ftp://user:passwd@ftp.xxxx.com/path/to/quotes.csv" target="_blank" rel="noopener">ftp://user:passwd@ftp.xxxx.com/path/to/quotes.csv</a>  </p></li></ul><p>同时还支持ftp， s3等远程输出。</p><p><strong>保存到数据库</strong><br>如果想输出到数据库中的话，以mongoDB为例，我们可以定义<strong>项目管道Item Pipelin</strong>e进行操作。<br>当Item在Spider中被收集之后，它将会被传递到Item Pipeline，这些Item Pipeline组件按定义的顺序处理Item。<br>每个Item Pipeline都是实现了简单方法的Python类，比如决定此Item是丢弃而存储。以下是item pipeline的常用处理操作：</p><blockquote><pre><code>1. 清理HTML数据2. 验证爬虫数据，检查爬虫字段3. 查重并丢弃重复内容4. 将爬取结果保存到数据库中</code></pre></blockquote><p>编写item pipeline很简单，item pipiline组件是一个独立的Python类，其中process_item()方法必须实现。<br>下面是一些方法示例：<br><img src="https://i.imgur.com/AmBVtAr.png" alt=""></p><p>下面为实际的代码：<br><img src="https://i.imgur.com/ITitSws.png" alt=""><br><img src="https://i.imgur.com/ViRiqt5.png" alt=""></p><p>这样我们便完成了一个入门案例了，正式踏入Scrapy框架的学习之路。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scrapy是一个用纯Python实现，为了爬取网站数据、提取结构性数据而编写的应用框架；我们只需要定制开发几个模块就可以轻松的实现一个爬虫，用来抓取网页内容以及各种图片，非常之方便。Scrapy 使用了Twisted多线程异步网络框架来处理网络通讯，可以加快我们的下载速度，不用自己去实现异步框架，并且包含了各种中间件接口，可以灵活的完成各种需求。&lt;br&gt;
    
    </summary>
    
      <category term="scrapy" scheme="http://yoursite.com/categories/scrapy/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Python下的设计模式一</title>
    <link href="http://yoursite.com/2018/05/22/Python%E4%B8%8B%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B8%80/"/>
    <id>http://yoursite.com/2018/05/22/Python下的设计模式一/</id>
    <published>2018-05-22T07:36:19.000Z</published>
    <updated>2018-09-01T16:10:46.876Z</updated>
    
    <content type="html"><![CDATA[<p>有时候写代码总想着如果有现成模板的话写起来不就方便很多吗？于是我开始在论坛和python相关的网站上搜索，看到python下的设计模式，于是乎将其转化为自己的笔记。这篇主要是归纳的创建型设计模式。<br><a id="more"></a></p><h2 id="什么是设计模式"><a href="#什么是设计模式" class="headerlink" title="什么是设计模式"></a>什么是设计模式</h2><p>设计模式是经过总结、优化的，对我们经常会碰到的一些编程问题的可重用解决方案。一个设计模式并不像一个类或一个库那样能够直接作用于我们的代码。反之，设计模式更为高级，它是一种必须在特定情形下实现的一种方法模板。设计模式不会绑定具体的编程语言。一个好的设计模式应该能够用大部分编程语言实现(如果做不到全部的话，具体取决于语言特性)。最为重要的是，设计模式也是一把双刃剑，如果设计模式被用在不恰当的情形下将会造成灾难，进而带来无穷的麻烦。然而如果设计模式在正确的时间被用在正确地地方，它将是你的救星。</p><p>起初，你会认为“模式”就是为了解决一类特定问题而特别想出来的明智之举。说的没错，看起来的确是通过很多人一起工作，从不同的角度看待问题进而形成的一个最通用、最灵活的解决方案。也许这些问题你曾经见过或是曾经解决过，但是你的解决方案很可能没有模式这么完备。</p><p>虽然被称为“设计模式”，但是它们同“设计“领域并非紧密联系。设计模式同传统意义上的分析、设计与实现不同，事实上设计模式将一个完整的理念根植于程序中，所以它可能出现在分析阶段或是更高层的设计阶段。很有趣的是因之创建为设计模式的具体体现是程序代码，因此可能会让你认为它不会在具体实现阶段之前出现(事实上在进入具体实现阶段之前你都没有意识到正在使用具体的设计模式)。</p><p>可以通过程序设计的基本概念来理解模式：增加一个抽象层。抽象一个事物就是隔离任何具体细节，这么做的目的是为了将那些不变的核心部分从其他细节中分离出来。当你发现你程序中的某些部分经常因为某些原因改动，而你不想让这些改动的部分引发其他部分的改动，这时候你就需要思考那些不会变动的设计方法了。这么做不仅会使代码可维护性更高，而且会让代码更易于理解，从而降低开发成本。</p><p>这里列举了三种最基本的设计模式：</p><ul><li>创建模式，提供实例化的方法，为适合的状况提供相应的对象创建方法。</li><li>结构化模式，通常用来处理实体之间的关系，使得这些实体能够更好地协同工作。</li><li>行为模式，用于在不同的实体建进行通信，为实体之间的通信提供更容易，更灵活的通信方法。</li></ul><p><strong>创建型</strong></p><ol><li><p>Factory Method（工厂方法）</p></li><li><p>Abstract Factory（抽象工厂）</p></li><li><p>Builder（建造者）</p></li><li><p>Prototype（原型）</p></li><li><p>Singleton（单例）</p></li></ol><p><strong>结构型</strong></p><ol start="6"><li><p>Adapter Class/Object（适配器）</p></li><li><p>Bridge（桥接）</p></li><li><p>Composite（组合）</p></li><li><p>Decorator（装饰）</p></li><li><p>Facade（外观）</p></li><li><p>Flyweight（享元）</p></li><li><p>Proxy（代理）</p></li></ol><p><strong>行为型</strong></p><ol start="13"><li><p>Interpreter（解释器）</p></li><li><p>Template Method（模板方法）</p></li><li><p>Chain of Responsibility（责任链）</p></li><li><p>Command（命令）</p></li><li><p>Iterator（迭代器）</p></li><li><p>Mediator（中介者）</p></li><li><p>Memento（备忘录）</p></li><li><p>Observer（观察者）</p></li><li><p>State（状态）</p></li><li><p>Strategy（策略）</p></li><li><p>Visitor（访问者）</p></li></ol><h2 id="创建型"><a href="#创建型" class="headerlink" title="创建型"></a>创建型</h2><p><strong>Factory Method（工厂方法）</strong></p><p><img src="https://i.imgur.com/QHdivGQ.png" alt=""></p><p>意图：</p><p>定义一个用于创建对象的接口，让子类决定实例化哪一个类。Factory Method 使一个类的实例化延迟到其子类。</p><p>适用性：</p><blockquote><pre><code>当一个类不知道它所必须创建的对象的类的时候。当一个类希望由它的子类来指定它所创建的对象的时候。当类将创建对象的职责委托给多个帮助子类中的某一个，并且你希望将哪一个帮助子类是代理者这一信息局部化的时候。</code></pre></blockquote><p>实现：</p><pre><code>class ChinaGetter:    &quot;&quot;&quot;A simple localizer a la gettext&quot;&quot;&quot;    def __init__(self):        self.trans = dict(dog=u&quot;小狗&quot;, cat=u&quot;小猫&quot;)    def get(self, msgid):        &quot;&quot;&quot;We&apos;ll punt if we don&apos;t have a translation&quot;&quot;&quot;        try:            return self.trans[msgid]        except KeyError:            return str(msgid)class EnglishGetter:    &quot;&quot;&quot;Simply echoes the msg ids&quot;&quot;&quot;    def get(self, msgid):        return str(msgid)def get_localizer(language=&quot;English&quot;):    &quot;&quot;&quot;The factory method&quot;&quot;&quot;    languages = dict(English=EnglishGetter, China=ChinaGetter)    return languages[language]()# Create our localizerse, g = get_localizer(&quot;English&quot;), get_localizer(&quot;China&quot;)# Localize some textfor msgid in &quot;dog parrot cat bear&quot;.split():    print(e.get(msgid), g.get(msgid))</code></pre><p><strong>Abstract Factory（抽象工厂）</strong></p><p><img src="https://i.imgur.com/LAVNQLD.png" alt=""></p><p>意图：</p><p>提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类。 </p><p>适用性：</p><blockquote><pre><code>一个系统要独立于它的产品的创建、组合和表示时。一个系统要由多个产品系列中的一个来配置时。当你要强调一系列相关的产品对象的设计以便进行联合使用时。当你提供一个产品类库，而只想显示它们的接口而不是实现时。</code></pre></blockquote><p>实现：  </p><pre><code>import randomclass PetShop:    &quot;&quot;&quot;A pet shop&quot;&quot;&quot;    def __init__(self, animal_factory=None):        &quot;&quot;&quot;pet_factory is our abstract factory.        We can set it at will.&quot;&quot;&quot;        self.pet_factory = animal_factory    def show_pet(self):        &quot;&quot;&quot;Creates and shows a pet using the        abstract factory&quot;&quot;&quot;        pet = self.pet_factory.get_pet()        print(&quot;This is a lovely&quot;, str(pet))        print(&quot;It says&quot;, pet.speak())        print(&quot;It eats&quot;, self.pet_factory.get_food())# Stuff that our factory makesclass Dog:    def speak(self):        return &quot;woof&quot;    def __str__(self):        return &quot;Dog&quot;class Cat:    def speak(self):        return &quot;meow&quot;    def __str__(self):        return &quot;Cat&quot;# Factory classesclass DogFactory:    def get_pet(self):        return Dog()    def get_food(self):        return &quot;dog food&quot;class CatFactory:    def get_pet(self):        return Cat()    def get_food(self):        return &quot;cat food&quot;# Create the proper familydef get_factory():    &quot;&quot;&quot;Let&apos;s be dynamic!&quot;&quot;&quot;    return random.choice([DogFactory, CatFactory])()# Show pets with various factoriesif __name__ == &quot;__main__&quot;:    shop = PetShop()    for i in range(3):        shop.pet_factory = get_factory()        shop.show_pet()        print(&quot;=&quot; * 20)</code></pre><p><strong>Builder（建造者)</strong></p><p>意图：</p><p>将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。</p><p>适用性：</p><blockquote><pre><code>当创建复杂对象的算法应该独立于该对象的组成部分以及它们的装配方式时。当构造过程必须允许被构造的对象有不同的表示时。</code></pre></blockquote><p><img src="https://i.imgur.com/c6Z7lSl.png" alt=""><br>实现：  </p><pre><code># Directorclass Director(object):    def __init__(self):        self.builder = None    def construct_building(self):        self.builder.new_building()        self.builder.build_floor()        self.builder.build_size()    def get_building(self):        return self.builder.building# Abstract Builderclass Builder(object):    def __init__(self):        self.building = None    def new_building(self):        self.building = Building()# Concrete Builderclass BuilderHouse(Builder):    def build_floor(self):        self.building.floor = &apos;One&apos;    def build_size(self):        self.building.size = &apos;Big&apos;class BuilderFlat(Builder):    def build_floor(self):        self.building.floor = &apos;More than One&apos;    def build_size(self):        self.building.size = &apos;Small&apos;# Productclass Building(object):    def __init__(self):        self.floor = None        self.size = None    def __repr__(self):        return &apos;Floor: %s | Size: %s&apos; % (self.floor, self.size)# Clientif __name__ == &quot;__main__&quot;:    director = Director()    director.builder = BuilderHouse()    director.construct_building()    building = director.get_building()    print(building)    director.builder = BuilderFlat()    director.construct_building()    building = director.get_building()    print(building)</code></pre><p><strong>Prototype（原型）</strong></p><p>意图：</p><p>用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。</p><p>适用性：</p><p>当要实例化的类是在运行时刻指定时，例如，通过动态装载；或者为了避免创建一个与产品类层次平行的工厂类层次时；或者当一个类的实例只能有几个不同状态组合中的一种时。建立相应数目的原型并克隆它们可能比每次用合适的状态手工实例化该类更方便一些。  </p><p><img src="https://i.imgur.com/9Adtnsm.png" alt=""></p><pre><code>import copyclass Prototype:    def __init__(self):        self._objects = {}    def register_object(self, name, obj):        &quot;&quot;&quot;Register an object&quot;&quot;&quot;        self._objects[name] = obj    def unregister_object(self, name):        &quot;&quot;&quot;Unregister an object&quot;&quot;&quot;        del self._objects[name]    def clone(self, name, **attr):        &quot;&quot;&quot;Clone a registered object and update inner attributes dictionary&quot;&quot;&quot;        obj = copy.deepcopy(self._objects.get(name))        obj.__dict__.update(attr)        return objdef main():    class A:        def __str__(self):            return &quot;I am A&quot;    a = A()    prototype = Prototype()    prototype.register_object(&apos;a&apos;, a)    b = prototype.clone(&apos;a&apos;, a=1, b=2, c=3)    print(a)    print(b.a, b.b, b.c)if __name__ == &apos;__main__&apos;:    main()</code></pre><p><strong>Singleton（单例）</strong></p><p>意图：</p><p>保证一个类仅有一个实例，并提供一个访问它的全局访问点。</p><p>适用性：</p><blockquote><pre><code>当类只能有一个实例而且客户可以从一个众所周知的访问点访问它时。当这个唯一实例应该是通过子类化可扩展的，并且客户应该无需更改代码就能使用一个扩展的实例时。  </code></pre></blockquote><p><img src="https://i.imgur.com/eTPN7Mc.png" alt=""></p><p>示例：</p><pre><code>import threadingclass Singleton(object):    _instance_lock = threading.Lock()    def __init__(self):        pass    def __new__(cls, *args, **kwargs):        if not hasattr(Singleton, &quot;_instance&quot;):            with Singleton._instance_lock:                if not hasattr(Singleton, &quot;_instance&quot;):                    Singleton._instance = object.__new__(cls)        return Singleton._instanceobj1 = Singleton()obj2 = Singleton()print(obj1,obj2)def task(arg):    obj = Singleton()    print(obj)for i in range(10):    t = threading.Thread(target=task,args=[i,])    t.start()</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;有时候写代码总想着如果有现成模板的话写起来不就方便很多吗？于是我开始在论坛和python相关的网站上搜索，看到python下的设计模式，于是乎将其转化为自己的笔记。这篇主要是归纳的创建型设计模式。&lt;br&gt;
    
    </summary>
    
      <category term="设计模式" scheme="http://yoursite.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    
    
      <category term="设计模式" scheme="http://yoursite.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>python多任务之协程</title>
    <link href="http://yoursite.com/2018/05/20/python%E5%A4%9A%E4%BB%BB%E5%8A%A1%E4%B9%8B%E5%8D%8F%E7%A8%8B/"/>
    <id>http://yoursite.com/2018/05/20/python多任务之协程/</id>
    <published>2018-05-20T12:36:02.000Z</published>
    <updated>2018-09-01T15:58:28.658Z</updated>
    
    <content type="html"><![CDATA[<p>协程是啥呢？协程其实可以认为是比线程更小的执行单元。 为啥说他是一个执行单元，因为他自带CPU上下文。这样只要在合适的时机，我们可以把一个协程 切换到另一个协程。 只要这个过程中保存或恢复 CPU上下文那么程序还是可以运行的。<br><a id="more"></a><br>通俗的理解：在一个线程中的某个函数，可以在任何地方保存当前函数的一些临时变量等信息，然后切换到另外一个函数中执行(注意不是通过调用函数的方式做到的)并且切换的次数以及什么时候再切换到原来的函数都由<strong>开发者自己确定</strong>。  </p><h2 id="协程和线程差异"><a href="#协程和线程差异" class="headerlink" title="协程和线程差异"></a>协程和线程差异</h2><p>从刚才的了解，协程看起来比线程差不多。其实不然, 线程切换从系统层面远不止保存和恢复 CPU上下文这么简单。操作系统为了程序运行的高效性每个线程都有自己缓存Cache等等数据，操作系统还会帮你做这些数据的恢复操作。所以线程的切换非常耗性能。但是协程的切换只是单纯的操作CPU的上下文，所以一秒钟切换个上百万次系统都抗的住。但是协程有一个问题，就是系统并不感知，所以操作系统不会帮你做切换。那么谁来帮你做切换？让需要执行的协程更多的获得CPU时间才是问题的关键。  </p><p>目前的协程框架一般都是设计成 1:N 模式。所谓 1:N 就是一个线程作为一个容器里面放置多个协程。 那么谁来适时的切换这些协程？<br>答案是有协程自己主动让出CPU，也就是每个协程池里面有一个调度器， 这个调度器是被动调度的。意思就是他不会主动调度。而且当一个协程发现自己执行不下去了(比如异步等待网络的数据回来，但是当前还没有数据到)， 这个时候就可以由这个协程通知调度器，这个时候执行到调度器的代码，调度器根据事先设计好的调度算法找到当前最需要CPU的协程。 切换这个协程的CPU上下文把CPU的运行权交个这个协程，直到这个协程出现执行不下去需要等等的情况，或者它调用主动让出CPU的API之类，触发下一次调度。注意：假设这个线程中有一个协程是CPU密集型的他没有IO操作， 也就是自己不会主动触发调度器调度的过程，那么就会出现其他协程得不到执行的情况， 所以这种情况下需要自己避免。 </p><h2 id="协程的好处"><a href="#协程的好处" class="headerlink" title="协程的好处"></a>协程的好处</h2><p>在IO密集型的程序中由于IO操作远远慢于CPU的操作，所以往往需要CPU去等IO操作。 同步IO下系统需要切换线程，让操作系统可以在IO过程中执行其他的东西。 这样虽然代码是符合人类的思维习惯但是由于大量的线程切换带来了大量的性能的浪费，尤其是IO密集型的程序。</p><p>所以人们发明了异步IO。就是当数据到达的时候触发我的回调。来减少线程切换带来性能损失。 但是这样的坏处也是很大的，主要的坏处就是操作被 “分片” 了，代码写的不是 “一气呵成” 这种。 而是每次来段数据就要判断 数据够不够处理哇，够处理就处理吧，不够处理就在等等吧。这样代码的可读性很低，其实也不符合人类的习惯。</p><p>但是协程可以很好解决这个问题。比如 把一个IO操作 写成一个协程。当触发IO操作的时候就自动让出CPU给其他协程。要知道协程的切换很轻的。  </p><p>协程通过这种对异步IO的封装 既保留了性能也保证了代码的容易编写和可读性。在高IO密集型的程序下很好。但是高CPU密集型的程序下没啥好处(需要占用大量CPU的情况下，一般采用多线程)。</p><p><strong>协程的实现其实是采用了生成器的原理</strong><br>下面看一个协程实现的多任务打印实例：<br><img src="https://i.imgur.com/rBnmrpo.png" alt="">  </p><h2 id="协程-greenlet"><a href="#协程-greenlet" class="headerlink" title="协程-greenlet"></a>协程-greenlet</h2><p>为了更好使用协程来完成多任务，python中的greenlet模块对其封装，从而使得切换任务变的更加简单<br><img src="https://i.imgur.com/2lf5TUK.png" alt=""></p><h2 id="协程-gevent"><a href="#协程-gevent" class="headerlink" title="协程-gevent"></a>协程-gevent</h2><p>greenlet已经实现了协程，但是这个还的人工切换，是不是觉得太麻烦了，不要捉急，python还有一个比greenlet更强大的并且能够自动切换任务的模块gevent。</p><p>其原理是当一个greenlet遇到IO(指的是input output 输入输出，比如网络、文件操作等)操作时，比如访问网络，就自动切换到其他的greenlet，等到IO操作完成，再在适当的时候切换回来继续执行。<br>由于IO操作非常耗时，经常使程序处于等待状态，有了gevent为我们自动切换协程，就保证总有greenlet在运行，而不是等待IO。<br><strong>gevent的使用</strong><br><img src="https://i.imgur.com/OrGQERV.png" alt=""></p><p><strong>gevent切换执行</strong><br><img src="https://i.imgur.com/GlIdZ4U.png" alt=""></p><p><strong>gevent执行并发任务</strong><br>实际代码里，不会用gevent.sleep()去切换协程，而是在执行到IO操作时，gevent自动切换。<br><img src="https://i.imgur.com/tXaURig.png" alt=""></p><p>从上能够看到是先发送的获取baidu的相关信息，然后依次是sina、runoob，但是收到数据的先后顺序不一定与发送顺序相同，这也就体现出了异步，即不确定什么时候会收到数据，顺序不一定。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;协程是啥呢？协程其实可以认为是比线程更小的执行单元。 为啥说他是一个执行单元，因为他自带CPU上下文。这样只要在合适的时机，我们可以把一个协程 切换到另一个协程。 只要这个过程中保存或恢复 CPU上下文那么程序还是可以运行的。&lt;br&gt;
    
    </summary>
    
      <category term="系统编程" scheme="http://yoursite.com/categories/%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B/"/>
    
    
      <category term="协程" scheme="http://yoursite.com/tags/%E5%8D%8F%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>python多任务之线程</title>
    <link href="http://yoursite.com/2018/05/17/python%E5%A4%9A%E4%BB%BB%E5%8A%A1%E4%B9%8B%E7%BA%BF%E7%A8%8B/"/>
    <id>http://yoursite.com/2018/05/17/python多任务之线程/</id>
    <published>2018-05-17T13:46:35.000Z</published>
    <updated>2018-09-01T15:58:01.152Z</updated>
    
    <content type="html"><![CDATA[<p>进程可以理解为系统资源分配的单位，简单来说就是一段运行着的程序；那么线程就是系统调度的单位，可以理解为进程中执行代码的游标，系统的调度算法来决定其如何运行。<br><a id="more"></a>  </p><h2 id="1-threading模块使用"><a href="#1-threading模块使用" class="headerlink" title="1.threading模块使用"></a>1.threading模块使用</h2><p>python的thread模块是比较底层的模块，python的threading模块是对thread做了一些包装的，可以更加方便的被使用。<br><strong>1.1单线程执行</strong><br><img src="https://i.imgur.com/x67NBCR.png" alt=""></p><p><strong>1.2多线程执行</strong><br><img src="https://i.imgur.com/rQH2Xtt.png" alt=""><br>经过对比，可以明显看出使用了多线程并发的操作，花费时间要短很多。  </p><p><strong>1.3主线程,子线程执行顺序</strong><br><img src="https://i.imgur.com/iX50xhn.png" alt=""></p><h2 id="2-线程执行代码的封装"><a href="#2-线程执行代码的封装" class="headerlink" title="2.线程执行代码的封装"></a>2.线程执行代码的封装</h2><p><strong>2.1线程子类</strong><br>通过使用threading模块能完成多任务的程序开发，为了让每个线程的封装性更完美，所以使用threading模块时，往往会定义一个新的子类class，只要继承threading.Thread就可以了，然后重写run方法.<br><img src="https://i.imgur.com/SQZGiNr.png" alt=""><br>python的threading.Thread类有一个run方法，用于定义线程的功能函数，可以在自己的线程类中覆盖该方法。<br>而创建自己的线程实例后，通过Thread类的start方法，可以启动该线程，交给python虚拟机进行调度，当该线程获得执行的机会时，就会调用run方法执行线程。<br><strong>2.2线程的执行顺序</strong><br><img src="https://i.imgur.com/2H0gsyC.png" alt=""><br>从代码和执行结果我们可以看出，多线程程序的执行顺序是不确定的。<br>当执行到sleep语句时，线程将被阻塞（Blocked），到sleep结束后，线程进入就绪（Runnable）状态，等待调度。而线程调度将自行选择一个线程执行。上面的代码中只能保证每个线程都运行完整个run函数，但是线程的启动顺序、run函数中每次循环的执行顺序都不能确定。  </p><p><strong>总结</strong><br>每个线程一定会有一个名字，尽管上面的例子中没有指定线程对象的name，但是python会自动为线程指定一个名字。<br>当线程的run()方法结束时该线程完成。<br>无法控制线程调度程序，但可以通过别的方式来影响线程调度的方式。<br>线程的几种状态:<br>新建，就绪，运行，等待（堵塞），死亡  </p><h2 id="3-多线程-共享全局变量"><a href="#3-多线程-共享全局变量" class="headerlink" title="3.多线程-共享全局变量"></a>3.多线程-共享全局变量</h2><p>实例1：<br><img src="https://i.imgur.com/kCgcdgP.png" alt=""></p><p>实例2：列表当做实参传递到线程中<br><img src="https://i.imgur.com/A27ZSCE.png" alt=""><br>在一个进程内的所有线程共享全局变量，能够在不适用其他方式的前提下完成多线程之间的数据共享（这点要比多进程要好）<br>缺点就是，线程是对全局变量随意遂改可能造成多线程之间对全局变量的混乱（即线程非安全）</p><h2 id="4-进程-线程对比"><a href="#4-进程-线程对比" class="headerlink" title="4.进程,线程对比"></a>4.进程,线程对比</h2><p><strong>4.1功能</strong>  </p><blockquote><pre><code>进程:能够完成多任务，比如 在一台电脑上能够同时运行多个QQ线程:能够完成多任务，比如 一个QQ中的多个聊天窗口</code></pre></blockquote><p><strong>4.2定义的不同</strong><br>进程是系统进行资源分配和调度的一个独立单位.<br>线程是进程的一个实体,是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位.线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源.</p><p><strong>4.3区别</strong><br>一个程序至少有一个进程,一个进程至少有一个线程.线程的划分尺度小于进程(资源比进程少)，使得多线程程序的并发性高。进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率;线线程不能够独立执行，必须依存在进程中</p><p><strong>4.4优缺点</strong><br>线程和进程在使用上各有优缺点：线程执行开销小，但不利于资源的管理和保护；而进程正相反。</p><h2 id="5-同步的概念"><a href="#5-同步的概念" class="headerlink" title="5.同步的概念"></a>5.同步的概念</h2><p>在理解同步之前，先看一个问题。多线程开发可能遇到的问题：<br>假设两个线程t1和t2都要对num=0进行增1运算，t1和t2都各对num修改10次，num的最终的结果应该为20。但是由于是多线程访问，有可能出现下面情况：<br>在num=0时，t1取得num=0。此时系统把t1调度为”sleeping”状态，把t2转换为”running”状态，t2也获得num=0。然后t2对得到的值进行加1并赋给num，使得num=1。然后系统又把t2调度为”sleeping”，把t1转为”running”。线程t1又把它之前得到的0加1后赋值给num。这样，明明t1和t2都完成了1次加1工作，但结果仍然是num=1。<br>下面我们就来测试一下：<br><img src="https://i.imgur.com/RmnqU61.png" alt=""><br><img src="https://i.imgur.com/0wX6ZEp.png" alt=""></p><p>问题产生的原因就是没有控制多个线程对同一资源的访问，对数据造成破坏，操作系统在调度一个线程时，执行过程出现暂停，执行另一个线程<br>使得线程运行的结果不可预期。这种现象称为“线程不安全”。</p><p><strong>5.1何为同步</strong><br>同步就是协同步调，按预定的先后次序进行运行。<br>如进程、线程同步，可理解为进程或线程A和B一块配合，A执行到一定程度时要依靠B的某个结果，于是停下来，示意B运行; B依言执行，再将结果给A; A再继续操作。  </p><p><strong>5.2解决问题的思路</strong><br>对于上述提出的那个计算错误的问题，可以通过线程同步来进行解决思路，如下:</p><p>1.系统调用t1，然后获取到num的值为0，此时上一把锁，即不允许其他现在操作num。<br>2.对num的值进行+1<br>3.解锁，此时num的值为1，其他的线程就可以使用num了，而且是num的值不是0而是1<br>4.同理其他线程在对num进行修改时，都要先上锁，处理完后再解锁，在上锁的整个过程中不允许其他线程访问，就保证了数据的正确性  </p><p><strong>5.3互斥锁</strong><br>当多个线程几乎同时修改某一个共享数据的时候，需要进行同步控制线程同步能够保证多个线程安全访问竞争资源，最简单的同步机制是引入互斥锁。</p><p>互斥锁为资源引入一个状态：锁定/非锁定。</p><p>某个线程要更改共享数据时，先将其锁定，此时资源的状态为“锁定”，其他线程不能更改；直到该线程释放资源，将资源的状态变成“非锁定”，其他的线程才能再次锁定该资源。互斥锁保证了每次只有一个线程进行写入操作，从而保证了多线程情况下数据的正确性。<br>threading模块中定义了Lock类，可以方便的处理锁定：</p><blockquote><pre><code>#创建锁mutex = threading.Lock()#锁定mutex.acquire([blocking])#释放mutex.release()</code></pre><p>其中，锁定方法acquire可以有一个blocking参数。<br>如果设定blocking为True，则当前线程会堵塞，直到获取到这个锁为止（如果没有指定，那么默认为True）<br>如果设定blocking为False，则当前线程不会堵塞<br>还有一个timeout参数，用来设定超时时间的。  </p></blockquote><p><img src="https://i.imgur.com/MyvGbrW.png" alt=""><br><img src="https://i.imgur.com/BxN3GGz.png" alt=""><br>可以看到，加入互斥锁后，运行结果与预期相符。  </p><p>过程分析：<br>上锁解锁过程<br>当一个线程调用锁的acquire()方法获得锁时，锁就进入“locked”状态。每次只有一个线程可以获得锁。如果此时另一个线程试图获得这个锁，该线程就会变为“blocked”状态，称为“阻塞”，直到拥有锁的线程调用锁的release()方法释放锁之后，锁进入“unlocked”状态。<br>线程调度程序从处于同步阻塞状态的线程中选择一个来获得锁，并使得该线程进入运行（running）状态。</p><p>总结<br>锁的好处：确保了某段关键代码只能由一个线程从头到尾完整地执行<br>锁的坏处：阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了<br>由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁  </p><h2 id="6-死锁"><a href="#6-死锁" class="headerlink" title="6.死锁"></a>6.死锁</h2><p>在线程间共享多个资源的时候，如果两个线程分别占有一部分资源并且同时等待对方的资源，就会造成死锁。<br>死锁实例：<br><img src="https://i.imgur.com/Xb4rFqx.png" alt=""><br><img src="https://i.imgur.com/5q20yT9.png" alt=""><br>程序会进入死锁状态，如何避免呢？ 1.程序设计前要尽量避免，减少变量交叉调用 2.添加超时时间(timeout)等。  </p><h2 id="7-多线程-非共享数据"><a href="#7-多线程-非共享数据" class="headerlink" title="7.多线程-非共享数据"></a>7.多线程-非共享数据</h2><p>对于全局变量，在多线程中要格外小心，否则容易造成数据错乱的情况发生;那么对于非全局变量我们怎么处理呢？<br><img src="https://i.imgur.com/lD7PF52.png" alt=""></p><p>在多线程开发中，全局变量是多个线程都共享的数据，而局部变量等是各自线程的，是非共享的。  </p><h2 id="8-同步应用"><a href="#8-同步应用" class="headerlink" title="8.同步应用"></a>8.同步应用</h2><p><strong>8.1多个线程有序执行</strong><br>使用互斥锁完成多个任务，有序的进程工作，这就是线程的同步。<br><img src="https://i.imgur.com/lwoA6Hk.png" alt=""><br><img src="https://i.imgur.com/3MXMmEz.png" alt=""></p><h2 id="9-生产者与消费者模式"><a href="#9-生产者与消费者模式" class="headerlink" title="9.生产者与消费者模式"></a>9.生产者与消费者模式</h2><p>为什么要使用生产者和消费者模式？  </p><p>在线程世界里，生产者就是生产数据的线程，消费者就是消费数据的线程。在多线程开发当中，如果生产者处理速度很快，而消费者处理速度很慢，那么生产者就必须等待消费者处理完，才能继续生产数据。同样的道理，如果消费者的处理能力大于生产者，那么消费者就必须等待生产者。为了解决这个问题于是引入了生产者和消费者模式。</p><p>什么是生产者消费者模式？  </p><p>生产者消费者模式是通过一个容器来解决生产者和消费者的强耦合问题。生产者和消费者彼此之间不直接通讯，而通过阻塞队列来进行通讯，所以生产者生产完数据之后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从阻塞队列里取，阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力。<br><img src="https://i.imgur.com/aIvKYxg.png" alt=""><br>这个阻塞队列就是用来给生产者和消费者<strong>解耦</strong>的。</p><p>队列Queue的说明：</p><blockquote><pre><code>对于Queue，在多线程通信之间扮演重要的角色添加数据到队列中，使用put()方法从队列中取数据，使用get()方法判断队列中是否还有数据，使用qsize()方法</code></pre></blockquote><p>实例：<br><img src="https://i.imgur.com/lwVSBjD.png" alt=""><br><img src="https://i.imgur.com/klbL1LC.png" alt=""></p><h2 id="10-ThreadLocal"><a href="#10-ThreadLocal" class="headerlink" title="10.ThreadLocal"></a>10.ThreadLocal</h2><p>在多线程环境下，每个线程都有自己的数据。一个线程使用自己的局部变量比使用全局变量好，因为局部变量只有线程自己能看见，不会影响其他线程，而全局变量的修改必须加锁。为了解决这个问题，我们用到ThreadLocal。<br><img src="https://i.imgur.com/m07r1f6.png" alt="">  </p><p>全局变量local_school就是一个ThreadLocal对象，每个Thread对它都可以读写student属性，但互不影响。你可以把local_school看成全局变量，但每个属性如local_school.student都是线程的局部变量，可以任意读写而互不干扰，也不用管理锁的问题，ThreadLocal内部会处理。</p><p>可以理解为全局变量local_school是一个dict，不但可以用local_school.student，还可以绑定其他变量，如local_school.teacher等等。ThreadLocal最常用的地方就是为每个线程绑定一个数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便地访问这些资源。</p><p>一个ThreadLocal变量虽然是全局变量，但每个线程都只能读写自己线程的独立副本，互不干扰。ThreadLocal解决了参数在一个线程中各个函数之间互相传递的问题</p><h2 id="11-异步"><a href="#11-异步" class="headerlink" title="11.异步"></a>11.异步</h2><p>同步是按顺序执行，执行完一个再执行下一个，需要等待、协调运行。<br>与同步相反，异步就是彼此独立,在等待某事件的过程中继续做自己的事，不需要等待这一事件完成后再工作。<br><img src="https://i.imgur.com/meBycn5.png" alt=""><br>从返回结果看，父进程在执行time.sleep(5)时，被系统分配去做了其他事情(执行callback)，做完后才回来执行print</p><h2 id="12-GIL-全局解释器锁"><a href="#12-GIL-全局解释器锁" class="headerlink" title="12.GIL-全局解释器锁"></a>12.GIL-全局解释器锁</h2><p>Python代码的执行由Python 虚拟机(也叫解释器主循环，CPython版本)来控制，Python 在设计之初就考虑到要在解释器的主循环中，同时只有一个线程在执行，即在任意时刻，只有一个线程在解释器中运行。这大大影响了python多线程的性能。而这个解释器锁由于历史原因，现在几乎无法消除。</p><p>python GIL 之所以会影响多线程等性能，是因为在多线程的情况下，只有当线程获得了一个全局锁的时候，那么该线程的代码才能运行，而全局锁只有一个，所以使用python多线程，在同一时刻也只有一个线程在运行，因此在即使在多核的情况下也只能发挥出单核的性能。</p><p>既然python在同一时刻下只能运行一个线程的代码，那线程之间是如何调度的呢？ </p><p>对于有io操作的线程，当一个线程在做io操作的时候，因为io操作不需要cpu，所以，这个时候，python会释放python全局锁，这样其他需要运行的线程就会使用该锁。 </p><p>对于cpu密集型的线程，比如一个线程可能一直需要使用cpu做计算，那么python中会有一个执行指令的计数器，当一个线程执行了一定数量的指令时，该线程就会停止执行并让出当前的锁，这样其他的线程就可以执行代码了。 </p><p>由上面可知，至少有两种情况python会做线程切换，一是一但有IO操作时，会有线程切换，二是当一个线程连续执行了一定数量的指令时，会出现线程切换。当然此处的线程切换不一定就一定会切换到其他线程执行，因为如果当前线程 优先级比较高的话，可能在让出锁以后，又继续获得锁，并优先执行。</p><p>在做科学计算的时候是用的单线程，因为这种计算是需要CPU一直做计算的，如果用多线程反而会降低计算速度。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;进程可以理解为系统资源分配的单位，简单来说就是一段运行着的程序；那么线程就是系统调度的单位，可以理解为进程中执行代码的游标，系统的调度算法来决定其如何运行。&lt;br&gt;
    
    </summary>
    
      <category term="系统编程" scheme="http://yoursite.com/categories/%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B/"/>
    
    
      <category term="多线程" scheme="http://yoursite.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>python多任务之进程</title>
    <link href="http://yoursite.com/2018/05/15/python%E5%A4%9A%E4%BB%BB%E5%8A%A1%E4%B9%8B%E8%BF%9B%E7%A8%8B/"/>
    <id>http://yoursite.com/2018/05/15/python多任务之进程/</id>
    <published>2018-05-15T13:28:56.000Z</published>
    <updated>2018-09-01T15:57:38.874Z</updated>
    
    <content type="html"><![CDATA[<p>在讲进程之前我们先理解一下多任务。什么叫“多任务”呢？简单地说，就是操作系统可以同时运行多个任务。<br><a id="more"></a><br>真正的并行执行多任务只能在多核CPU上实现，但在实际应用中，由于任务数量远远多于CPU的核心数量，所以，操作系统也会自动把很多任务轮流调度到每个核心上执行，让各个任务交替执行，任务1执行0.01秒，切换到任务2，任务2执行0.01秒，再切换到任务3，执行0.01秒……这样反复执行下去。表面上看，每个任务都是交替执行的，但是，由于CPU的执行速度实在是太快了，我们感觉就像所有任务都在同时执行一样。</p><h2 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h2><p>可以这样理解，编写完毕的代码，在没有运行的时候，称之为程序；正在运行着的代码，就成为进程。在Unix/Linux操作系统中，提供了一个fork()系统函数来创建子进程，由于我这里用的是Windows系统，所以没有fork调用。但Python是跨平台的，自然也应该提供一个跨平台的多进程支持。multiprocessing模块就是跨平台版本的多进程模块。  </p><h2 id="进程的创建"><a href="#进程的创建" class="headerlink" title="进程的创建"></a>进程的创建</h2><p><strong>multiprocessing</strong>模块提供了一个<strong>Process</strong>类来代表一个进程对象。<br>Process语法结构如下：<br><code>Process([group [, target [, name [, args [, kwargs]]]]])</code>  </p><blockquote><pre><code>target：表示这个进程实例所调用对象；</code></pre></blockquote><blockquote><pre><code>args：表示调用对象的位置参数元组；</code></pre></blockquote><blockquote><pre><code>kwargs：表示调用对象的关键字参数字典；</code></pre></blockquote><blockquote><pre><code>name：为当前进程实例的别名；</code></pre></blockquote><blockquote><pre><code>group：大多数情况下用不到；  </code></pre></blockquote><p> Process类常用方法：  </p><blockquote><pre><code>is_alive()：判断进程实例是否还在执行；</code></pre></blockquote><blockquote><pre><code>join([timeout])：是否等待进程实例执行结束，或等待多少秒；</code></pre></blockquote><blockquote><pre><code>start()：启动进程实例（创建子进程）；</code></pre></blockquote><blockquote><pre><code>run()：如果没有给定target参数，对这个对象调用start()方法时，就将执行对象中的run()方法；</code></pre></blockquote><blockquote><pre><code>terminate()：不管任务是否完成，立即终止；</code></pre></blockquote><p>Process类常用属性：</p><blockquote><pre><code>name：当前进程实例别名，默认为Process-N，N为从1开始递增的整数；</code></pre></blockquote><blockquote><pre><code>pid：当前进程实例的PID值；</code></pre></blockquote><p>进程创建实例1：<br><img src="https://i.imgur.com/Jq7MXSb.png" alt="">  </p><p>进程创建实例2：<br><img src="https://i.imgur.com/ZCtey8Z.png" alt="">  </p><p>进程创建实例3：<br><img src="https://i.imgur.com/bstU9Dm.png" alt=""><br><img src="https://i.imgur.com/zQ1O5aa.png" alt=""></p><h2 id="创建进程——Process子类"><a href="#创建进程——Process子类" class="headerlink" title="创建进程——Process子类"></a>创建进程——Process子类</h2><p>创建新的进程还能够使用类的方式，可以自定义一个类，继承Process类，每次实例化这个类的时候，就等同于实例化一个进程对象。<br><img src="https://i.imgur.com/XfmluhR.png" alt=""></p><h2 id="进程池Pool"><a href="#进程池Pool" class="headerlink" title="进程池Pool"></a>进程池Pool</h2><p>当需要创建的子进程数量不多时，可以直接利用multiprocessing中的Process动态成生多个进程，但如果是上百甚至上千个目标，<br>手动的去创建进程的工作量巨大，此时就可以用到multiprocessing模块提供的Pool方法。<br>初始化Pool时，可以指定一个最大进程数，当有新的请求提交到Pool中时，如果池还没有满，那么就会创建一个新的进程用来执行该请求；<br>但如果池中的进程数已经达到指定的最大值，那么该请求就会等待，直到池中有进程结束，才会创建新的进程来执行。  </p><p>multiprocessing.Pool常用函数解析:  </p><blockquote><pre><code>apply_async(func[, args[, kwds]]) ：使用非阻塞方式调用func（并行执行，堵塞方式必须等待上一个进程退出才能执行下一个进程），args为传递给func的参数列表，kwds为传递给func的关键字参数列表；</code></pre></blockquote><blockquote><pre><code>apply(func[, args[, kwds]])：使用阻塞方式调用func;</code></pre></blockquote><blockquote><pre><code>close()：关闭Pool，使其不再接受新的任务；</code></pre></blockquote><blockquote><pre><code>terminate()：不管任务是否完成，立即终止；</code></pre></blockquote><blockquote><pre><code>join()：主进程阻塞，等待子进程的退出， 必须在close或terminate之后使用；</code></pre></blockquote><p>创建实例1-非阻塞方式：<br><img src="https://i.imgur.com/N9rWlgc.png" alt=""></p><p>创建实例2-阻塞方式：<br><img src="https://i.imgur.com/RLDT4Xo.png" alt=""></p><h2 id="进程间通信-Queue"><a href="#进程间通信-Queue" class="headerlink" title="进程间通信-Queue"></a>进程间通信-Queue</h2><p>Process之间有时需要通信，操作系统提供了很多机制来实现进程间的通信。  </p><p><strong>Queue的使用</strong><br>可以使用multiprocessing模块的Queue实现多进程之间的数据传递，Queue本身是一个消息列队程序。<br>Queue说明:</p><blockquote><pre><code>初始化Queue()对象时（例如：q=Queue()），若括号中没有指定最大可接收的消息数量，或数量为负值，那么就代表可接受的消息数量没有上限（直到内存的尽头）；</code></pre></blockquote><blockquote><pre><code>Queue.qsize()：返回当前队列包含的消息数量；</code></pre></blockquote><blockquote><pre><code>Queue.empty()：如果队列为空，返回True，反之False ；</code></pre></blockquote><blockquote><pre><code>Queue.full()：如果队列满了，返回True,反之False；</code></pre></blockquote><blockquote><pre><code>Queue.get([block[, timeout]])：获取队列中的一条消息，然后将其从列队中移除，block默认值为True；</code></pre></blockquote><blockquote><pre><code>1）如果block使用默认值，且没有设置timeout（单位秒），消息列队如果为空，此时程序将被阻塞（停在读取状态），直到从消息列队读到消息为止，如果设置了timeout，则会等待timeout秒，若还没读取到任何消息，则抛出&quot;Queue.Empty&quot;异常；</code></pre></blockquote><blockquote><pre><code>2）如果block值为False，消息列队如果为空，则会立刻抛出&quot;Queue.Empty&quot;异常；</code></pre></blockquote><blockquote><pre><code>Queue.get_nowait()：相当Queue.get(False)；</code></pre></blockquote><blockquote><pre><code>Queue.put(item,[block[, timeout]])：将item消息写入队列，block默认值为True；</code></pre></blockquote><blockquote><pre><code>1）如果block使用默认值，且没有设置timeout（单位秒），消息列队如果已经没有空间可写入，此时程序将被阻塞（停在写入状态），直到从消息列队腾出空间为止，如果设置了timeout，则会等待timeout秒，若还没空间，则抛出&quot;Queue.Full&quot;异常；</code></pre></blockquote><blockquote><pre><code>2）如果block值为False，消息列队如果没有空间可写入，则会立刻抛出&quot;Queue.Full&quot;异常；</code></pre></blockquote><blockquote><pre><code>Queue.put_nowait(item)：相当Queue.put(item, False)；</code></pre></blockquote><p>实例1:<br><img src="https://i.imgur.com/t5fT4hS.png" alt=""></p><p>实例2：<br><img src="https://i.imgur.com/6VOpMmA.png" alt=""><br><img src="https://i.imgur.com/8fkdENa.png" alt=""></p><h2 id="进程池中的Queue"><a href="#进程池中的Queue" class="headerlink" title="进程池中的Queue"></a>进程池中的Queue</h2><p>如果要使用Pool创建进程，就需要使用multiprocessing.Manager()中的Queue()，而不是multiprocessing.Queue()。<br>进程池中的进程如何通信实例:<br><img src="https://i.imgur.com/fIsQvuz.png" alt=""><br><img src="https://i.imgur.com/VMCeJ3t.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在讲进程之前我们先理解一下多任务。什么叫“多任务”呢？简单地说，就是操作系统可以同时运行多个任务。&lt;br&gt;
    
    </summary>
    
      <category term="系统编程" scheme="http://yoursite.com/categories/%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B/"/>
    
    
      <category term="多进程" scheme="http://yoursite.com/tags/%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    
  </entry>
  
</feed>
